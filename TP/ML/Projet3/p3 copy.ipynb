{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 5\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "\n",
    "vocab_size = 2908\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def replace_rare_words(text_file, output_file, threshold=5):\n",
    "    # Read the text file\n",
    "    with open(text_file, 'r') as f:\n",
    "        text = f.read().split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    # Replace words that appear less than the threshold with <unk>\n",
    "    modified_text = [word if word_counts[word] >= threshold else '<unk>' for word in text]\n",
    "    \n",
    "    # Write the modified text to the output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(' '.join(modified_text))\n",
    "\n",
    "# Example usage\n",
    "replace_rare_words('Le_comte_de_Monte_Cristo.train.tok', 'Le_comte_de_Monte_Cristo.train.100.unk5.tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocab import Vocab\n",
    "\n",
    "def get_word_and_next_k_indexes(text, vocab_dict, k=3):\n",
    "    with open(text, 'r') as f:\n",
    "        words = f.read().split()\n",
    "    \n",
    "    indexed_data = []\n",
    "    \n",
    "    for i in range(len(words) - k):\n",
    "        temp = []\n",
    "        current_word_index = vocab_dict.get(words[i], vocab_dict['<unk>'])\n",
    "        \n",
    "        next_k_indexes = [vocab_dict.get(words[i + j + 1], vocab_dict['<unk>']) for j in range(k)]\n",
    "        temp = next_k_indexes\n",
    "        temp.insert(0, current_word_index)\n",
    "        indexed_data.append(temp)\n",
    "    \n",
    "    return indexed_data\n",
    "\n",
    "def transform_to_embeddings(data, embeddings_file):\n",
    "    vocab = Vocab(emb_filename=embeddings_file)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len(data)):\n",
    "        res.append([vocab.get_emb_torch(word) for word in data[i]])\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'</s>': 0, '<s>': 1, '<unk>': 2, ',': 3, 'de': 4, '.': 5, 'le': 6, 'et': 7, 'la': 8, 'à': 9, 'il': 10, '—': 11, 'l’': 12, '-': 13, 'que': 14, 'un': 15, 'vous': 16, ';': 17, 'en': 18, 'd’': 19, 'les': 20, 'qui': 21, 'une': 22, 'je': 23, 'qu’': 24, 'est': 25, 'ce': 26, '?': 27, 'pas': 28, 'son': 29, 'dit': 30, '!': 31, 'du': 32, 'était': 33, 'dans': 34, 'ne': 35, 's’': 36, 'lui': 37, 'se': 38, 'au': 39, 'avait': 40, 'pour': 41, 'des': 42, 'n’': 43, 'mais': 44, 'sur': 45, 'on': 46, 'c’': 47, ':': 48, 'dantès': 49, 'comme': 50, 'plus': 51, 'sa': 52, 'cette': 53, 'avec': 54, 'a': 55, 'tout': 56, 'bien': 57, 'par': 58, 'si': 59, 'nous': 60, 'homme': 61, 'ses': 62, 'me': 63, 'j’': 64, 'mon': 65, 'deux': 66, 'y': 67, 'elle': 68, 'même': 69, 'franz': 70, 'moi': 71, 'ces': 72, 'monsieur': 73, 'ai': 74, 'ou': 75, 'être': 76, 'sans': 77, 'fait': 78, 'donc': 79, 'm’': 80, 'où': 81, 'votre': 82, 'oui': 83, 'cela': 84, 'faire': 85, 'jeune': 86, 'ils': 87, 'encore': 88, 'morrel': 89, 'villefort': 90, 'aux': 91, 'alors': 92, 'autre': 93, 'albert': 94, 'comte': 95, 'avez': 96, 'edmond': 97, 'non': 98, 'eh': 99, 'dire': 100, 'puis': 101, 'caderousse': 102, 'rien': 103, 'tous': 104, 'demanda': 105, 'leur': 106, 'fois': 107, 'temps': 108, 'm': 109, 'père': 110, 'abbé': 111, 'car': 112, 'été': 113, 'cet': 114, 'avoir': 115, 'peu': 116, '…': 117, 'peut': 118, 'là': 119, 'trois': 120, 'après': 121, 'dont': 122, 'tu': 123, 'jour': 124, 'répondit': 125, 'yeux': 126, 'fit': 127, 't': 128, 'quelque': 129, 'suis': 130, 'eût': 131, 'celui': 132, 'ma': 133, 'main': 134, 'porte': 135, 'fut': 136, 'toute': 137, 'chose': 138, 'étaient': 139, 'point': 140, 'êtes': 141, 'moins': 142, 'vers': 143, 'aussi': 144, 'heure': 145, 'danglars': 146, 'reprit': 147, 'ainsi': 148, 'près': 149, 'tête': 150, 'dieu': 151, 'fernand': 152, 'quand': 153, 'moment': 154, 'sous': 155, 'instant': 156, 'mort': 157, 'chez': 158, 'déjà': 159, 'toujours': 160, 'grand': 161, 'toutes': 162, 'île': 163, 'seul': 164, 'devant': 165, 'ah': 166, 'mercédès': 167, 'heures': 168, 'depuis': 169, 'maintenant': 170, 'voir': 171, 'oh': 172, 'quelques': 173, 'avaient': 174, 'écria': 175, 'voilà': 176, 'jusqu’': 177, 'sont': 178, 'seulement': 179, 'fille': 180, 'entre': 181, 'ami': 182, 'jamais': 183, 'cependant': 184, 'mille': 185, 'lettre': 186, 'cher': 187, 'monde': 188, 'vu': 189, 'hommes': 190, 'comment': 191, 'pouvait': 192, 'quatre': 193, 'roi': 194, 'nuit': 195, 'fort': 196, 'coup': 197, 'jours': 198, 'soit': 199, 'faria': 200, 'cœur': 201, 'place': 202, 'vie': 203, 'dix': 204, 'ans': 205, 'enfin': 206, 'continua': 207, 'doute': 208, 'pendant': 209, 'leurs': 210, 'cinq': 211, 'bon': 212, 'voix': 213, 'gens': 214, 'vit': 215, 'pauvre': 216, 'mer': 217, 'faut': 218, 'bruit': 219, 'nom': 220, 'mes': 221, 'capitaine': 222, 'terre': 223, 'effet': 224, 'avons': 225, 'autres': 226, 'presque': 227, 'chambre': 228, 'vieillard': 229, 'eu': 230, 'peine': 231, 'venait': 232, 'regard': 233, 'trop': 234, 'ni': 235, 'sais': 236, 'mois': 237, 'premier': 238, 'excellence': 239, 'tour': 240, 'maître': 241, 'bâtiment': 242, 'côté': 243, 'vingt': 244, 'bras': 245, 'première': 246, 'monte': 247, 'notre': 248, 'aller': 249, 'maison': 250, 'marseille': 251, 'voulez': 252, 'air': 253, 'bout': 254, 'prit': 255, 'fût': 256, 'rue': 257, 'cristo': 258, 'ici': 259, 'mains': 260, 'laquelle': 261, 'petit': 262, 'visage': 263, 'amis': 264, 'prisonnier': 265, 'assez': 266, 'dites': 267, 'celle': 268, 'lit': 269, 'allait': 270, 'mieux': 271, 'allons': 272, 'reste': 273, 'lequel': 274, 'prison': 275, 'voiture': 276, 'fils': 277, 'avant': 278, 'contre': 279, 'table': 280, 'ont': 281, 'femme': 282, 'esprit': 283, 'geôlier': 284, 'crois': 285, 'milieu': 286, 'savez': 287, 'serait': 288, 'soir': 289, 'sera': 290, 'gouverneur': 291, 'vampa': 292, 'paris': 293, 'vos': 294, 'ailleurs': 295, 'luigi': 296, 'teresa': 297, 'honneur': 298, 'pu': 299, 'cent': 300, 'lorsqu’': 301, 'rome': 302, 'chaque': 303, 'donné': 304, 'tandis': 305, 'eut': 306, 'personne': 307, 'six': 308, 'pris': 309, 'lieu': 310, 'va': 311, 'tant': 312, 'prendre': 313, 'louis': 314, 'seule': 315, 'faisait': 316, 'inspecteur': 317, 'saint': 318, 'malheur': 319, 'ci': 320, 'seconde': 321, 'avais': 322, 'francs': 323, 'armateur': 324, 'voyez': 325, 'grande': 326, 'pieds': 327, 'semblait': 328, 'eux': 329, 'marin': 330, 'quant': 331, 'besoin': 332, 'quoi': 333, 'belle': 334, 'sire': 335, 'pharaon': 336, 'quel': 337, 'voici': 338, 'allez': 339, '«': 340, 'hui': 341, 'idée': 342, 'patron': 343, 'pastrini': 344, 'parler': 345, 'abord': 346, 'ceux': 347, 'choses': 348, 'aujourd’': 349, 'pied': 350, 'passa': 351, 'matin': 352, 'murmura': 353, 'pourquoi': 354, 'anglais': 355, 'étais': 356, 'lorsque': 357, 'or': 358, 'passé': 359, 'palais': 360, 'mal': 361, 'part': 362, 'te': 363, 'vérité': 364, 'liberté': 365, 'papier': 366, 'passer': 367, 'aussitôt': 368, 'œil': 369, 'nouveau': 370, 'derrière': 371, 'elles': 372, 'mettre': 373, 'alla': 374, 'mouvement': 375, 'venu': 376, 'autour': 377, 'vrai': 378, 'leva': 379, 'mots': 380, 'lendemain': 381, 'habitude': 382, 'veux': 383, 'hôte': 384, 'arrivé': 385, 'bord': 386, 'front': 387, 'heureux': 388, 'chemin': 389, 'cinquante': 390, 'comtesse': 391, 'château': 392, 'barque': 393, 'sourire': 394, 'faites': 395, 'regarda': 396, 'bas': 397, 'parce': 398, 'donner': 399, 'petite': 400, 'huit': 401, 'ciel': 402, 'joie': 403, 'force': 404, 'demande': 405, 'aurait': 406, 'quelle': 407, 'cachot': 408, 'vieux': 409, 'demi': 410, 'signe': 411, 'xviii': 412, 'vint': 413, 'demander': 414, 'argent': 415, 'moitié': 416, 'bonheur': 417, 'ouvrit': 418, 'jeunes': 419, 'aucune': 420, 'savoir': 421, 'grâce': 422, 'jeta': 423, 'impossible': 424, 'vais': 425, 'sept': 426, 'fou': 427, 'mit': 428, 'compagnon': 429, 'bientôt': 430, 'mot': 431, 'peppino': 432, 'raison': 433, 'nouvelle': 434, 'chercher': 435, 'aura': 436, 'moyen': 437, 'paroles': 438, 'pierre': 439, 'silence': 440, 'trouva': 441, 'chef': 442, 'chacun': 443, 'sombre': 444, 'contraire': 445, 'resta': 446, 'malgré': 447, 'dis': 448, 'corps': 449, 'endroit': 450, 'ouverture': 451, 'longtemps': 452, 'étrange': 453, 'police': 454, 'majesté': 455, 'ministre': 456, 'ordre': 457, 'autant': 458, 'libre': 459, 'espèce': 460, 'vient': 461, 'dessus': 462, 'malheureux': 463, 'veut': 464, 'trouvé': 465, 'devait': 466, 'attendre': 467, 'face': 468, 'perdu': 469, 'semble': 470, 'nos': 471, '»': 472, 'escalier': 473, 'as': 474, 'costume': 475, 'fortune': 476, 'vue': 477, 'napoléon': 478, 'gaetano': 479, 'mourir': 480, 'route': 481, 'second': 482, 'sens': 483, 'cri': 484, 'diable': 485, 'pareil': 486, 'noirtier': 487, 'duc': 488, 'aucun': 489, 'if': 490, 'dernier': 491, 'veille': 492, 'juste': 493, 'garçon': 494, 'faite': 495, 'trouver': 496, 'bonne': 497, 'fond': 498, 'demain': 499, 'retour': 500, 'cas': 501, 'pensée': 502, 'plutôt': 503, 'entendit': 504, 'venir': 505, 'matelots': 506, 'riant': 507, 'vin': 508, 'tenait': 509, 'rendre': 510, 'france': 511, 'madame': 512, 'entendre': 513, 'trésor': 514, 'bandits': 515, 'rocher': 516, 'jacopo': 517, 'grotte': 518, 'état': 519, 'voyage': 520, 'parole': 521, 'minutes': 522, 'voyons': 523, 'cents': 524, 'sortit': 525, 'coin': 526, 'croire': 527, 'assis': 528, 'lumière': 529, 'douleur': 530, 'entra': 531, 'renée': 532, 'bandit': 533, 'port': 534, 'cheveux': 535, 'revint': 536, 'fenêtre': 537, 'sait': 538, 'toi': 539, 'voyait': 540, 'appelle': 541, 'disait': 542, 'aide': 543, 'feu': 544, 'entendu': 545, 'justice': 546, 'parut': 547, 'cabinet': 548, 'carlini': 549, 'surtout': 550, 'sorte': 551, 'voyant': 552, 'donne': 553, 'perdre': 554, 'plaisir': 555, 'doit': 556, 'soyez': 557, 'faisant': 558, 'hôtel': 559, 'voulu': 560, 'voulait': 561, 'spada': 562, 'put': 563, 'calme': 564, 'long': 565, 'enfant': 566, 'peur': 567, 'espère': 568, 'soleil': 569, 'arrêta': 570, 'profond': 571, 'laisser': 572, 'fin': 573, 'navire': 574, 'affaire': 575, 'conversation': 576, 'façon': 577, 'dernière': 578, 'souriant': 579, 'visite': 580, 'suite': 581, 'pâle': 582, 'verre': 583, 'sommes': 584, 'très': 585, 'but': 586, 'ajouta': 587, 'pareille': 588, 'haut': 589, 'fenêtres': 590, 'étranger': 591, 'coclès': 592, 'tira': 593, 'loin': 594, 'nouvelles': 595, 'droit': 596, 'quoique': 597, 'bouche': 598, 'beau': 599, 'rêve': 600, 'disparu': 601, 'mesure': 602, 'fer': 603, 'parfaitement': 604, 'ordres': 605, 'arrêté': 606, 'possible': 607, 'laissa': 608, 'inutile': 609, 'femmes': 610, 'foi': 611, 'debout': 612, 'croyez': 613, 'terrible': 614, 'cours': 615, 'furent': 616, 'italie': 617, 'rentra': 618, 'calèche': 619, 'loge': 620, 'donnait': 621, 'histoire': 622, 'cucumetto': 623, 'muraille': 624, 'eau': 625, 'elbe': 626, 'simple': 627, 'devenu': 628, 'question': 629, 'arrêter': 630, 'vois': 631, 'sentit': 632, 'service': 633, 'rouge': 634, 'attendez': 635, 'espérance': 636, 'resté': 637, 'profonde': 638, 'voulut': 639, 'obscurité': 640, 'blacas': 641, 'trente': 642, 'paraît': 643, 'attention': 644, 'aime': 645, 'donna': 646, 'oreille': 647, 'gauche': 648, 'famille': 649, 'riche': 650, 'général': 651, 'voyageur': 652, 'viens': 653, 'sembla': 654, 'terreur': 655, 'véritablement': 656, 'tranquille': 657, 'paraissait': 658, 'prisonniers': 659, 'fusil': 660, 'césar': 661, 'diamant': 662, 'emmanuel': 663, 'julie': 664, 'vivement': 665, 'parlez': 666, 'puisque': 667, 'merci': 668, 'fais': 669, 'lèvres': 670, 'importe': 671, 'beaucoup': 672, 'attendait': 673, 'amour': 674, 'entrer': 675, 'couteau': 676, 'tomber': 677, 'compte': 678, 'procureur': 679, 'messieurs': 680, 'substitut': 681, 'sûr': 682, 'marquise': 683, 'somme': 684, 'journée': 685, 'pouvez': 686, 'prenez': 687, 'laissé': 688, 'voisin': 689, 'parle': 690, 'savait': 691, 'travers': 692, 'espoir': 693, 'tenir': 694, 'véritable': 695, 'usurpateur': 696, 'ouvrir': 697, 'ombre': 698, 'souvent': 699, 'sauvé': 700, 'manteau': 701, 'arriva': 702, 'occasion': 703, 'carnaval': 704, 'ville': 705, 'retournant': 706, 'arriver': 707, 'chargé': 708, 'pardon': 709, 'dîner': 710, 'sourit': 711, 'catalans': 712, 'fiancée': 713, 'retourna': 714, 'ton': 715, 'sueur': 716, 'comprends': 717, 'dès': 718, 'cause': 719, 'descendit': 720, 'ferme': 721, 'approcha': 722, 'adieu': 723, 'bois': 724, 'français': 725, 'bonapartiste': 726, 'restait': 727, 'mémoire': 728, 'âme': 729, 'comprit': 730, 'travail': 731, 'intérêt': 732, 'écoutez': 733, 'corridor': 734, 'vent': 735, 'domestique': 736, 'sentinelle': 737, 'piastres': 738, 'poste': 739, 'cria': 740, 'brave': 741, 'revenir': 742, 'parlé': 743, 'quelqu’': 744, 'quinze': 745, 'suivit': 746, 'autrefois': 747, 'sang': 748, 'immense': 749, 'hors': 750, 'mademoiselle': 751, 'sort': 752, 'soldats': 753, 'hier': 754, 'plein': 755, 'troisième': 756, 'médecin': 757, 'sortir': 758, 'souper': 759, 'avança': 760, 'empereur': 761, 'selon': 762, 'grands': 763, 'ta': 764, 'bourse': 765, 'dents': 766, 'prêt': 767, 'auquel': 768, 'font': 769, 'douze': 770, 'pays': 771, 'immobile': 772, 'rochers': 773, 'aperçut': 774, 'tuer': 775, 'crime': 776, 'levant': 777, 'repas': 778, 'midi': 779, 'passe': 780, 'partie': 781, 'ordinaire': 782, 'reconnut': 783, 'secret': 784, 'inconnu': 785, 'appelait': 786, 'del': 787, 'yacht': 788, 'simbad': 789, 'san': 790, 'arrivée': 791, 'heureusement': 792, 'inquiétude': 793, 'chapeau': 794, 'uns': 795, 'figure': 796, 'époque': 797, 'dois': 798, 'accent': 799, 'arrive': 800, 'comprenez': 801, 'forces': 802, 'regardant': 803, 'oublié': 804, 'disant': 805, 'tenez': 806, 'ayant': 807, 'mère': 808, 'mauvaise': 809, 'rester': 810, 'pouvoir': 811, 'poussa': 812, 'reçu': 813, 'personnes': 814, 'justement': 815, 'angle': 816, 'désespoir': 817, 'courage': 818, 'cru': 819, 'pain': 820, 'corse': 821, 'forcé': 822, 'fauteuil': 823, 'suivre': 824, 'passait': 825, 'plusieurs': 826, 'années': 827, 'contrebandiers': 828, 'boville': 829, 'garde': 830, 'forme': 831, 'curieux': 832, 'éclair': 833, 'venez': 834, 'arrière': 835, 'certaine': 836, 'répondre': 837, 'reconnaissance': 838, 'crainte': 839, 'étonnement': 840, 'fera': 841, 'parti': 842, 'premiers': 843, 'arrivait': 844, 'regardait': 845, 'parmi': 846, 'demeura': 847, 'propre': 848, 'duquel': 849, 'exemple': 850, 'grandes': 851, 'nommé': 852, 'suivi': 853, 'voile': 854, 'lesquels': 855, 'coups': 856, 'avance': 857, 'servir': 858, 'clef': 859, 'année': 860, 'morcerf': 861, 'triste': 862, 'rapide': 863, 'foule': 864, 'résolution': 865, 'danger': 866, 'croix': 867, 'agent': 868, 'portait': 869, 'nuage': 870, 'entré': 871, 'aurai': 872, 'pourrait': 873, 'larmes': 874, 'ensemble': 875, 't’': 876, 'avenir': 877, 'convaincu': 878, 'an': 879, 'mari': 880, 'rentrer': 881, 'attendant': 882, 'côte': 883, 'écriture': 884, 'salle': 885, 'complètement': 886, 'commença': 887, 'étonné': 888, 'coupable': 889, 'aviez': 890, 'rendez': 891, 'existence': 892, 'marquis': 893, 'gouvernement': 894, 'passage': 895, 'sentait': 896, 'lueur': 897, 'poitrine': 898, 'apprendre': 899, 'condamné': 900, 'faim': 901, 'cerveau': 902, 'baron': 903, 'reconnaître': 904, 'dû': 905, 'cadavre': 906, 'captivité': 907, 'blanc': 908, 'prêtre': 909, 'maximilien': 910, 'vague': 911, 'outre': 912, 'excellent': 913, 'haine': 914, 'porto': 915, 'trompé': 916, 'ferai': 917, 'réalité': 918, 'expression': 919, 'double': 920, 'seigneur': 921, 'appris': 922, 'large': 923, 'attendu': 924, 'ait': 925, 'cour': 926, 'faveur': 927, 'frère': 928, 'élança': 929, 'muet': 930, 'adresse': 931, 'rire': 932, 'connaissez': 933, 'droite': 934, 'preuve': 935, 'murailles': 936, 'tué': 937, 'laissant': 938, 'trône': 939, 'jeté': 940, 'poche': 941, ')': 942, 'souvenir': 943, 'pâleur': 944, 'mis': 945, 'connaissait': 946, 'circonstance': 947, 'méran': 948, 'échafaud': 949, 'folie': 950, 'suprême': 951, 'ligne': 952, 'détails': 953, 'antichambre': 954, 'reconnu': 955, 'partir': 956, 'essaya': 957, 'livres': 958, 'haute': 959, 'laissait': 960, 'connu': 961, 'galerie': 962, 'église': 963, 'réponse': 964, 'cardinal': 965, 'pape': 966, 'matelot': 967, 'amélie': 968, 'geste': 969, 'leclère': 970, 'rapport': 971, 'longue': 972, 'demie': 973, 'tomba': 974, 'vide': 975, 'connais': 976, 'santé': 977, 'prie': 978, 'écrire': 979, 'voudrez': 980, 'asseoir': 981, 'neuf': 982, 'laisse': 983, 'chaise': 984, 'malade': 985, 'genoux': 986, 'bons': 987, 'es': 988, 'catalan': 989, 'noir': 990, 'tirer': 991, 'sol': 992, 'commis': 993, 'pleine': 994, 'amant': 995, 'malheureusement': 996, 'continuait': 997, 'laissez': 998, 'lignes': 999, 'connaissance': 1000, '(': 1001, 'hasard': 1002, 'nature': 1003, 'quart': 1004, 'avis': 1005, 'fuir': 1006, 'armes': 1007, 'scène': 1008, 'vite': 1009, 'froid': 1010, 'millions': 1011, 'dévouement': 1012, 'traits': 1013, 'spectacle': 1014, 'ô': 1015, 'manger': 1016, 'grave': 1017, 'âge': 1018, 'prise': 1019, 'attendit': 1020, 'descendre': 1021, 'curiosité': 1022, 'rivage': 1023, 'compter': 1024, 'mur': 1025, 'quitter': 1026, 'difficile': 1027, 'présent': 1028, 'taille': 1029, 'précaution': 1030, 'clefs': 1031, 'dessous': 1032, 'déjeuner': 1033, 'diamants': 1034, 'livourne': 1035, 'ali': 1036, 'penelon': 1037, 'colisée': 1038, 'rospoli': 1039, 'cap': 1040, 'noirs': 1041, 'réponds': 1042, 'pardieu': 1043, 'croyais': 1044, 'serai': 1045, 'demandé': 1046, 'affaires': 1047, 'rappelle': 1048, 'besogne': 1049, 'tort': 1050, 'morceau': 1051, 'certainement': 1052, 'rejoindre': 1053, 'regards': 1054, 'espagne': 1055, 'ennemi': 1056, 'vis': 1057, 'soupir': 1058, 'croyait': 1059, 'tard': 1060, 'avoue': 1061, 'plume': 1062, 'dénonciation': 1063, 'volonté': 1064, 'facile': 1065, 'enfants': 1066, 'seuls': 1067, 'trouve': 1068, 'promesse': 1069, 'moindre': 1070, 'trouvait': 1071, 'donnant': 1072, 'peuple': 1073, 'sinon': 1074, 'hélas': 1075, 'fallait': 1076, 'juge': 1077, 'parfois': 1078, 'distinguer': 1079, 'no': 1080, 'portant': 1081, 'prix': 1082, 'chevaux': 1083, 'pitié': 1084, 'certain': 1085, 'vaut': 1086, 'quelconque': 1087, 'pût': 1088, 'trou': 1089, 'compagnons': 1090, 'torche': 1091, 'thomson': 1092, 'french': 1093, 'présenter': 1094, 'vicomte': 1095, 'serez': 1096, 'équipage': 1097, 'guerre': 1098, 'aimé': 1099, 'départ': 1100, 'voudrais': 1101, 'allées': 1102, 'combien': 1103, 'pense': 1104, 'offre': 1105, 'prenant': 1106, 'langue': 1107, 'parlait': 1108, 'comprendre': 1109, 'ferez': 1110, 'vêtu': 1111, 'touché': 1112, 'courant': 1113, 'plaisanterie': 1114, 'écoute': 1115, 'commençait': 1116, 'donnez': 1117, 'registre': 1118, 'derniers': 1119, 'tombe': 1120, 'encre': 1121, 'œuvre': 1122, 'émotion': 1123, 'demandez': 1124, 'faisaient': 1125, 'élevé': 1126, 'recommandation': 1127, 'digne': 1128, 'sentiment': 1129, 'doux': 1130, 'mêmes': 1131, 'présenta': 1132, 'appartement': 1133, 'cocher': 1134, 'épaules': 1135, 'cesse': 1136, 'messager': 1137, 'rues': 1138, 'lire': 1139, 'brisé': 1140, 'borgia': 1141, 'fini': 1142, 'montre': 1143, 'exécution': 1144, 'pierres': 1145, 'signal': 1146, 'afin': 1147, 'pioche': 1148, 'popolo': 1149, 'carmela': 1150, 'février': 1151, 'dame': 1152, 'lentement': 1153, 'entrée': 1154, 'voiles': 1155, 'impatience': 1156, 'camarade': 1157, 'devoir': 1158, 'quelquefois': 1159, 'congé': 1160, 'italien': 1161, 'essayer': 1162, 'permission': 1163, 'firent': 1164, 'bande': 1165, 'horizon': 1166, 'répondu': 1167, 'mauvais': 1168, 'monta': 1169, 'tressaillit': 1170, 'regarde': 1171, 'levé': 1172, 'clair': 1173, 'simplement': 1174, 'intelligence': 1175, 'sorti': 1176, 'effort': 1177, 'coucher': 1178, 'rayons': 1179, 'marchait': 1180, 'cacher': 1181, 'portes': 1182, 'attend': 1183, 'commissaire': 1184, 'conviction': 1185, 'projet': 1186, 'entier': 1187, 'agissait': 1188, 'influence': 1189, 'devez': 1190, 'remit': 1191, 'gendarmes': 1192, 'craindre': 1193, 'espace': 1194, 'tantôt': 1195, 'contenait': 1196, 'marche': 1197, 'meurtre': 1198, 'bleu': 1199, 'événements': 1200, 'entièrement': 1201, 'soin': 1202, 'eussent': 1203, 'trésors': 1204, 'oreilles': 1205, 'crut': 1206, 'posa': 1207, 'caché': 1208, 'merveille': 1209, 'dalle': 1210, 'cris': 1211, 'tartane': 1212, 'aubergiste': 1213, 'felice': 1214, 'rita': 1215, 'bouquet': 1216, 'partit': 1217, 'réserve': 1218, 'quitta': 1219, 'particulier': 1220, 'monter': 1221, 'étiez': 1222, 'maréchal': 1223, 'joyeux': 1224, 'absence': 1225, 'plaindre': 1226, 'suivait': 1227, 'serais': 1228, 'regarder': 1229, 'soixante': 1230, 'écus': 1231, 'rendu': 1232, 'apparaître': 1233, 'quitte': 1234, 'amoureux': 1235, 'annoncer': 1236, 'village': 1237, 'semblaient': 1238, 'misérable': 1239, 'paraître': 1240, 'dirai': 1241, 'retomba': 1242, 'vive': 1243, 'remarqué': 1244, 'jeter': 1245, 'gros': 1246, 'vengeance': 1247, 'lut': 1248, 'direction': 1249, 'fiançailles': 1250, 'couché': 1251, 'çà': 1252, 'certes': 1253, 'objets': 1254, 'disparaître': 1255, 'prévenu': 1256, 'jambes': 1257, 'onze': 1258, 'convives': 1259, 'fussent': 1260, 'pouvaient': 1261, 'secours': 1262, 'disparut': 1263, 'davantage': 1264, 'aurais': 1265, 'ensuite': 1266, 'chute': 1267, 'manière': 1268, 'lieues': 1269, 'duel': 1270, 'bourreau': 1271, 'ouvert': 1272, 'aimait': 1273, 'quitté': 1274, 'éloigné': 1275, 'position': 1276, 'désir': 1277, 'absolument': 1278, 'réellement': 1279, 'portefeuille': 1280, 'repos': 1281, 'celles': 1282, 'signes': 1283, 'retira': 1284, 'idées': 1285, 'sac': 1286, 'toile': 1287, 'habitué': 1288, 'continuer': 1289, 'quarante': 1290, 'romains': 1291, 'pont': 1292, 'carconte': 1293, 'naples': 1294, 'appartient': 1295, 'ancre': 1296, 'sauta': 1297, 'commencement': 1298, 'honnête': 1299, 'suivant': 1300, 'sien': 1301, 'perdue': 1302, 'présence': 1303, 'appelé': 1304, 'ferrajo': 1305, 'remis': 1306, 'ouverte': 1307, 'revenait': 1308, 'épaule': 1309, 'confiance': 1310, 'revoir': 1311, 'conduit': 1312, 'prend': 1313, 'joues': 1314, 'bah': 1315, 'sois': 1316, 'changer': 1317, 'blanches': 1318, 'hâte': 1319, 'légère': 1320, 'pittoresque': 1321, 'soldat': 1322, 'pensées': 1323, 'lu': 1324, 'court': 1325, 'boire': 1326, 'porta': 1327, 'rappela': 1328, 'trouverez': 1329, 'lever': 1330, 'trouvez': 1331, 'releva': 1332, 'écrivit': 1333, 'portée': 1334, 'tourna': 1335, 'mission': 1336, 'générale': 1337, 'sommeil': 1338, 'servi': 1339, 'surprise': 1340, 'montra': 1341, 'efforts': 1342, 'interrogatoire': 1343, 'porter': 1344, 'descendu': 1345, 'effectivement': 1346, 'jusque': 1347, 'aurez': 1348, 'royaliste': 1349, 'société': 1350, 'sortait': 1351, 'politique': 1352, 'lesquelles': 1353, 'côtes': 1354, 'sainte': 1355, 'supplice': 1356, 'petits': 1357, 'couper': 1358, 'bureau': 1359, 'salon': 1360, 'frappé': 1361, 'mouvements': 1362, 'jure': 1363, 'promis': 1364, 'reprendre': 1365, 'blessé': 1366, 'serment': 1367, 'abîme': 1368, 'referma': 1369, 'aspect': 1370, 'ténèbres': 1371, 'gendarme': 1372, 'regardez': 1373, 'destinée': 1374, 'disposition': 1375, 'auparavant': 1376, 'courut': 1377, 'échange': 1378, 'toilette': 1379, 'changement': 1380, 'base': 1381, 'prisons': 1382, 'évident': 1383, 'écouta': 1384, 'écoulèrent': 1385, 'imagination': 1386, 'manquait': 1387, 'assurer': 1388, 'morts': 1389, 'soupe': 1390, 'faute': 1391, 'balle': 1392, 'habits': 1393, 'venise': 1394, 'foyer': 1395, 'magnifique': 1396, '15': 1397, 'pistolets': 1398, 'ceinture': 1399, 'pâtre': 1400, 'damas': 1401, 'andrea': 1402, 'pilote': 1403, 'hauteur': 1404, 'obéit': 1405, 'manœuvre': 1406, 'envers': 1407, 'motif': 1408, 'adressant': 1409, 'envoyé': 1410, 'papiers': 1411, 'tient': 1412, 'enfermé': 1413, 'probable': 1414, 'patience': 1415, 'retenir': 1416, 'pâlissant': 1417, 'auriez': 1418, 'obligé': 1419, 'café': 1420, 'coffre': 1421, 'basse': 1422, 'tailleur': 1423, 'habit': 1424, 'agréable': 1425, 'poignée': 1426, 'fâché': 1427, 'probabilité': 1428, 'devenir': 1429, 'belles': 1430, 'services': 1431, 'arrivés': 1432, 'inconnue': 1433, 'venaient': 1434, 'espagnol': 1435, 'voit': 1436, 'pareils': 1437, 'particulière': 1438, 'interroger': 1439, 'mérite': 1440, 'songez': 1441, 'ferait': 1442, 'poignard': 1443, 'chercha': 1444, 'étions': 1445, 'tendit': 1446, 'assit': 1447, 'offrir': 1448, 'empêcher': 1449, 'crier': 1450, 'disais': 1451, 'ivre': 1452, 'frappa': 1453, 'supériorité': 1454, 'soutenir': 1455, 'comprenait': 1456, 'vingtaine': 1457, 'pourra': 1458, 'écrit': 1459, 'marins': 1460, 'charmant': 1461, 'événement': 1462, 'saisit': 1463, 'sourd': 1464, 'enlevé': 1465, 'douce': 1466, 'situation': 1467, 'pria': 1468, 'prières': 1469, 'pointe': 1470, 'découvert': 1471, 'apporte': 1472, 'innocent': 1473, 'conduire': 1474, 'oublier': 1475, 'salvieux': 1476, 'lors': 1477, 'toucher': 1478, 'prévenir': 1479, 'douleurs': 1480, 'magistrat': 1481, 'lutte': 1482, 'léger': 1483, 'écoutait': 1484, 'relations': 1485, 'sombres': 1486, 'dernières': 1487, 'assuré': 1488, 'étendit': 1489, 'barreaux': 1490, 'descendait': 1491, 'brise': 1492, 'camarades': 1493, 'carabine': 1494, 'canon': 1495, 'cruche': 1496, 'changé': 1497, 'devenait': 1498, 'puisse': 1499, 'peuvent': 1500, 'précieux': 1501, 'craignant': 1502, 'note': 1503, 'sujet': 1504, 'entrez': 1505, 'tempête': 1506, 'ouverts': 1507, 'approchait': 1508, 'couvert': 1509, 'su': 1510, 'ressources': 1511, 'traces': 1512, 'trace': 1513, 'moyens': 1514, 'décidé': 1515, 'quoiqu’': 1516, 'secondes': 1517, 'pièce': 1518, 'reparut': 1519, 'providence': 1520, 'rayon': 1521, 'humaine': 1522, 'cinquième': 1523, 'cercle': 1524, 'plan': 1525, 'nuages': 1526, 'science': 1527, 'souleva': 1528, 'instrument': 1529, 'casserole': 1530, 'retard': 1531, 'barbe': 1532, 'ignorait': 1533, 'mystérieux': 1534, 'cheval': 1535, 'fameux': 1536, 'cardinaux': 1537, 'riches': 1538, 'neveu': 1539, 'valeur': 1540, 'montait': 1541, 'condamnés': 1542, 'flots': 1543, 'azur': 1544, 'gouvernail': 1545, 'hatchis': 1546, 'excellences': 1547, 'via': 1548, 'beppo': 1549, 'théâtre': 1550, 'allaient': 1551, 'boulet': 1552, 'conseil': 1553, 'partout': 1554, 'connaître': 1555, 'approchant': 1556, 'intention': 1557, 'servait': 1558, 'paquet': 1559, 'remettre': 1560, 'manqué': 1561, 'jolie': 1562, 'maîtresse': 1563, 'refuser': 1564, 'caractère': 1565, 'occupait': 1566, 'ayez': 1567, 'payer': 1568, 'prends': 1569, 'revenu': 1570, 'bonté': 1571, 'objet': 1572, 'vraiment': 1573, 'opinion': 1574, 'salua': 1575, 'trompe': 1576, 'peau': 1577, 'fasse': 1578, 'certains': 1579, 'entendait': 1580, 'répondez': 1581, 'misère': 1582, 'appeler': 1583, 'tournant': 1584, 'savais': 1585, 'devint': 1586, 'insensé': 1587, 'poussant': 1588, 'envie': 1589, 'ivresse': 1590, 'siège': 1591, 'masse': 1592, 'seront': 1593, 'paraissez': 1594, 'montrant': 1595, 'religion': 1596, 'arrêtant': 1597, 'trouvera': 1598, 'étage': 1599, 'devaient': 1600, 'accordé': 1601, 'courait': 1602, 'poudre': 1603, 'tuileries': 1604, 'membres': 1605, 'montant': 1606, 'retirer': 1607, 'gouttes': 1608, 'appuya': 1609, 'passée': 1610, 'façons': 1611, 'irai': 1612, 'sortant': 1613, 'enchanté': 1614, 'restauration': 1615, 'politiques': 1616, 'lieue': 1617, 'fallu': 1618, 'vouloir': 1619, 'preuves': 1620, 'vigueur': 1621, 'singulière': 1622, 'adressée': 1623, 'apportait': 1624, 'apercevant': 1625, 'pénétrer': 1626, 'naturel': 1627, 'aider': 1628, 'accomplir': 1629, 'reçut': 1630, 'cheminée': 1631, 'acte': 1632, 'monument': 1633, 'seuil': 1634, 'lourd': 1635, 'paru': 1636, 'serrure': 1637, 'descendirent': 1638, 'ailes': 1639, 'percer': 1640, 'adresser': 1641, 'vivre': 1642, 'nourriture': 1643, 'décidément': 1644, 'repris': 1645, 'prier': 1646, 'change': 1647, 'auprès': 1648, 'détacha': 1649, 'échapper': 1650, 'vivant': 1651, 'lampe': 1652, 'souvenirs': 1653, 'envoyer': 1654, 'certitude': 1655, 'recevoir': 1656, 'remonta': 1657, 'jacques': 1658, 'faux': 1659, 'importance': 1660, 'poids': 1661, 'puissance': 1662, 'profondément': 1663, 'échappé': 1664, 'recherches': 1665, 'nécessaire': 1666, 'courte': 1667, 'donnaient': 1668, 'surface': 1669, 'sauver': 1670, 'retrouva': 1671, 'placé': 1672, 'frais': 1673, 'dangereux': 1674, 'écouter': 1675, 'précautions': 1676, '27': 1677, 'christ': 1678, 'aventure': 1679, 'mouchoir': 1680, 'pirates': 1681, 'éloigner': 1682, 'longues': 1683, 'alexandre': 1684, 'granit': 1685, 'ouvrage': 1686, 'souterrain': 1687, 'raconta': 1688, 'aie': 1689, 'lune': 1690, 'accès': 1691, 'grottes': 1692, 'pierreries': 1693, 'déserte': 1694, 'ombres': 1695, 'pantalon': 1696, 'transport': 1697, 'gaspard': 1698, 'cavalier': 1699, 'soie': 1700, 'billet': 1701, 'paysannes': 1702, 'transtévère': 1703, 'voitures': 1704, '1815': 1705, 'jean': 1706, 'creusé': 1707, 'accident': 1708, 'tristesse': 1709, 'content': 1710, 'tombé': 1711, 'titre': 1712, 'croit': 1713, 'juger': 1714, 'questions': 1715, 'entr’': 1716, 'fier': 1717, 'mourant': 1718, 'faudra': 1719, 'disent': 1720, 'génie': 1721, 'habitait': 1722, 'tremblant': 1723, 'inquiet': 1724, 'fît': 1725, 'phrase': 1726, 'noire': 1727, 'pourrais': 1728, 'catalane': 1729, 'environs': 1730, 'nue': 1731, 'conservé': 1732, 'auxquelles': 1733, 'couleur': 1734, 'débris': 1735, 'jambe': 1736, 'loi': 1737, 'amitié': 1738, 'pêcheur': 1739, 'rêves': 1740, 'bleue': 1741, 'rage': 1742, 'mauvaises': 1743, 'pouvant': 1744, 'recula': 1745, 'tiens': 1746, 'poussé': 1747, 'imbécile': 1748, 'numéro': 1749, 'mariage': 1750, 'meure': 1751, 'instruments': 1752, 'aise': 1753, 'marcher': 1754, 'vagues': 1755, 'éclairée': 1756, 'désirs': 1757, 'eurent': 1758, 'auraient': 1759, 'auxquels': 1760, 'douceur': 1761, 'manche': 1762, 'ça': 1763, 'mars': 1764, 'condition': 1765, 'ignore': 1766, 'sourde': 1767, 'officier': 1768, 'probablement': 1769, 'rapporte': 1770, 'restèrent': 1771, 'arrestation': 1772, 'caisse': 1773, 'répéta': 1774, 'pensait': 1775, 'garder': 1776, 'acteurs': 1777, 'porté': 1778, 'naturellement': 1779, 'prière': 1780, 'prononcé': 1781, 'valet': 1782, 'lettres': 1783, 'masque': 1784, 'semblable': 1785, 'gérard': 1786, 'politesse': 1787, 'traversa': 1788, 'impression': 1789, 'douloureux': 1790, 'pensa': 1791, 'têtes': 1792, 'pensez': 1793, 'supérieur': 1794, 'indicible': 1795, 'inutilement': 1796, 'éteindre': 1797, 'ardente': 1798, 'bal': 1799, 'rendait': 1800, 'fixés': 1801, 'deviner': 1802, 'apparut': 1803, 'mine': 1804, 'promenade': 1805, 'risquer': 1806, 'commencé': 1807, 'essayait': 1808, 'informer': 1809, 'action': 1810, 'exécutés': 1811, 'rentré': 1812, 'désespéré': 1813, 'poussière': 1814, 'travailler': 1815, 'interrompit': 1816, 'nouvel': 1817, 'donnerai': 1818, 'met': 1819, 'conséquence': 1820, 'méditerranée': 1821, 'soulever': 1822, 'emporter': 1823, 'quesnel': 1824, 'résultat': 1825, 'victime': 1826, 'termes': 1827, 'admiration': 1828, 'présentait': 1829, 'montrer': 1830, 'évidemment': 1831, 'petites': 1832, 'envoya': 1833, 'ancien': 1834, 'énorme': 1835, 'plâtre': 1836, 'réputation': 1837, 'ange': 1838, 'évasion': 1839, 'solitude': 1840, 'désert': 1841, 'tiré': 1842, 'apercevoir': 1843, 'trompait': 1844, 'dura': 1845, 'faible': 1846, 'frapper': 1847, 'répondait': 1848, 'assiette': 1849, 'mettait': 1850, 'introduisit': 1851, 'philosophie': 1852, 'avouer': 1853, 'opération': 1854, 'indiquait': 1855, 'armoire': 1856, 'nuits': 1857, 'conséquent': 1858, 'livre': 1859, 'testament': 1860, 'offrit': 1861, 'bréviaire': 1862, 'retrouver': 1863, 'ruine': 1864, 'honneurs': 1865, 'barre': 1866, 'quatorze': 1867, 'accepter': 1868, 'perles': 1869, 'chasse': 1870, 'myrtes': 1871, 'chevreau': 1872, 'meilhan': 1873, 'voyageurs': 1874, 'lieutenant': 1875, 'compliments': 1876, 'procurer': 1877, 'mardi': 1878, 'romain': 1879, 'berger': 1880, 'rançon': 1881, 'palestrina': 1882, 'forêt': 1883, 'costumes': 1884, 'masques': 1885, 'smyrne': 1886, 'instinct': 1887, 'atteint': 1888, 'ordonna': 1889, 'exécuter': 1890, 'interlocuteur': 1891, 'comptable': 1892, 'désire': 1893, 'pur': 1894, 'flamme': 1895, 'mettez': 1896, 'eusse': 1897, 'verrez': 1898, 'permettez': 1899, 'passant': 1900, 'hésitation': 1901, 'marier': 1902, 'semaines': 1903, 'espérances': 1904, 'remercie': 1905, 'proposition': 1906, 'rapidement': 1907, 'barques': 1908, 'quai': 1909, 'laissons': 1910, 'située': 1911, 'cherche': 1912, 'balbutia': 1913, 'monnaie': 1914, 'bienvenu': 1915, 'voisins': 1916, 'meilleure': 1917, 'personnage': 1918, 'offert': 1919, 'brun': 1920, 'ardent': 1921, 'indiqué': 1922, 'bouteille': 1923, 'nu': 1924, 'bâtiments': 1925, 'fidèles': 1926, 'dehors': 1927, 'héritage': 1928, 'file': 1929, 'promettre': 1930, 'boutons': 1931, 'chacune': 1932, 'poings': 1933, 'semblent': 1934, 'meilleur': 1935, 'pressé': 1936, 'meilleurs': 1937, 'heureuse': 1938, 'tâche': 1939, 'espagnols': 1940, 'regret': 1941, 'pousser': 1942, 'excellente': 1943, 'bouteilles': 1944, 'manquer': 1945, 'grec': 1946, 'erreur': 1947, 'cherchant': 1948, 'désirez': 1949, 'déposa': 1950, 'retomber': 1951, 'songe': 1952, 'puisqu’': 1953, 'troupe': 1954, 'pauvres': 1955, 'côtés': 1956, 'larges': 1957, 'frissonner': 1958, 'entendez': 1959, 'facilement': 1960, 'îles': 1961, 'orage': 1962, 'difficultés': 1963, 'explosion': 1964, 'promenait': 1965, 'partons': 1966, 'retentit': 1967, 'avançant': 1968, 'arrêt': 1969, 'renseignements': 1970, 'portière': 1971, 'rencontre': 1972, 'manières': 1973, 'pensé': 1974, 'remarquer': 1975, 'remercier': 1976, 'débarquement': 1977, 'tournure': 1978, 'armée': 1979, 'terribles': 1980, 'langues': 1981, 'différentes': 1982, 'bouquets': 1983, 'soient': 1984, 'girondin': 1985, 'prétexte': 1986, 'alliance': 1987, 'présente': 1988, 'sérieusement': 1989, 'accusé': 1990, 'finit': 1991, 'crédit': 1992, 'appuyé': 1993, 'prince': 1994, 'élégant': 1995, 'secrétaire': 1996, 'demeurer': 1997, 'frissonna': 1998, 'jetait': 1999, 'prononcer': 2000, 'habile': 2001, 'glace': 2002, 'conspiration': 2003, 'savons': 2004, 'liasse': 2005, 'briller': 2006, 'intérieur': 2007, 'continuez': 2008, 'préviens': 2009, 'rude': 2010, 'violente': 2011, 'maladie': 2012, 'violent': 2013, 'escabeau': 2014, 'plaça': 2015, 'résistance': 2016, 'rouler': 2017, 'fusils': 2018, 'marcha': 2019, 'tentative': 2020, 'honte': 2021, 'songeait': 2022, 'juges': 2023, 'mettant': 2024, 'corde': 2025, 'posé': 2026, 'songé': 2027, 'tourné': 2028, 'reflet': 2029, 'animal': 2030, 'sauvage': 2031, 'gagner': 2032, 'marches': 2033, 'partez': 2034, 'entretien': 2035, 'minute': 2036, 'rang': 2037, 'carrière': 2038, 'sonna': 2039, 'appuyée': 2040, 'doigt': 2041, 'remonter': 2042, 'guides': 2043, 'bonaparte': 2044, 'sécurité': 2045, 'date': 2046, 'dos': 2047, 'salut': 2048, 'aigle': 2049, 'tenant': 2050, 'proie': 2051, 'rapidité': 2052, 'arme': 2053, 'prochain': 2054, 'retenu': 2055, 'redingote': 2056, 'exactement': 2057, 'quels': 2058, 'laissée': 2059, 'défaut': 2060, 'fuite': 2061, 'annonça': 2062, 'pétition': 2063, 'rendit': 2064, 'puissante': 2065, 'prenait': 2066, 'assise': 2067, 'souffrir': 2068, 'quelles': 2069, 'réflexion': 2070, 'florence': 2071, 'offrait': 2072, 'distraction': 2073, 'sublime': 2074, 'cloche': 2075, 'intervalles': 2076, 'replaça': 2077, 'levier': 2078, 'inerte': 2079, 'portion': 2080, 'chaleur': 2081, 'vi': 2082, 'hôtes': 2083, 'carré': 2084, 'réfléchit': 2085, 'linge': 2086, 'examina': 2087, 'pouces': 2088, 'écrite': 2089, 'parfaite': 2090, 'certaines': 2091, 'récit': 2092, 'ouvrait': 2093, 'inviter': 2094, 'pourrons': 2095, 'liqueur': 2096, 'tendant': 2097, 'propriétaire': 2098, 'sainteté': 2099, 'rospigliosi': 2100, 'charmante': 2101, 'mourut': 2102, 'train': 2103, 'pendule': 2104, 'cou': 2105, 'comptait': 2106, 'distinguait': 2107, 'sommet': 2108, 'chèvres': 2109, 'invitation': 2110, 'course': 2111, 'bruyères': 2112, 'genre': 2113, 'atmosphère': 2114, 'tirant': 2115, 'italiens': 2116, 'beaucaire': 2117, 'helder': 2118, '5': 2119, 'cicerone': 2120, 'montagne': 2121, 'corses': 2122, 'carabiniers': 2123, 'servis': 2124, 'statues': 2125, 'peaux': 2126, 'intendant': 2127, 'paysanne': 2128, 'galop': 2129, 'grecque': 2130, 'aborder': 2131, 'avançait': 2132, 'néanmoins': 2133, 'diriger': 2134, 'spectateurs': 2135, 'anse': 2136, 'holà': 2137, 'ordinaires': 2138, 'ronde': 2139, 'amène': 2140, 'commandement': 2141, 'marchant': 2142, 'métier': 2143, 'jetant': 2144, 'voulais': 2145, 'suivie': 2146, 'acheter': 2147, 'appartenait': 2148, 'oncle': 2149, 'éloigna': 2150, 'rapprocha': 2151, 'bonnes': 2152, 'excusez': 2153, 'reconnaissant': 2154, 'prouve': 2155, 'excepté': 2156, 'prendrez': 2157, 'guère': 2158, 'querelle': 2159, 'avions': 2160, 'glissa': 2161, 'attendais': 2162, 'conte': 2163, 'vécu': 2164, 'bonhomme': 2165, 'pièces': 2166, 'qualité': 2167, 'rassurer': 2168, 'banquier': 2169, 'important': 2170, 'vieilles': 2171, 'apporter': 2172, 'branches': 2173, 'joyeuse': 2174, 'mystérieuse': 2175, 'feuille': 2176, 'morte': 2177, 'teinte': 2178, 'antique': 2179, 'fleurs': 2180, 'coins': 2181, 'gris': 2182, 'fixe': 2183, 'noce': 2184, 'reprocher': 2185, 'franchise': 2186, 'oubliez': 2187, 'vendre': 2188, 'secouant': 2189, 'devient': 2190, 'répète': 2191, 'tenter': 2192, 'chemise': 2193, 'veste': 2194, 'cruelle': 2195, 'baissa': 2196, 'ressemblait': 2197, 'gémissement': 2198, 'relevant': 2199, 'pénétrait': 2200, 'sourcil': 2201, 'abandonner': 2202, 'statue': 2203, 'menaçant': 2204, 'quittant': 2205, 'genou': 2206, 'serions': 2207, 'prévu': 2208, 'tonnelle': 2209, 'épuisé': 2210, 'nez': 2211, 'pêcheurs': 2212, 'parlant': 2213, 'avala': 2214, 'commence': 2215, 'aimer': 2216, 'tranquillement': 2217, 'raisonnable': 2218, 'fiancé': 2219, 'souffert': 2220, 'agir': 2221, 'aille': 2222, 'montent': 2223, 'nation': 2224, 'disiez': 2225, 'agit': 2226, 'tranquilles': 2227, 'pistolet': 2228, 'accusation': 2229, 'éternellement': 2230, 'préparé': 2231, 'villes': 2232, 'futurs': 2233, 'arrivant': 2234, 'braves': 2235, 'achevé': 2236, 'essayé': 2237, 'velours': 2238, 'conduisait': 2239, 'placer': 2240, 'fantaisie': 2241, 'viennent': 2242, 'perdit': 2243, 'vœu': 2244, 'méprise': 2245, 'froide': 2246, 'précipita': 2247, 'occuper': 2248, 'précédé': 2249, 'restés': 2250, 'soutenu': 2251, 'eussiez': 2252, 'gardé': 2253, 'sienne': 2254, 'pouvons': 2255, 'pareilles': 2256, 'aristocratique': 2257, 'rangs': 2258, 'traité': 2259, 'triomphant': 2260, 'élégante': 2261, 'vont': 2262, 'égalité': 2263, 'colonne': 2264, 'rois': 2265, 'dignes': 2266, 'goût': 2267, 'restée': 2268, 'bravo': 2269, 'né': 2270, 'règne': 2271, 'éloquence': 2272, 'jouer': 2273, 'joué': 2274, 'moyennant': 2275, 'instants': 2276, 'découvrir': 2277, 'dossier': 2278, 'pourrai': 2279, 'familles': 2280, 'entière': 2281, 'million': 2282, 'rencontrer': 2283, 'noble': 2284, 'compagnie': 2285, 'rencontra': 2286, 'notes': 2287, 'prévenus': 2288, 'légèrement': 2289, 'surpris': 2290, 'allais': 2291, 'rend': 2292, 'appela': 2293, 'circonstances': 2294, 'atteindre': 2295, 'lisait': 2296, 'glacée': 2297, 'viendra': 2298, 'inclina': 2299, 'chêne': 2300, 'exempt': 2301, 'lumières': 2302, 'suivirent': 2303, 'regardèrent': 2304, 'bateau': 2305, 'passèrent': 2306, 'attaché': 2307, 'anéanti': 2308, 'regardaient': 2309, 'roche': 2310, 'murs': 2311, 'finir': 2312, 'degrés': 2313, 'machinalement': 2314, 'dormir': 2315, 'secoua': 2316, 'cachots': 2317, 'fous': 2318, 'murmurant': 2319, 'rentrant': 2320, 'passons': 2321, 'plaît': 2322, 'gravité': 2323, 'approche': 2324, 'ambition': 2325, 'blessure': 2326, 'séparait': 2327, 'tenu': 2328, 'jugement': 2329, 'éteint': 2330, 'craignait': 2331, 'horace': 2332, 'dandré': 2333, 'occupé': 2334, 'fatigue': 2335, 'haleine': 2336, 'empêchait': 2337, 'complet': 2338, 'soulevant': 2339, 'respect': 2340, 'acquis': 2341, 'voie': 2342, 'assassinat': 2343, 'épais': 2344, 'légion': 2345, 'fatigué': 2346, 'telle': 2347, 'ceci': 2348, 'passées': 2349, 'capitale': 2350, 'vôtre': 2351, 'bords': 2352, 'prudence': 2353, 'informe': 2354, 'puissant': 2355, 'nombreux': 2356, 'assurance': 2357, 'rappelez': 2358, 'rencontré': 2359, 'trouvés': 2360, 'oiseau': 2361, 'folle': 2362, 'plafond': 2363, 'chambres': 2364, 'inutiles': 2365, 'capable': 2366, 'manque': 2367, 'noms': 2368, 'enfer': 2369, 'millionnaire': 2370, 'couverture': 2371, 'gigantesque': 2372, 'assister': 2373, 'observateur': 2374, 'meurs': 2375, 'désirait': 2376, 'prudent': 2377, 'éclatant': 2378, 'géant': 2379, 'cessé': 2380, 'vivres': 2381, 'écoula': 2382, 'pression': 2383, 'distance': 2384, 'satisfaction': 2385, 'penser': 2386, 'étendant': 2387, 'résolut': 2388, 'fragments': 2389, 'creuser': 2390, 'excavation': 2391, 'coucha': 2392, 'pratiquée': 2393, 'échelle': 2394, 'glisser': 2395, 'extérieure': 2396, 'dangers': 2397, 'évasions': 2398, 'plumes': 2399, 'exprimer': 2400, 'opposée': 2401, 'sauvages': 2402, 'brûlé': 2403, 'aiguille': 2404, 'meurt': 2405, 'parlons': 2406, 'unes': 2407, 'aurons': 2408, 'nageur': 2409, 'caractères': 2410, 'lisez': 2411, 'trouvèrent': 2412, 'vigne': 2413, 'pâlit': 2414, 'approcher': 2415, 'arbre': 2416, 'pianosa': 2417, 'ôter': 2418, 'commencèrent': 2419, 'déclara': 2420, 'odeur': 2421, 'fossoyeurs': 2422, 'civière': 2423, 'plaine': 2424, 'fugitif': 2425, 'paysan': 2426, 'chaloupe': 2427, 'gourde': 2428, 'fermés': 2429, 'fumée': 2430, 'projets': 2431, 'cigares': 2432, 'placée': 2433, 'renoncer': 2434, 'étendue': 2435, 'arbres': 2436, 'boucles': 2437, 'singulier': 2438, 'septembre': 2439, 'mandataire': 2440, 'embarras': 2441, 'préoccupé': 2442, 'londres': 2443, 'magnifiques': 2444, 'mariniers': 2445, 'places': 2446, 'hospitalité': 2447, 'tendue': 2448, 'charmantes': 2449, 'fête': 2450, 'italienne': 2451, 'paysans': 2452, 'rocca': 2453, 'confrérie': 2454, 'argentina': 2455, 'corso': 2456, 'moccoletto': 2457, 'bracciano': 2458, 'venant': 2459, 'secousse': 2460, 'mouillage': 2461, 'apprêtait': 2462, 'étroite': 2463, 'beaux': 2464, 'enfance': 2465, 'civita': 2466, 'chargement': 2467, 'visiblement': 2468, 'fièvre': 2469, 'épée': 2470, 'cherchait': 2471, 'conseils': 2472, 'oblique': 2473, 'consulter': 2474, 'directement': 2475, 'caprice': 2476, 'éloignait': 2477, 'déposé': 2478, 'déposer': 2479, 'loisir': 2480, 'solde': 2481, 'saisissant': 2482, 'eue': 2483, 'canot': 2484, 'chance': 2485, 'sauter': 2486, 'différence': 2487, 'retenant': 2488, 'monté': 2489, 'tremblante': 2490, 'irait': 2491, 'laissés': 2492, 'déchiré': 2493, 'bel': 2494, 'poches': 2495, 'douzaine': 2496, 'doucement': 2497, 'chut': 2498, 'revers': 2499, 'rares': 2500, 'remarqua': 2501, 'tôt': 2502, 'restera': 2503, 'fidèle': 2504, 'tes': 2505, 'serons': 2506, 'verrons': 2507, 'desquels': 2508, 'oiseaux': 2509, 'aride': 2510, 'antiques': 2511, 'pères': 2512, 'unique': 2513, 'maisons': 2514, 'coude': 2515, 'époux': 2516, 'ruines': 2517, 'utile': 2518, 'accepte': 2519, 'tire': 2520, 'rapports': 2521, 'mienne': 2522, 'attends': 2523, 'vaincu': 2524, 'sœur': 2525, 'impassible': 2526, 'froidement': 2527, 'nul': 2528, 'serpent': 2529, 'flot': 2530, 'reconnaissez': 2531, 'colère': 2532, 'sinistre': 2533, 'briser': 2534, 'berceau': 2535, 'aimable': 2536, 'rival': 2537, 'ajouté': 2538, 'taire': 2539, 'menace': 2540, 'mourait': 2541, 'devrait': 2542, 'aveugle': 2543, 'assure': 2544, 'égoïsme': 2545, 'élus': 2546, 'remède': 2547, 'pis': 2548, 'suffit': 2549, 'dise': 2550, 'sentiments': 2551, 'semblant': 2552, 'vouliez': 2553, 'renversée': 2554, 'habituelle': 2555, 'cabine': 2556, 'lecture': 2557, 'entraîner': 2558, 'allongea': 2559, 'demeuré': 2560, 'régnait': 2561, 'noces': 2562, 'choix': 2563, 'voyaient': 2564, 'blancs': 2565, 'premières': 2566, 'exception': 2567, 'maire': 2568, 'ferma': 2569, 'propres': 2570, 'vivait': 2571, 'choc': 2572, 'bouleversé': 2573, 'livide': 2574, 'rumeur': 2575, 'porteur': 2576, 'ému': 2577, 'muette': 2578, 'formalité': 2579, 'évanoui': 2580, 'répugnance': 2581, 'avantage': 2582, 'enragé': 2583, 'ambitieux': 2584, 'ressemble': 2585, 'mêlé': 2586, 'officiers': 2587, 'levèrent': 2588, 'enthousiasme': 2589, 'monarchie': 2590, 'souffle': 2591, 'bonapartistes': 2592, 'type': 2593, 'séparé': 2594, 'obtenir': 2595, 'convenu': 2596, 'profession': 2597, 'sévère': 2598, 'royalistes': 2599, 'vieil': 2600, 'delà': 2601, 'regardé': 2602, 'promettez': 2603, 'indulgence': 2604, 'robe': 2605, 'profondeur': 2606, 'rôle': 2607, 'demandait': 2608, 'compris': 2609, 'remarquez': 2610, 'ferais': 2611, 'vienne': 2612, 'voleurs': 2613, 'teint': 2614, 'favoris': 2615, 'complot': 2616, 'absent': 2617, 'herbe': 2618, 'destin': 2619, 'physionomie': 2620, 'beauté': 2621, 'augmenter': 2622, 'intérieure': 2623, 'appartenant': 2624, 'inouïe': 2625, 'savant': 2626, 'toutefois': 2627, 'songer': 2628, 'voulaient': 2629, 'veulent': 2630, 'moments': 2631, 'revenant': 2632, 'poser': 2633, 'affection': 2634, 'tel': 2635, 'délire': 2636, 'comptais': 2637, 'annonçait': 2638, 'existe': 2639, 'sonnette': 2640, 'fixes': 2641, 'passent': 2642, 'hésitait': 2643, 'conduisit': 2644, 'verrous': 2645, 'ouvrant': 2646, 'tenaient': 2647, 'explication': 2648, 'étranges': 2649, 'pensif': 2650, 'phare': 2651, 'éclairait': 2652, 'retint': 2653, 'éprouvait': 2654, 'geôliers': 2655, 'plancher': 2656, 'abattu': 2657, 'roc': 2658, 'passaient': 2659, 'reflets': 2660, 'logement': 2661, 'parois': 2662, 'affreux': 2663, 'séjour': 2664, 'montrait': 2665, 'cachait': 2666, 'humide': 2667, 'toscan': 2668, 'permis': 2669, 'offrant': 2670, 'excuser': 2671, 'intermédiaire': 2672, 'repoussa': 2673, 'mortel': 2674, 'retentissant': 2675, 'trembler': 2676, 'fermée': 2677, 'risque': 2678, 'bondit': 2679, 'retrouvé': 2680, 'présenté': 2681, 'fantastique': 2682, 'mise': 2683, 'serviteur': 2684, 'provence': 2685, 'satisfait': 2686, 'ordonne': 2687, 'serviteurs': 2688, 'entières': 2689, 'héros': 2690, 'gorge': 2691, 'cailloux': 2692, 'gagné': 2693, 'volontiers': 2694, 'vainement': 2695, 'télégraphe': 2696, 'employé': 2697, 'tenue': 2698, 'introduit': 2699, 'piombino': 2700, 'rassurez': 2701, 'comptez': 2702, 'orgueil': 2703, 'flottait': 2704, 'couronne': 2705, 'soupçon': 2706, 'agonie': 2707, 'club': 2708, 'sourcils': 2709, 'individu': 2710, 'trahir': 2711, 'récompense': 2712, 'rappeler': 2713, 'fiacre': 2714, 'timbre': 2715, 'visibles': 2716, 'sauve': 2717, 'bâtir': 2718, 'poursuit': 2719, 'cravate': 2720, 'posée': 2721, 'obéissance': 2722, 'tranquillité': 2723, 'empire': 2724, 'lancé': 2725, 'sentir': 2726, 'examen': 2727, 'réclamer': 2728, 'éclairé': 2729, 'registres': 2730, 'écrou': 2731, 'venons': 2732, 'transporté': 2733, 'levée': 2734, 'douter': 2735, 'apparition': 2736, 'espérait': 2737, 'moindres': 2738, 'écoutant': 2739, 'valait': 2740, 'donnent': 2741, '1811': 2742, 'gonds': 2743, 'étonna': 2744, 'disposé': 2745, 'rigueur': 2746, 'supplices': 2747, 'examiner': 2748, 'donnera': 2749, 'sauveur': 2750, 'plongea': 2751, 'possédait': 2752, 'réveillé': 2753, 'née': 2754, 'tint': 2755, 'horrible': 2756, 'cessa': 2757, 'commencer': 2758, 'tombait': 2759, 'éducation': 2760, 'gigantesques': 2761, 'éternelle': 2762, 'vol': 2763, 'horreur': 2764, 'punir': 2765, 'pente': 2766, 'naître': 2767, 'vaisseau': 2768, 'faisais': 2769, 'tours': 2770, 'vêtement': 2771, 'ignorance': 2772, 'cesser': 2773, 'ciment': 2774, 'tombant': 2775, 'extrémités': 2776, 'parvint': 2777, 'maigre': 2778, 'obstacle': 2779, 'toucha': 2780, 'tiboulen': 2781, 'amer': 2782, 'voûte': 2783, 'chrétien': 2784, 'descendant': 2785, 'facultés': 2786, 'ciseau': 2787, 'résignation': 2788, 'consoler': 2789, 'failli': 2790, 'réfléchir': 2791, 'extrémité': 2792, 'aient': 2793, 'bougie': 2794, 'épreuve': 2795, 'aperçu': 2796, 'traversé': 2797, 'réuni': 2798, 'raconté': 2799, 'seules': 2800, 'organisation': 2801, 'naturelle': 2802, 'jurer': 2803, 'flacon': 2804, 'dimanche': 2805, 'fixement': 2806, 'égoïste': 2807, 'étude': 2808, 'connus': 2809, 'infinie': 2810, 'couler': 2811, 'crise': 2812, 'visible': 2813, 'étendu': 2814, 'tracés': 2815, 'frisson': 2816, 'richesses': 2817, 'spéculation': 2818, 'nobles': 2819, 'mortelle': 2820, 'déclare': 2821, 'sein': 2822, 'jaune': 2823, 'crique': 2824, 'héritier': 2825, 'romaines': 2826, 'instructions': 2827, 'angoisse': 2828, 'violettes': 2829, 'habituel': 2830, 'devons': 2831, 'humain': 2832, 'subite': 2833, 'emporta': 2834, 'continuaient': 2835, 'amener': 2836, 'falot': 2837, 'porteurs': 2838, 'souliers': 2839, 'retentir': 2840, 'indiqua': 2841, 'étendait': 2842, 'étoiles': 2843, 'bonnet': 2844, 'dirigea': 2845, 'bordées': 2846, 'opérer': 2847, 'maltais': 2848, 'chargés': 2849, 'courir': 2850, 'timonier': 2851, 'devenue': 2852, 'contrebandier': 2853, 'élégance': 2854, 'faculté': 2855, 'engagement': 2856, 'richesse': 2857, 'centaine': 2858, 'tapis': 2859, 'terrain': 2860, 'insista': 2861, 'entailles': 2862, 'indiquer': 2863, 'mai': 2864, 'vaste': 2865, 'anneau': 2866, 'crosse': 2867, 'couvercle': 2868, 'supérieure': 2869, 'demeurait': 2870, 'concierge': 2871, 'auberge': 2872, 'décider': 2873, 'situé': 2874, 'chien': 2875, 'française': 2876, 'gilet': 2877, 'exactitude': 2878, 'caissier': 2879, 'traites': 2880, 'joue': 2881, 'gaumard': 2882, 'délai': 2883, 'traite': 2884, 'mets': 2885, 'merveilleuse': 2886, 'insouciance': 2887, 'préoccupation': 2888, 'divan': 2889, 'tunis': 2890, 'domestiques': 2891, 'vecchio': 2892, 'fêtes': 2893, 'cigare': 2894, 'satisfaire': 2895, 'troupeau': 2896, 'diavolaccio': 2897, 'quadrille': 2898, 'seraient': 2899, 'villa': 2900, 'ballet': 2901, 'tavolette': 2902, 'paillasse': 2903, 'confetti': 2904, 'moccoli': 2905, 'torlonia': 2906, '21': 2907}\n",
      "[[1, 1, 6, 2, 2, 1705], [1, 6, 2, 2, 1705, 3], [6, 2, 2, 1705, 3, 8], [2, 2, 1705, 3, 8, 2], [2, 1705, 3, 8, 2, 4], [1705, 3, 8, 2, 4, 248], [3, 8, 2, 4, 248, 13], [8, 2, 4, 248, 13, 1152], [2, 4, 248, 13, 1152, 4], [4, 248, 13, 1152, 4, 8], [248, 13, 1152, 4, 8, 830], [13, 1152, 4, 8, 830, 2], [1152, 4, 8, 830, 2, 6], [4, 8, 830, 2, 6, 120], [8, 830, 2, 6, 120, 13], [830, 2, 6, 120, 13, 2], [2, 6, 120, 13, 2, 6], [6, 120, 13, 2, 6, 336], [120, 13, 2, 6, 336, 3], [13, 2, 6, 336, 3, 2459], [2, 6, 336, 3, 2459, 4], [6, 336, 3, 2459, 4, 1886], [336, 3, 2459, 4, 1886, 3], [3, 2459, 4, 1886, 3, 2], [2459, 4, 1886, 3, 2, 7], [4, 1886, 3, 2, 7, 1294], [1886, 3, 2, 7, 1294, 5], [3, 2, 7, 1294, 5, 0], [2, 7, 1294, 5, 0, 0], [7, 1294, 5, 0, 0, 1], [1294, 5, 0, 0, 1, 1], [5, 0, 0, 1, 1, 50], [0, 0, 1, 1, 50, 2], [0, 1, 1, 50, 2, 382], [1, 1, 50, 2, 382, 3], [1, 50, 2, 382, 3, 15], [50, 2, 382, 3, 15, 1403], [2, 382, 3, 15, 1403, 2], [382, 3, 15, 1403, 2, 1217], [3, 15, 1403, 2, 1217, 2], [15, 1403, 2, 1217, 2, 32], [1403, 2, 1217, 2, 32, 534], [2, 1217, 2, 32, 534, 3], [1217, 2, 32, 534, 3, 2], [2, 32, 534, 3, 2, 6], [32, 534, 3, 2, 6, 2], [534, 3, 2, 6, 2, 2], [3, 2, 6, 2, 2, 490], [2, 6, 2, 2, 490, 3], [6, 2, 2, 490, 3, 7], [2, 2, 490, 3, 7, 374], [2, 490, 3, 7, 374, 2131], [490, 3, 7, 374, 2131, 6], [3, 7, 374, 2131, 6, 574], [7, 374, 2131, 6, 574, 181], [374, 2131, 6, 574, 181, 6], [2131, 6, 574, 181, 6, 1040], [6, 574, 181, 6, 1040, 4], [574, 181, 6, 1040, 4, 2], [181, 6, 1040, 4, 2, 7], [6, 1040, 4, 2, 7, 2], [1040, 4, 2, 7, 2, 2], [4, 2, 7, 2, 2, 4], [2, 7, 2, 2, 4, 2], [7, 2, 2, 4, 2, 5], [2, 2, 4, 2, 5, 0], [2, 4, 2, 5, 0, 0], [4, 2, 5, 0, 0, 1], [2, 5, 0, 0, 1, 1], [5, 0, 0, 1, 1, 2], [0, 0, 1, 1, 2, 3], [0, 1, 1, 2, 3, 50], [1, 1, 2, 3, 50, 2], [1, 2, 3, 50, 2, 382], [2, 3, 50, 2, 382, 88], [3, 50, 2, 382, 88, 3], [50, 2, 382, 88, 3, 8], [2, 382, 88, 3, 8, 2], [382, 88, 3, 8, 2, 13], [88, 3, 8, 2, 13, 831], [3, 8, 2, 13, 831, 32], [8, 2, 13, 831, 32, 196], [2, 13, 831, 32, 196, 318], [13, 831, 32, 196, 318, 13], [831, 32, 196, 318, 13, 1706], [32, 196, 318, 13, 1706, 2], [196, 318, 13, 1706, 2, 2], [318, 13, 1706, 2, 2, 2], [13, 1706, 2, 2, 2, 4], [1706, 2, 2, 2, 4, 832], [2, 2, 2, 4, 832, 17], [2, 2, 4, 832, 17, 112], [2, 4, 832, 17, 112, 2], [4, 832, 17, 112, 2, 25], [832, 17, 112, 2, 25, 160], [17, 112, 2, 25, 160, 22], [112, 2, 25, 160, 22, 326], [2, 25, 160, 22, 326, 575], [25, 160, 22, 326, 575, 2], [160, 22, 326, 575, 2, 251]]\n",
      "224533\n",
      "torch.Size([224533, 6])\n",
      "[[tensor([ 3.4741e-01, -7.5196e-01,  5.9132e-01, -1.2106e-01,  6.9907e-01,\n",
      "        -7.1380e-01, -8.3328e-01, -6.8692e-01, -7.8697e-01,  1.4951e+00,\n",
      "         3.8787e-01,  9.0744e-01, -1.6725e+00, -4.9250e-01, -1.9552e-01,\n",
      "         1.2953e+00, -2.3788e-02, -3.4488e-01,  5.2960e-01, -9.7020e-02,\n",
      "        -7.4229e-01,  4.6669e-01,  4.6507e-01, -3.1354e-01,  1.7783e-01,\n",
      "        -6.7539e-01, -1.1501e-01, -1.7200e-01,  8.9514e-02,  2.9198e-01,\n",
      "        -7.0343e-01, -7.1737e-01, -1.0621e+00,  2.1012e-01, -2.4800e-01,\n",
      "        -1.5323e-01,  5.2777e-01, -2.2904e-01, -3.7000e-01, -6.8490e-01,\n",
      "        -1.1928e+00, -1.0746e-01,  1.0139e+00, -3.2678e-01,  1.6848e-01,\n",
      "         7.4065e-01,  1.4056e-01,  1.1364e-01, -1.4951e-02,  3.5595e-01,\n",
      "         2.4379e-02,  3.3920e-01, -9.8620e-02, -2.6859e-01, -2.7445e-02,\n",
      "        -1.6242e-01, -7.1941e-01,  3.6980e-02, -1.2822e+00,  3.2500e-02,\n",
      "        -2.4572e-01, -1.5038e-01,  3.0198e-01, -3.0359e-01, -3.4648e-01,\n",
      "         3.0359e-01,  5.7370e-02,  8.0076e-01,  6.8260e-02,  6.7500e-01,\n",
      "         7.3514e-01, -4.1765e-01, -6.6137e-01, -6.0858e-02,  2.5959e-01,\n",
      "        -3.4760e-01, -1.0097e+00, -6.7128e-01,  7.6304e-01, -5.6706e-01,\n",
      "         3.4949e-01,  3.0874e-01, -5.3167e-01, -3.5064e-01,  4.7007e-01,\n",
      "        -6.7388e-02,  3.1092e-02,  6.5207e-02,  1.1254e+00, -5.6591e-01,\n",
      "        -2.6593e-01,  5.0682e-01, -1.2296e+00,  1.2650e+00, -4.6824e-02,\n",
      "         7.3440e-01, -1.4600e-04, -2.4581e-01,  1.5369e-02, -3.6524e-01]), tensor([ 3.4741e-01, -7.5196e-01,  5.9132e-01, -1.2106e-01,  6.9907e-01,\n",
      "        -7.1380e-01, -8.3328e-01, -6.8692e-01, -7.8697e-01,  1.4951e+00,\n",
      "         3.8787e-01,  9.0744e-01, -1.6725e+00, -4.9250e-01, -1.9552e-01,\n",
      "         1.2953e+00, -2.3788e-02, -3.4488e-01,  5.2960e-01, -9.7020e-02,\n",
      "        -7.4229e-01,  4.6669e-01,  4.6507e-01, -3.1354e-01,  1.7783e-01,\n",
      "        -6.7539e-01, -1.1501e-01, -1.7200e-01,  8.9514e-02,  2.9198e-01,\n",
      "        -7.0343e-01, -7.1737e-01, -1.0621e+00,  2.1012e-01, -2.4800e-01,\n",
      "        -1.5323e-01,  5.2777e-01, -2.2904e-01, -3.7000e-01, -6.8490e-01,\n",
      "        -1.1928e+00, -1.0746e-01,  1.0139e+00, -3.2678e-01,  1.6848e-01,\n",
      "         7.4065e-01,  1.4056e-01,  1.1364e-01, -1.4951e-02,  3.5595e-01,\n",
      "         2.4379e-02,  3.3920e-01, -9.8620e-02, -2.6859e-01, -2.7445e-02,\n",
      "        -1.6242e-01, -7.1941e-01,  3.6980e-02, -1.2822e+00,  3.2500e-02,\n",
      "        -2.4572e-01, -1.5038e-01,  3.0198e-01, -3.0359e-01, -3.4648e-01,\n",
      "         3.0359e-01,  5.7370e-02,  8.0076e-01,  6.8260e-02,  6.7500e-01,\n",
      "         7.3514e-01, -4.1765e-01, -6.6137e-01, -6.0858e-02,  2.5959e-01,\n",
      "        -3.4760e-01, -1.0097e+00, -6.7128e-01,  7.6304e-01, -5.6706e-01,\n",
      "         3.4949e-01,  3.0874e-01, -5.3167e-01, -3.5064e-01,  4.7007e-01,\n",
      "        -6.7388e-02,  3.1092e-02,  6.5207e-02,  1.1254e+00, -5.6591e-01,\n",
      "        -2.6593e-01,  5.0682e-01, -1.2296e+00,  1.2650e+00, -4.6824e-02,\n",
      "         7.3440e-01, -1.4600e-04, -2.4581e-01,  1.5369e-02, -3.6524e-01]), tensor([-0.2770,  0.4366, -0.0928,  0.0227, -0.9203, -0.9404, -0.2116, -0.0604,\n",
      "         1.0540,  0.2607, -0.5052,  0.5363, -0.4702,  1.2799,  0.1291, -0.1911,\n",
      "         0.1815,  0.8200,  0.4139, -0.1091, -0.0523,  0.7290,  1.0534, -0.3458,\n",
      "         0.3767,  0.2141,  0.7512,  0.5001, -1.0588, -0.3632,  0.5794,  0.2473,\n",
      "        -0.1537,  0.9696, -0.2082,  0.3509,  1.3823, -0.3980,  1.1288, -0.9694,\n",
      "         0.0098,  0.5108, -0.3707,  1.1038,  0.6557, -0.2339, -0.0154, -0.7066,\n",
      "        -0.4023, -0.5206,  1.2951,  0.1086, -0.0349, -0.9903,  0.3590,  0.3822,\n",
      "        -0.1104, -0.1984, -0.6289, -0.2016, -0.8785,  0.5648, -0.2928, -0.1167,\n",
      "         0.0349,  0.7099,  0.6595, -0.4948, -1.1107, -0.5474, -0.0334,  0.5133,\n",
      "         0.6254,  0.5046, -0.3006,  0.2795,  0.4403,  0.4054, -0.8723, -0.0909,\n",
      "         0.0678,  1.2187,  1.5220, -0.1120, -0.0232,  0.3660, -0.7272,  0.8602,\n",
      "         1.4147,  0.2054, -0.0343,  0.0043,  0.0788, -0.0813, -0.7014,  0.1745,\n",
      "         0.0587, -0.6266,  0.4252, -0.0759]), tensor([-0.7540, -0.4252,  0.8847, -0.5711, -1.1832,  0.4315, -0.1015,  0.3610,\n",
      "         0.6748, -0.2943,  0.0806, -0.3012,  0.3224, -0.3388,  0.2792, -0.3733,\n",
      "        -0.0463,  0.4485,  0.1808,  0.0154, -0.0369,  1.3411,  0.7360, -0.2890,\n",
      "        -0.3320, -0.0946,  0.0738, -0.1439,  0.3266,  0.3589, -0.0649,  0.2675,\n",
      "         0.1096,  0.3682, -1.6357,  0.4843,  0.1966, -0.6430, -0.2285, -0.6177,\n",
      "         0.2770,  0.1185, -0.0947, -0.4767,  0.9916,  0.5090, -0.0052,  1.1076,\n",
      "        -0.0516, -0.3816, -0.3570, -0.0850, -0.4774,  0.5608, -0.6031,  0.6178,\n",
      "         0.3321, -0.1559,  0.1738,  0.3237,  0.8285, -0.0329,  0.0278,  0.1339,\n",
      "         0.2101,  0.4487, -0.3369, -0.4247,  1.6172,  0.4399, -0.5080,  0.9142,\n",
      "         0.1382, -0.9105, -0.2193, -0.3520, -1.1709, -0.4227, -0.3652,  0.3255,\n",
      "         0.5528,  0.3926,  0.3193,  0.1168, -0.5096,  0.0993, -0.4654, -0.4608,\n",
      "         0.7337, -0.5065, -0.4382,  0.5142,  0.2937, -0.4284,  0.6115,  0.1540,\n",
      "         0.1826, -0.3982, -0.5670,  0.1095]), tensor([-0.7540, -0.4252,  0.8847, -0.5711, -1.1832,  0.4315, -0.1015,  0.3610,\n",
      "         0.6748, -0.2943,  0.0806, -0.3012,  0.3224, -0.3388,  0.2792, -0.3733,\n",
      "        -0.0463,  0.4485,  0.1808,  0.0154, -0.0369,  1.3411,  0.7360, -0.2890,\n",
      "        -0.3320, -0.0946,  0.0738, -0.1439,  0.3266,  0.3589, -0.0649,  0.2675,\n",
      "         0.1096,  0.3682, -1.6357,  0.4843,  0.1966, -0.6430, -0.2285, -0.6177,\n",
      "         0.2770,  0.1185, -0.0947, -0.4767,  0.9916,  0.5090, -0.0052,  1.1076,\n",
      "        -0.0516, -0.3816, -0.3570, -0.0850, -0.4774,  0.5608, -0.6031,  0.6178,\n",
      "         0.3321, -0.1559,  0.1738,  0.3237,  0.8285, -0.0329,  0.0278,  0.1339,\n",
      "         0.2101,  0.4487, -0.3369, -0.4247,  1.6172,  0.4399, -0.5080,  0.9142,\n",
      "         0.1382, -0.9105, -0.2193, -0.3520, -1.1709, -0.4227, -0.3652,  0.3255,\n",
      "         0.5528,  0.3926,  0.3193,  0.1168, -0.5096,  0.0993, -0.4654, -0.4608,\n",
      "         0.7337, -0.5065, -0.4382,  0.5142,  0.2937, -0.4284,  0.6115,  0.1540,\n",
      "         0.1826, -0.3982, -0.5670,  0.1095]), tensor([ 0.0690, -0.0641,  0.1511, -0.0312,  0.0690, -0.1564,  0.0008, -0.0817,\n",
      "        -0.0283,  0.3170,  0.0111,  0.1760, -0.1708, -0.1592, -0.1034,  0.1926,\n",
      "         0.1665,  0.0205,  0.1813,  0.2341, -0.0395,  0.0366,  0.1370, -0.0487,\n",
      "         0.0608, -0.1184,  0.0487, -0.0027,  0.1402,  0.1059, -0.0859, -0.0654,\n",
      "        -0.1731,  0.0747, -0.0765,  0.0871,  0.1456, -0.0989, -0.0007, -0.1036,\n",
      "        -0.0049,  0.1226, -0.1341, -0.0910,  0.1982,  0.1588, -0.0753, -0.0139,\n",
      "        -0.0304,  0.0897, -0.0855, -0.0211,  0.0377, -0.0871,  0.0316, -0.1006,\n",
      "         0.0928, -0.0354, -0.0089, -0.2349,  0.0514, -0.0938, -0.0163, -0.0024,\n",
      "        -0.0794,  0.1436,  0.0883, -0.0677,  0.0935,  0.0378, -0.0181, -0.0224,\n",
      "         0.0142, -0.0707, -0.0461, -0.0146, -0.1938,  0.0356, -0.1225, -0.0458,\n",
      "         0.1181,  0.0009,  0.0373, -0.1049,  0.0348,  0.1197,  0.0828, -0.0896,\n",
      "         0.3073, -0.0211, -0.0435,  0.1007, -0.0571,  0.1539, -0.0317, -0.0550,\n",
      "        -0.0498,  0.1906, -0.0338, -0.1210])], [tensor([ 3.4741e-01, -7.5196e-01,  5.9132e-01, -1.2106e-01,  6.9907e-01,\n",
      "        -7.1380e-01, -8.3328e-01, -6.8692e-01, -7.8697e-01,  1.4951e+00,\n",
      "         3.8787e-01,  9.0744e-01, -1.6725e+00, -4.9250e-01, -1.9552e-01,\n",
      "         1.2953e+00, -2.3788e-02, -3.4488e-01,  5.2960e-01, -9.7020e-02,\n",
      "        -7.4229e-01,  4.6669e-01,  4.6507e-01, -3.1354e-01,  1.7783e-01,\n",
      "        -6.7539e-01, -1.1501e-01, -1.7200e-01,  8.9514e-02,  2.9198e-01,\n",
      "        -7.0343e-01, -7.1737e-01, -1.0621e+00,  2.1012e-01, -2.4800e-01,\n",
      "        -1.5323e-01,  5.2777e-01, -2.2904e-01, -3.7000e-01, -6.8490e-01,\n",
      "        -1.1928e+00, -1.0746e-01,  1.0139e+00, -3.2678e-01,  1.6848e-01,\n",
      "         7.4065e-01,  1.4056e-01,  1.1364e-01, -1.4951e-02,  3.5595e-01,\n",
      "         2.4379e-02,  3.3920e-01, -9.8620e-02, -2.6859e-01, -2.7445e-02,\n",
      "        -1.6242e-01, -7.1941e-01,  3.6980e-02, -1.2822e+00,  3.2500e-02,\n",
      "        -2.4572e-01, -1.5038e-01,  3.0198e-01, -3.0359e-01, -3.4648e-01,\n",
      "         3.0359e-01,  5.7370e-02,  8.0076e-01,  6.8260e-02,  6.7500e-01,\n",
      "         7.3514e-01, -4.1765e-01, -6.6137e-01, -6.0858e-02,  2.5959e-01,\n",
      "        -3.4760e-01, -1.0097e+00, -6.7128e-01,  7.6304e-01, -5.6706e-01,\n",
      "         3.4949e-01,  3.0874e-01, -5.3167e-01, -3.5064e-01,  4.7007e-01,\n",
      "        -6.7388e-02,  3.1092e-02,  6.5207e-02,  1.1254e+00, -5.6591e-01,\n",
      "        -2.6593e-01,  5.0682e-01, -1.2296e+00,  1.2650e+00, -4.6824e-02,\n",
      "         7.3440e-01, -1.4600e-04, -2.4581e-01,  1.5369e-02, -3.6524e-01]), tensor([-0.2770,  0.4366, -0.0928,  0.0227, -0.9203, -0.9404, -0.2116, -0.0604,\n",
      "         1.0540,  0.2607, -0.5052,  0.5363, -0.4702,  1.2799,  0.1291, -0.1911,\n",
      "         0.1815,  0.8200,  0.4139, -0.1091, -0.0523,  0.7290,  1.0534, -0.3458,\n",
      "         0.3767,  0.2141,  0.7512,  0.5001, -1.0588, -0.3632,  0.5794,  0.2473,\n",
      "        -0.1537,  0.9696, -0.2082,  0.3509,  1.3823, -0.3980,  1.1288, -0.9694,\n",
      "         0.0098,  0.5108, -0.3707,  1.1038,  0.6557, -0.2339, -0.0154, -0.7066,\n",
      "        -0.4023, -0.5206,  1.2951,  0.1086, -0.0349, -0.9903,  0.3590,  0.3822,\n",
      "        -0.1104, -0.1984, -0.6289, -0.2016, -0.8785,  0.5648, -0.2928, -0.1167,\n",
      "         0.0349,  0.7099,  0.6595, -0.4948, -1.1107, -0.5474, -0.0334,  0.5133,\n",
      "         0.6254,  0.5046, -0.3006,  0.2795,  0.4403,  0.4054, -0.8723, -0.0909,\n",
      "         0.0678,  1.2187,  1.5220, -0.1120, -0.0232,  0.3660, -0.7272,  0.8602,\n",
      "         1.4147,  0.2054, -0.0343,  0.0043,  0.0788, -0.0813, -0.7014,  0.1745,\n",
      "         0.0587, -0.6266,  0.4252, -0.0759]), tensor([-0.7540, -0.4252,  0.8847, -0.5711, -1.1832,  0.4315, -0.1015,  0.3610,\n",
      "         0.6748, -0.2943,  0.0806, -0.3012,  0.3224, -0.3388,  0.2792, -0.3733,\n",
      "        -0.0463,  0.4485,  0.1808,  0.0154, -0.0369,  1.3411,  0.7360, -0.2890,\n",
      "        -0.3320, -0.0946,  0.0738, -0.1439,  0.3266,  0.3589, -0.0649,  0.2675,\n",
      "         0.1096,  0.3682, -1.6357,  0.4843,  0.1966, -0.6430, -0.2285, -0.6177,\n",
      "         0.2770,  0.1185, -0.0947, -0.4767,  0.9916,  0.5090, -0.0052,  1.1076,\n",
      "        -0.0516, -0.3816, -0.3570, -0.0850, -0.4774,  0.5608, -0.6031,  0.6178,\n",
      "         0.3321, -0.1559,  0.1738,  0.3237,  0.8285, -0.0329,  0.0278,  0.1339,\n",
      "         0.2101,  0.4487, -0.3369, -0.4247,  1.6172,  0.4399, -0.5080,  0.9142,\n",
      "         0.1382, -0.9105, -0.2193, -0.3520, -1.1709, -0.4227, -0.3652,  0.3255,\n",
      "         0.5528,  0.3926,  0.3193,  0.1168, -0.5096,  0.0993, -0.4654, -0.4608,\n",
      "         0.7337, -0.5065, -0.4382,  0.5142,  0.2937, -0.4284,  0.6115,  0.1540,\n",
      "         0.1826, -0.3982, -0.5670,  0.1095]), tensor([-0.7540, -0.4252,  0.8847, -0.5711, -1.1832,  0.4315, -0.1015,  0.3610,\n",
      "         0.6748, -0.2943,  0.0806, -0.3012,  0.3224, -0.3388,  0.2792, -0.3733,\n",
      "        -0.0463,  0.4485,  0.1808,  0.0154, -0.0369,  1.3411,  0.7360, -0.2890,\n",
      "        -0.3320, -0.0946,  0.0738, -0.1439,  0.3266,  0.3589, -0.0649,  0.2675,\n",
      "         0.1096,  0.3682, -1.6357,  0.4843,  0.1966, -0.6430, -0.2285, -0.6177,\n",
      "         0.2770,  0.1185, -0.0947, -0.4767,  0.9916,  0.5090, -0.0052,  1.1076,\n",
      "        -0.0516, -0.3816, -0.3570, -0.0850, -0.4774,  0.5608, -0.6031,  0.6178,\n",
      "         0.3321, -0.1559,  0.1738,  0.3237,  0.8285, -0.0329,  0.0278,  0.1339,\n",
      "         0.2101,  0.4487, -0.3369, -0.4247,  1.6172,  0.4399, -0.5080,  0.9142,\n",
      "         0.1382, -0.9105, -0.2193, -0.3520, -1.1709, -0.4227, -0.3652,  0.3255,\n",
      "         0.5528,  0.3926,  0.3193,  0.1168, -0.5096,  0.0993, -0.4654, -0.4608,\n",
      "         0.7337, -0.5065, -0.4382,  0.5142,  0.2937, -0.4284,  0.6115,  0.1540,\n",
      "         0.1826, -0.3982, -0.5670,  0.1095]), tensor([ 0.0690, -0.0641,  0.1511, -0.0312,  0.0690, -0.1564,  0.0008, -0.0817,\n",
      "        -0.0283,  0.3170,  0.0111,  0.1760, -0.1708, -0.1592, -0.1034,  0.1926,\n",
      "         0.1665,  0.0205,  0.1813,  0.2341, -0.0395,  0.0366,  0.1370, -0.0487,\n",
      "         0.0608, -0.1184,  0.0487, -0.0027,  0.1402,  0.1059, -0.0859, -0.0654,\n",
      "        -0.1731,  0.0747, -0.0765,  0.0871,  0.1456, -0.0989, -0.0007, -0.1036,\n",
      "        -0.0049,  0.1226, -0.1341, -0.0910,  0.1982,  0.1588, -0.0753, -0.0139,\n",
      "        -0.0304,  0.0897, -0.0855, -0.0211,  0.0377, -0.0871,  0.0316, -0.1006,\n",
      "         0.0928, -0.0354, -0.0089, -0.2349,  0.0514, -0.0938, -0.0163, -0.0024,\n",
      "        -0.0794,  0.1436,  0.0883, -0.0677,  0.0935,  0.0378, -0.0181, -0.0224,\n",
      "         0.0142, -0.0707, -0.0461, -0.0146, -0.1938,  0.0356, -0.1225, -0.0458,\n",
      "         0.1181,  0.0009,  0.0373, -0.1049,  0.0348,  0.1197,  0.0828, -0.0896,\n",
      "         0.3073, -0.0211, -0.0435,  0.1007, -0.0571,  0.1539, -0.0317, -0.0550,\n",
      "        -0.0498,  0.1906, -0.0338, -0.1210]), tensor([-1.1253, -0.4867,  0.4996, -0.1207, -0.6023, -0.5513,  0.0560, -0.8366,\n",
      "         0.2459,  0.9828,  0.2693,  0.9612, -0.6770, -1.1182, -0.2213,  1.0402,\n",
      "         0.0904,  0.3942,  0.7759,  0.2325, -0.6446,  1.2470,  0.2070, -0.5528,\n",
      "        -0.1022, -0.0803, -0.4964, -0.3185, -0.7057, -0.1158,  0.1077, -0.3167,\n",
      "         0.2509,  0.2470, -0.8037,  0.4990,  0.1482,  0.1107, -0.2470, -0.9704,\n",
      "         0.3362, -0.3108,  0.2388,  0.4466,  0.9459,  0.0103, -0.0193,  0.6928,\n",
      "        -0.6218, -0.2212,  0.0586,  0.2643, -0.6026,  0.4392, -0.4276,  0.7923,\n",
      "        -0.7127, -0.1792, -0.8561,  0.2615,  0.0493,  0.4419,  0.0812,  0.2462,\n",
      "         0.1633,  0.3605, -0.2708, -0.2677,  0.9905,  0.2504, -0.1442,  0.6315,\n",
      "        -1.0937, -0.1998, -0.4155, -0.6800, -0.4622, -0.1613,  0.0940, -0.3884,\n",
      "         0.2529,  0.8331,  0.0245, -0.6046, -0.2765, -0.2241, -0.5246, -0.6579,\n",
      "         0.5434,  0.0155, -0.6994,  0.2768, -0.4993,  0.1805,  0.3522,  0.3669,\n",
      "         0.2198, -1.0057,  0.1562, -0.1613])]]\n"
     ]
    }
   ],
   "source": [
    "from Vocab import Vocab\n",
    "with open(\"embeddings-word2vecofficial.train.unk5.txt\", 'r') as f:\n",
    "    text = f.read().split()\n",
    "\n",
    "vocab = Vocab(emb_filename=\"embeddings-word2vecofficial.train.unk5.txt\")\n",
    "print(vocab.get_emb('the'))\n",
    "vocab_dict = vocab.get_vocab_dict()\n",
    "print(vocab_dict)\n",
    "result = get_word_and_next_k_indexes(\"Le_comte_de_Monte_Cristo.train.100.unk5.tok\", vocab_dict, k=context_size)\n",
    "print(result[:100])\n",
    "print(len(result))\n",
    "result_tensor = torch.tensor(result, dtype=torch.long)\n",
    "print(result_tensor.shape)\n",
    "\n",
    "######\"\"\n",
    "embeddings = transform_to_embeddings(result, \"embeddings-word2vecofficial.train.unk5.txt\")\n",
    "print(embeddings[:2]) #OKKKKKKKKKKK on l'a!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size=3, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - vocab_size (int): Taille du vocabulaire (|V|).\n",
    "        - embedding_dim (int): Dimension des plongements de mots (d).\n",
    "        - context_size (int): Nombre de mots de contexte (k).\n",
    "        - hidden_dim (int): Dimension de la couche cachée (dh).\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        # Taille de la couche d'entrée (dx = k * d)\n",
    "        self.input_dim = context_size * embedding_dim\n",
    "        \n",
    "        # Embedding layer pour transformer les indices de mots en vecteurs\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Matrice W et vecteur b1 pour la transformation de la couche cachée\n",
    "        self.fc1 = nn.Linear(self.input_dim, hidden_dim)  # h' = xW + b1\n",
    "        \n",
    "        # Matrice U et vecteur b2 pour la transformation de la couche de sortie\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)  # y' = hU + b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x est une séquence d'indices de mots (batch_size, context_size)\n",
    "        \n",
    "        # Transformer les indices en embeddings\n",
    "        x = self.embedding(x)  # (batch_size, context_size, embedding_dim)\n",
    "        \n",
    "        # Aplatir la dimension de contexte pour former la couche d'entrée\n",
    "        x = x.view(x.size(0), -1)  # (batch_size, context_size * embedding_dim)\n",
    "        \n",
    "        # Calcul de la couche cachée avec ReLU\n",
    "        h = F.relu(self.fc1(x))  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Calcul de la couche de sortie avec Softmax pour obtenir une distribution de probabilité\n",
    "        y = F.softmax(self.fc2(h), dim=1)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return y\n",
    "\n",
    "# Exemple d'instanciation du modèle\n",
    "\n",
    "\n",
    "# Exemple d'usage\n",
    "model = MultilayerPerceptron(vocab_size, embedding_dim, context_size, hidden_dim)\n",
    "#output = model(torch.randint(0, 2908, (32, 4)))  # 32 est le batch_size et 3 est le nombre de mots en contexte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 1/5, Loss: 7.6296\n",
      "Epoch 2/5, Loss: 7.6266\n",
      "Epoch 3/5, Loss: 7.6264\n",
      "Epoch 4/5, Loss: 7.6261\n",
      "Epoch 5/5, Loss: 7.6238\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_model(model, data, vocab, epochs, batch_size, learning_rate, context_size):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle en utilisant l'entropie croisée comme fonction de perte.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): Le modèle neuronal à entraîner.\n",
    "    - data (List[List[int]]): Données d'entraînement, liste de séquences d'indices de mots.\n",
    "    - vocab (Vocab): Instance de la classe Vocab pour le vocabulaire.\n",
    "    - epochs (int): Nombre d'époques pour l'entraînement.\n",
    "    - batch_size (int): Taille du lot.\n",
    "    - learning_rate (float): Taux d'apprentissage.\n",
    "    - context_size (int): Nombre de mots de contexte.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Module: Le modèle entraîné.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Préparation des données\n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "    inputs, targets = data[:, :-1], data[:, -1]  # Dernier mot de chaque séquence est la cible\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Définition de la fonction de perte (entropie croisée) et de l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Mise en mode entraînement\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_inputs, batch_targets in train_loader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Remise à zéro des gradients\n",
    "            \n",
    "            # Calcul de la sortie du modèle\n",
    "            outputs = model(batch_inputs)  # Prédictions du modèle\n",
    "            \n",
    "            # Calcul de la perte (entropie croisée)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Rétropropagation et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Affichage de la perte moyenne par époque\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "#stacked_embeddings = [torch.stack(embedding) for embedding in embeddings]\n",
    "\n",
    "# Convert the list of stacked tensors to a single tensor\n",
    "#embeddings_tensor = torch.stack(stacked_embeddings)\n",
    "\n",
    "#print(embeddings_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create random indices for training data\n",
    "randomized_embeddings = torch.randint(0, len(embeddings), (1000, 4)).tolist()\n",
    "\n",
    "# Train the model with randomized embeddings\n",
    "trained_model = train_model(model, result, vocab, epochs=5, batch_size=32, learning_rate=0.001, context_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(path, vocab_size, embedding_dim, hidden_dim):\n",
    "    model = MultilayerPerceptron(vocab_size, embedding_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2058.09033203125\n",
      "2058.09033203125\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_perplexity(model, data_loader):\n",
    "    model.eval()\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # Reshape outputs to (batch_size * sequence_length, vocab_size)\n",
    "            targets = targets.view(-1)  # Reshape targets to (batch_size * sequence_length)\n",
    "            log_prob = torch.log(F.softmax(outputs, dim=-1))\n",
    "            log_prob = log_prob[range(len(targets)), targets]\n",
    "            total_log_prob += log_prob.sum().item()\n",
    "            total_words += len(targets)\n",
    "    perplexity = torch.exp(torch.tensor(-total_log_prob / total_words))\n",
    "    print(\"Perplexity:\", perplexity.item())\n",
    "    return perplexity.item()\n",
    "\n",
    "vocab_size = 2908\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "batch_size= 32\n",
    "data = torch.tensor(result, dtype=torch.long)\n",
    "inputs, targets = data[:, :-1], data[:, -1]  # Dernier mot de chaque séquence est la cible\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(calculate_perplexity(model, data_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x300 and 500x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Ensure the seed text is correctly encoded\u001b[39;00m\n\u001b[0;32m     41\u001b[0m seed_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_to_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_text(model, seed_text, vocab_dict, idx_to_word, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_text(model, seed_text, vocab_dict, idx_to_word, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, seed_text, vocab_dict, idx_to_word, max_len, context_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 22\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m         output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output[\u001b[38;5;241m0\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         next_word_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(output, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mMultilayerPerceptron.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, context_size * embedding_dim)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calcul de la couche cachée avec ReLU\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (batch_size, hidden_dim)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Calcul de la couche de sortie avec Softmax pour obtenir une distribution de probabilité\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(h), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x300 and 500x128)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generate_text(model, seed_text, vocab_dict, idx_to_word, max_len=50, context_size=3):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    generated_text = seed_text.copy()\n",
    "    input_seq = torch.tensor([vocab_dict.get(word, vocab_dict['<unk>']) for word in seed_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Ensure input_seq has the correct context_size\n",
    "    if input_seq.size(1) < context_size:\n",
    "        padding = torch.zeros((1, context_size - input_seq.size(1)), dtype=torch.long).to(device)\n",
    "        input_seq = torch.cat([padding, input_seq], dim=1)\n",
    "    else:\n",
    "        input_seq = input_seq[:, -context_size:]\n",
    "    \n",
    "    taille = 0\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "            output = F.softmax(output[0, :], dim=-1)  # Apply softmax to get probabilities\n",
    "            next_word_idx = torch.multinomial(output, 1).item()\n",
    "            next_word = idx_to_word.get(next_word_idx, '<unk>')\n",
    "            generated_text.append(next_word)\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_word_idx]], dtype=torch.long).to(device)], dim=1)\n",
    "            input_seq = input_seq[:, -context_size:]  # Keep only the last context_size elements\n",
    "            taille+=1\n",
    "            if next_word == '<end>':\n",
    "                break\n",
    "    \n",
    "    return (' '.join(generated_text),taille)\n",
    "\n",
    "# Example usage (assuming model, vocab_dict, and idx_to_word are defined)\n",
    "\n",
    "idx_to_word = {idx: word for word, idx in vocab_dict.items()}\n",
    "\n",
    "\n",
    "# Ensure the seed text is correctly encoded\n",
    "seed_text = [\"<start>\"]\n",
    "print(generate_text(model, seed_text, vocab_dict, idx_to_word, max_len=50, context_size=3))\n",
    "print(generate_text(model, seed_text, vocab_dict, idx_to_word, max_len=50, context_size=3))\n",
    "\n",
    "print(generate_text(model, seed_text, vocab_dict, idx_to_word, max_len=50, context_size=3))\n",
    "\n",
    "print(generate_text(model, seed_text, vocab_dict, idx_to_word, max_len=50, context_size=3))\n",
    "\n",
    "# Generate multiple sentences\n",
    "for _ in range(5):\n",
    "    generated_sentence, length = generate_text(model, seed_text, vocab_dict, idx_to_word, max_len=50, context_size=context_size)\n",
    "    print(f\"Generated Sentence: {generated_sentence}\")\n",
    "    print(f\"Length: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 1/15, Loss: 7.6291\n",
      "Epoch 2/15, Loss: 7.6209\n",
      "Epoch 3/15, Loss: 7.6172\n",
      "Epoch 4/15, Loss: 7.6156\n",
      "Epoch 5/15, Loss: 7.6147\n",
      "Epoch 6/15, Loss: 7.6137\n",
      "Epoch 7/15, Loss: 7.6130\n",
      "Epoch 8/15, Loss: 7.6124\n",
      "Epoch 9/15, Loss: 7.6119\n",
      "Epoch 10/15, Loss: 7.6118\n",
      "Epoch 11/15, Loss: 7.6110\n",
      "Epoch 12/15, Loss: 7.6111\n",
      "Epoch 13/15, Loss: 7.6103\n",
      "Epoch 14/15, Loss: 7.6100\n",
      "Epoch 15/15, Loss: 7.6099\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, data, vocab, epochs, batch_size, learning_rate, context_size):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle en utilisant l'entropie croisée comme fonction de perte.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): Le modèle neuronal à entraîner.\n",
    "    - data (List[List[int]]): Données d'entraînement, liste de séquences d'indices de mots.\n",
    "    - vocab (Vocab): Instance de la classe Vocab pour le vocabulaire.\n",
    "    - epochs (int): Nombre d'époques pour l'entraînement.\n",
    "    - batch_size (int): Taille du lot.\n",
    "    - learning_rate (float): Taux d'apprentissage.\n",
    "    - context_size (int): Nombre de mots de contexte.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Module: Le modèle entraîné.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Préparation des données\n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "    inputs, targets = data[:, :-1], data[:, -1]  # Dernier mot de chaque séquence est la cible\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Définition de la fonction de perte (entropie croisée) et de l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Mise en mode entraînement\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_inputs, batch_targets in train_loader:\n",
    "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Remise à zéro des gradients\n",
    "            \n",
    "            # Calcul de la sortie du modèle\n",
    "            outputs = model(batch_inputs)  # Prédictions du modèle\n",
    "            \n",
    "            # Calcul de la perte (entropie croisée)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Rétropropagation et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Affichage de la perte moyenne par époque\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Exemple d'utilisation\n",
    "batch_size = 32 # Vous pouvez ajuster cette valeur pour trouver la valeur optimale\n",
    "model = MultilayerPerceptron(vocab_size, embedding_dim, context_size, hidden_dim)\n",
    "trained_model = train_model(model, result, vocab, epochs=15, batch_size=batch_size, learning_rate=0.001, context_size=context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: <start> ici noirtier laissait étais absence fusil chut prix assiette îles\n",
      "Length: 10\n",
      "Generated Sentence: <start> veulent parti prière élégant meurs teresa crosse encore étendant calèche\n",
      "Length: 10\n",
      "Generated Sentence: <start> tracés balle confetti base coup que garçon fâché ordre sortant\n",
      "Length: 10\n",
      "Generated Sentence: <start> mouvements ferme voici individu rocher langues bureau amer cet nu\n",
      "Length: 10\n",
      "Generated Sentence: <start> préoccupation prenait dirigea village vaut passée hier désirait balbutia selon\n",
      "Length: 10\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generated_sentence, length = generate_text(trained_model, seed_text, vocab_dict, idx_to_word, max_len=10, context_size=context_size)\n",
    "    print(f\"Generated Sentence: {generated_sentence}\")\n",
    "    print(f\"Length: {length}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
