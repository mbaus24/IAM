{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET NOUN AUX VERB DET NOUN CCONJ ADP DET NOUN ADP DET NOUN PUNCT VERB DET NOUN ADP DET NOUN ADP PUNCT NUM NOUN ADP NOUN ADJ ADP DET NOUN ADP PROPN PUNCT PUNCT\n",
      "NOUN ADJ DET NOUN ADP DET NOUN ADP NUM NOUN NUM ADP NUM NOUN PUNCT\n",
      "ADV ADP DET NOUN PUNCT PRON VERB DET NOUN VERB ADP DET ADJ NOUN PUNCT\n",
      "DET NOUN ADP DET NOUN VERB DET NOUN ADP PRON VERB DET NOUN PUNCT ADP NUM NOUN PUNCT NOUN PROPN PUNCT\n",
      "NOUN PROPN VERB NUM NOUN PUNCT\n",
      "PRON AUX DET NOUN ADP DET ADJ PROPN PROPN PUNCT ADV ADJ ADP DET NOUN PUNCT NOUN ADP DET NOUN ADP ADP NUM PUNCT NOUN ADP PRON PRON AUX VERB ADP VERB DET NOUN ADP PROPN ADP VERB VERB DET NOUN ADJ ADP PROPN PUNCT\n",
      "ADP VERB DET NOUN PUNCT PROPN PROPN PUNCT NOUN ADP DET NOUN ADP NOUN ADP PROPN PUNCT\n",
      "DET ADJ NOUN PUNCT ADP DET NOUN ADP PRON PUNCT DET NOUN PROPN PUNCT AUX VERB ADP DET NOUN DET NOUN ADP NOUN PUNCT DET NOUN ADP DET NOUN PUNCT\n",
      "DET ADJ NOUN AUX VERB SCONJ PRON PRON VERB CCONJ ADV ADP DET NUM NOUN PUNCT NOUN NUM NOUN PUNCT ADP DET NOUN ADP DET NOUN ADJ ADJ ADP DET NOUN ADP NOUN CCONJ NOUN ADP NOUN ADP DET PROPN PRON PRON VERB DET ADJ NOUN PUNCT ADP DET NOUN ADP PROPN PROPN PUNCT ADP DET NOUN ADJ PUNCT\n",
      "PROPN PROPN AUX VERB ADP PROPN ADP DET PROPN PUNCT DET NUM NOUN NUM PUNCT ADJ ADP DET NOUN ADP NUM NOUN PUNCT\n",
      "SCONJ PRON ADV VERB ADV ADP DET NOUN ADJ PUNCT PRON VERB DET NOUN ADP ADJ CCONJ VERB ADJ VERB DET NOUN ADP NOUN NOUN PUNCT CCONJ VERB ADV NOUN ADP VERB DET NOUN ADP NOUN ADP DET NOUN ADP NOUN PUNCT\n",
      "DET NOUN ADP DET NOUN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN PUNCT VERB ADP PUNCT VERB ADJ DET NOUN PUNCT PUNCT\n",
      "ADP DET NOUN PUNCT PRON ADV VERB ADV ADV ADV NUM NOUN CCONJ DET NOUN ADP DET NOUN ADP DET NOUN PUNCT\n",
      "PUNCT PROPN PUNCT NOUN ADP NOUN PUNCT PUNCT\n",
      "DET NOUN ADJ VERB ADV DET NOUN PUNCT ADV DET NOUN PUNCT CCONJ DET NOUN ADP SCONJ NOUN PUNCT NOUN CCONJ NOUN PRON VERB ADP NOUN PUNCT\n",
      "CCONJ ADP DET NOUN ADP DET NOUN DET NOUN CCONJ DET NOUN ADJ PUNCT DET NOUN AUX ADV ADV ADJ CCONJ VERB ADP VERB ADV ADV ADP DET NOUN PUNCT\n",
      "PUNCT PRON ADV AUX ADV DET NOUN ADJ PUNCT CCONJ ADV DET NOUN ADP NOUN PUNCT PUNCT VERB PROPN PROPN PUNCT NOUN ADP DET NOUN ADP DET NOUN PUNCT\n",
      "DET PROPN PRON VERB ADP PROPN ADP NOUN PUNCT\n",
      "DET NOUN VERB DET NOUN ADP NOUN CCONJ AUX VERB ADP DET NOUN ADJ PRON DET NOUN ADP NOUN AUX VERB ADP NUM NOUN ADP NOUN ADP NOUN PUNCT\n",
      "DET NOUN ADJ VERB ADP DET NOUN ADP DET NOUN PUNCT\n",
      "DET NOUN ADP DET NOUN AUX ADP DET NOUN ADP DET NOUN PUNCT\n",
      "DET NOUN AUX ADJ PUNCT\n",
      "NOUN PUNCT PROPN PROPN PUNCT\n",
      "PUNCT PRON VERB ADJ NOUN ADP VERB ADP PRON DET NOUN ADP VERB DET NOUN ADJ ADJ ADP DET NOUN ADP DET NOUN PUNCT CCONJ ADP DET NOUN PUNCT DET NOUN AUX VERB ADP DET NUM NOUN ADJ PUNCT VERB DET NOUN ADP PROPN PROPN PROPN PUNCT\n",
      "CCONJ SCONJ DET NOUN ADP NOUN ADP DET NOUN VERB DET ADJ NOUN ADP ADV ADP NUM NOUN ADP NOUN ADP NOUN ADJ PUNCT\n",
      "PRON VERB VERB DET PUNCT NOUN PUNCT CCONJ VERB ADP DET NOUN ADP DET NOUN ADJ ADP DET NOUN CCONJ ADP DET NOUN PUNCT\n",
      "DET NOUN VERB NUM NOUN PUNCT PRON VERB DET NOUN PRON VERB ADP DET NOUN ADP DET NOUN PUNCT\n",
      "CCONJ DET NOUN ADJ VERB ADP DET NOUN PUNCT\n",
      "ADP VERB DET NOUN ADP DET NOUN CCONJ ADV ADP VERB DET NOUN ADJ PUNCT VERB PROPN PROPN PUNCT\n",
      "NOUN PROPN AUX VERB DET PUNCT NOUN ADJ ADP DET NOUN ADP DET NOUN ADP DET NOUN PUNCT ADP PROPN ADV SCONJ DET NOUN ADP NOUN VERB NOUN ADP NOUN PUNCT NOUN PUNCT CCONJ NOUN ADP DET NOUN VERB PRON ADJ PUNCT\n",
      "L' association a changé les décors et avec l' aide de plusieurs bénévoles , établi différents tableaux sur le thème de \" Cinq siècles d' activité économique de la région d' Ancerville \" .\n",
      "Ouverture tous les jours sauf le lundi de 14 h 30 à 18 h .\n",
      "Quant à le sous-préfet , il apprécie l' énergie dépensée pour une telle réalisation .\n",
      "Les membres de le club auront l' occasion de s' entraîner cet après-midi , dès 14 h , salle Jean-Mathieu .\n",
      "M. Hosneld avait 44 ans .\n",
      "C' est le cas de ce brave Joseph Bari , toujours présent à le comité , vedette de la colombophilie jusqu' en 1962 , époque à laquelle il fut obligé de quitter son pigeonnier de Châtenois-les-Forges pour aller habiter un espace réduit à Grand-Charmont .\n",
      "Pour animer cette soirée , François Puel , chercheur de le laboratoire d' astrophysique de Besançon .\n",
      "Les Petits Princes , à le nombre de cinq , le roi Pierre ... ont réveillé chez le public des souvenirs d' enfance , le temps d' une représentation .\n",
      "Le double événement était fêté comme il se doit et conjointement par les deux établissements , mercredi 23 juin , à l' occasion de l' assemblée générale décentralisée de les Offices de tourisme et Syndicats d' initiative de le Doubs qui se tenait le même jour , sous la présidence d' Edmond Maire , à la Saline Royale .\n",
      "Suzanne Collin était née à Brauvilliers dans la Meuse , le 21 janvier 1924 , issue d' une famille de cinq enfants .\n",
      "Quand elle ne vaquait pas à ses occupations ménagères , elle utilisait les transports en commun et descendait seule faire ses courses en centre ville , et prenait alors plaisir à faire un brin de causette avec ses copines de rencontre .\n",
      "Le fond de le problème : l' opposition avec le président de l' Harmonie , accusé de \" prendre seul les décisions \" .\n",
      "Selon les démissionnaires , il ne reste plus aujourd'hui que trois clairons et un tambour à le sein de la batterie .\n",
      "\" Verdun , ville de lumière ! \"\n",
      "La gare routière attend toujours ses illuminations , pas des guirlandes , mais des lampadaires pour que scolaires , usagés et conducteurs se sentent en sécurité .\n",
      "Mais dans le secteur de le Pré L' Evêque et les prés avoisinants , la glace est encore bien présente et risque d' être encore là pendant plusieurs jours .\n",
      "\" Ce ne sera pas un cours magistral , mais plutôt un partage de connaissances \" , souligne Dominique Richard , responsable de les animations dans le village .\n",
      "L' EBM s' inclina devant Joeuf par 70-61 .\n",
      "Le conducteur présente des signes d' ivresse et est conduit à le commissariat central où son taux d' alcoolémie est fixé à 1,74 gramme par litre de sang .\n",
      "L' association paroissiale investit dans les locaux de la cure .\n",
      "La réévaluation de cette participation était à l' ordre de le jour .\n",
      "Le comité est inchangé :\n",
      "vice-président : André Ménétrez ;\n",
      "\" Nous avions bon espoir d' obtenir d' elle un prêt-relais pour acheter les matières premières nécessaires à le redémarrage de l' activité , car dans l' usine , les machines sont arrêtées depuis le 14 janvier dernier \" explique le directeur d' EFI Michel Balandier .\n",
      "Et comme le plan de résorption de les dettes prévoyait un autre versement de près de deux millions d' euros en juin prochain ...\n",
      "Ils pourront découvrir le \" Planétarium \" et partir à la rencontre de le monde merveilleux de les étoiles et de les constellations .\n",
      "La température affichant -6 °C , il emmenait les gamins se réchauffer dans les camions de les pompiers .\n",
      "Et l' acte malveillant figure parmi les hypothèses .\n",
      "Pour éviter une reprise de le feu mais aussi pour prévenir des actes malveillants \" constatait Martial Bourquin .\n",
      "M. Rivasseau a réaffirmé l' \" exigence absolue de le respect de les droits de l' Homme \" pour Paris alors que des témoignages de réfugiés font état d' exactions , viols , et pillages par des éléments semble -t-il libériens .\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "for sent in parse_incr(open(\"sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    print(\" \".join(tok[\"upos\"] for tok in sent))\n",
    "\n",
    "for sent in parse_incr(open(\"sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    print(\" \".join(tok[\"form\"] for tok in sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self, d_embed, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(d_in, d_embed, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.decision = nn.Linear(d_embed, d_out)\n",
    "    def forward(self, idx_words):\n",
    "        embedded = self.embed(idx_words)\n",
    "        averaged = torch.mean(embedded, dim=1) # dim 0 is batch\n",
    "        return self.decision(self.dropout(averaged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,   3,   4,   5,   6,   7,   8,   9,  10,  11],\n",
      "        [ 32,  33,   6,  34,  35,  20,  36,  12,  37,  38],\n",
      "        [ 42,  40,  20,  43,  15,  44,  45,  10,  46,  47],\n",
      "        [ 52,  53,  12,  20,  54,  55,  10,  56,  12,  57],\n",
      "        [ 64,  65,  66,  67,  68,  31,   0,   0,   0,   0],\n",
      "        [ 69,  70,  20,  71,  12,  72,  73,  74,  75,  15],\n",
      "        [ 98,  99, 100, 101,  15, 102, 103,  15, 104,  12],\n",
      "        [ 52, 108, 109,  15,  40,  20, 110,  12, 111,  15],\n",
      "        [124, 125, 126, 127, 128, 129,  44, 130, 131,   8],\n",
      "        [157, 158, 127, 159,  40, 160, 161,  28, 162,  15],\n",
      "        [169, 170, 171, 172, 173,  40, 174, 175, 176,  15],\n",
      "        [124, 193,  12,  20, 194, 195,  10, 196,   9,  20],\n",
      "        [203,   6, 204,  15,  44, 171, 205, 206, 207, 208],\n",
      "        [ 22, 214,  15, 185,  12, 215, 216,  22,   0,   0],\n",
      "        [217, 218, 219, 220,  76, 174, 221,  15, 173, 119],\n",
      "        [230, 161,  20, 231,  12,  20, 232,   2, 233,   8],\n",
      "        [ 22, 244, 171, 245, 173,  94, 246, 247,  15, 223],\n",
      "        [  2, 257,  57, 258, 259, 260, 133, 261,  31,   0],\n",
      "        [124, 262, 239, 119, 263,  25, 264,   8,  70, 265],\n",
      "        [  2,   3, 276, 277, 161,   6, 278,  12,  28, 279],\n",
      "        [217, 280,  12, 100, 281, 127,  40,  10, 282,  12],\n",
      "        [124,  78,  70, 283, 195,   0,   0,   0,   0,   0],\n",
      "        [284, 195, 285, 286, 287,   0,   0,   0,   0,   0],\n",
      "        [ 22, 288, 289, 290, 291,  25, 292,  25, 170,  94],\n",
      "        [311, 129,  20, 312,  12, 313,  12,   6, 314, 315],\n",
      "        [322, 323, 324,  20,  22, 325,  22,   8, 326,  40],\n",
      "        [217, 331, 332, 333, 334,  15,  44, 335,   6, 336],\n",
      "        [311,  10, 340, 341, 342, 343,   6, 344,  31,   0],\n",
      "        [ 98, 345,  49, 346,  12,  20, 347, 223, 348,  48],\n",
      "        [ 64, 355,   4, 356,  10,  22, 357, 358,  12,  20]])\n",
      "tensor([[ 2,  3,  4,  5,  2,  3,  6,  7,  2,  3],\n",
      "        [ 3, 10,  2,  3,  7,  2,  3,  7,  9,  3],\n",
      "        [12,  7,  2,  3,  8, 13,  5,  2,  3,  5],\n",
      "        [ 2,  3,  7,  2,  3,  5,  2,  3,  7, 13],\n",
      "        [ 3, 11,  5,  9,  3,  8,  0,  0,  0,  0],\n",
      "        [13,  4,  2,  3,  7,  2, 10, 11, 11,  8],\n",
      "        [ 7,  5,  2,  3,  8, 11, 11,  8,  3,  7],\n",
      "        [ 2, 10,  3,  8,  7,  2,  3,  7, 13,  8],\n",
      "        [ 2, 10,  3,  4,  5, 14, 13, 13,  5,  6],\n",
      "        [11, 11,  4,  5,  7, 11,  7,  2, 11,  8],\n",
      "        [14, 13, 12,  5, 12,  7,  2,  3, 10,  8],\n",
      "        [ 2,  3,  7,  2,  3,  8,  2,  3,  7,  2],\n",
      "        [ 7,  2,  3,  8, 13, 12,  5, 12, 12, 12],\n",
      "        [ 8, 11,  8,  3,  7,  3,  8,  8,  0,  0],\n",
      "        [ 2,  3, 10,  5, 12,  2,  3,  8, 12,  2],\n",
      "        [ 6,  7,  2,  3,  7,  2,  3,  2,  3,  6],\n",
      "        [ 8, 13, 12,  4, 12,  2,  3, 10,  8,  6],\n",
      "        [ 2, 11, 13,  5,  7, 11,  7,  3,  8,  0],\n",
      "        [ 2,  3,  5,  2,  3,  7,  3,  6,  4,  5],\n",
      "        [ 2,  3, 10,  5,  7,  2,  3,  7,  2,  3],\n",
      "        [ 2,  3,  7,  2,  3,  4,  7,  2,  3,  7],\n",
      "        [ 2,  3,  4, 10,  8,  0,  0,  0,  0,  0],\n",
      "        [ 3,  8, 11, 11,  8,  0,  0,  0,  0,  0],\n",
      "        [ 8, 13,  5, 10,  3,  7,  5,  7, 13,  2],\n",
      "        [ 6, 14,  2,  3,  7,  3,  7,  2,  3,  5],\n",
      "        [13,  5,  5,  2,  8,  3,  8,  6,  5,  7],\n",
      "        [ 2,  3,  5,  9,  3,  8, 13,  5,  2,  3],\n",
      "        [ 6,  2,  3, 10,  5,  7,  2,  3,  8,  0],\n",
      "        [ 7,  5,  2,  3,  7,  2,  3,  6, 12,  7],\n",
      "        [ 3, 11,  4,  5,  2,  8,  3, 10,  7,  2]])\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import torch \n",
    "\n",
    "def pad_tensor(X, max_len):\n",
    "    res = torch.full((len(X), max_len), 0) # padding\n",
    "    for (i, row) in enumerate(X) :\n",
    "        x_len = min(max_len, len(X[i]))\n",
    "        res[i,:x_len] = torch.LongTensor(X[i][:x_len])\n",
    "    return res\n",
    "\n",
    "wordvocab = defaultdict(lambda : len(wordvocab))\n",
    "wordvocab[\"<PAD>\"]; wordvocab[\"<UNK>\"] # Special token IDs\n",
    "\n",
    "sentences = []\n",
    "for sent in parse_incr(open(\"sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    sentences.append([wordvocab[tok[\"form\"]] for tok in sent])\n",
    "\n",
    "max_len = max(len(s) for s in sentences)\n",
    "max_len = 10\n",
    "padded_sentences = pad_tensor(sentences, max_len)\n",
    "\n",
    "print(padded_sentences)\n",
    "\n",
    "\n",
    "uposvocab = defaultdict(lambda: len(uposvocab))\n",
    "uposvocab[\"<PAD>\"]; uposvocab[\"<UNK>\"]  # Special token IDs\n",
    "\n",
    "upos_sentences = []\n",
    "for sent in parse_incr(open(\"sequoia-ud.parseme.frsemcor.simple.small\", encoding='UTF-8')):\n",
    "    upos_sentences.append([uposvocab[tok[\"upos\"]] for tok in sent])\n",
    "\n",
    "padded_upos_sentences = pad_tensor(upos_sentences, max_len)\n",
    "\n",
    "print(padded_upos_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 15])\n",
      "tensor([[-0.1445,  0.1108, -0.2580,  ..., -0.1363, -0.2051, -0.0924],\n",
      "        [-0.0880,  0.1836, -0.4074,  ..., -0.0267, -0.3332,  0.1166],\n",
      "        [-0.0565,  0.0232, -0.3631,  ..., -0.0447, -0.2186,  0.1038],\n",
      "        ...,\n",
      "        [-0.1693,  0.0580,  0.0689,  ..., -0.1117, -0.0036,  0.0155],\n",
      "        [-0.0692, -0.1959, -0.0450,  ..., -0.0759, -0.0883,  0.0386],\n",
      "        [-0.1118, -0.2656, -0.0656,  ..., -0.0065, -0.0535, -0.1043]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tds = TensorDataset(padded_sentences, padded_upos_sentences)\n",
    "dataloader = DataLoader(tds, batch_size=2, shuffle=True)\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, d_embed, d_hidden, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(d_in, d_embed, padding_idx=0)\n",
    "        self.gru = nn.GRU(d_embed, d_hidden, batch_first=True, bias=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.decision = nn.Linear(d_hidden, d_out)\n",
    "    \n",
    "    def forward(self, idx_words):\n",
    "        embedded = self.embed(idx_words)\n",
    "        hidden, _ = self.gru(embedded)\n",
    "        hidden = hidden.contiguous().view(-1, hidden.size(-1))  # Flatten hidden states\n",
    "        return self.decision(self.dropout(hidden))\n",
    "\n",
    "\n",
    "res = GRUClassifier(10, 20, len(wordvocab), len(uposvocab))(padded_sentences)\n",
    "print(res.shape)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGRUClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwordvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muposvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, dataloader, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(model, dataloader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      2\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 3\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      5\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "def fit(model, dataloader, epochs=10):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for words, upos in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(words)\n",
    "            output = output.view(-1, output.size(-1))  # Reshape output to (batch_size * sequence_length, num_classes)\n",
    "            upos = upos.view(-1)  # Flatten target tensor to (batch_size * sequence_length)\n",
    "            loss = loss_fn(output, upos)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "fit(GRUClassifier(10, 2, len(wordvocab), len(uposvocab)), dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
