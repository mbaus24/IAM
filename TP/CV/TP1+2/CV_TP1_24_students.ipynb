{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpK-LUDH5WJR"
      },
      "source": [
        "##Playing around colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmYvDDt3OClU"
      },
      "outputs": [],
      "source": [
        "#Mount your google drive: require authorization\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfAY7OEuOtmU"
      },
      "outputs": [],
      "source": [
        "#Launch some linux commands by adding \"!\" before a command\n",
        "!pwd\n",
        "!ls\n",
        "!ls drive/My\\ Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snv-W5TgOvUT"
      },
      "outputs": [],
      "source": [
        "#Change your location in the arborescence\n",
        "import os\n",
        "#################################################CHANGE TO YOUR PATH\n",
        "os.chdir('/content/drive/My Drive/PATH_TO_YOUR_CV_TP1_FOLDER')#########################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMboH93MPErm"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzvo10ZV2Za9"
      },
      "source": [
        "# PART 1: Image processing and matching with opencv, numpy and matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9nlyoz-5v9s"
      },
      "source": [
        "## Open, read and display images\n",
        "Open the image of Lena using cv2.imread()\n",
        "\n",
        "Display it using cv2_imshow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r3HTzwn0OOUS"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cv2_imshow' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlena.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcv2_imshow\u001b[49m(img)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'cv2_imshow' is not defined"
          ]
        }
      ],
      "source": [
        "img = cv2.imread('lena.jpeg')\n",
        "cv2_imshow(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NtRzjXtBN_UW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "uint8\n",
            "225\n",
            "224\n",
            "3\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Check the image matrix data type (could know the bit depth of the image)\n",
        "print(img.dtype)\n",
        "# Check the height of image\n",
        "print(img.shape[0])\n",
        "# Check the width of image\n",
        "print(img.shape[1])\n",
        "# Check the number of channels of the image\n",
        "print(img.shape[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9BRb6z_Am2"
      },
      "source": [
        "##Color channels\n",
        "\n",
        "By default opencv uses BGR color channels\n",
        "\n",
        "Use cv2.cvtColor() function to convert the color space to RGB and grayscale and display the resulting images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q--qmWZtYZCR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdMQWuivACFp"
      },
      "source": [
        "##Convolution\n",
        "\n",
        "Use the convolve2D function to apply several filters to the image.\n",
        "\n",
        "Apply Sobel filters, Gaussian, edge detector and display the results.\n",
        "\n",
        "Explain the padding and strides parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtA6VZ5KABW4"
      },
      "outputs": [],
      "source": [
        "def convolve2D(image, kernel, padding=0, strides=1):\n",
        "    # Cross Correlation\n",
        "    kernel = np.flipud(np.fliplr(kernel))\n",
        "\n",
        "    # Gather Shapes of Kernel + Image + Padding\n",
        "    xKernShape = kernel.shape[0]\n",
        "    yKernShape = kernel.shape[1]\n",
        "    xImgShape = image.shape[0]\n",
        "    yImgShape = image.shape[1]\n",
        "\n",
        "    # Shape of Output Convolution\n",
        "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
        "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
        "    output = np.zeros((xOutput, yOutput))\n",
        "\n",
        "    # Apply Equal Padding to All Sides\n",
        "    if padding != 0:\n",
        "        imagePadded = np.zeros((image.shape[0] + padding*2, image.shape[1] + padding*2))\n",
        "        imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image\n",
        "        print(imagePadded)\n",
        "    else:\n",
        "        imagePadded = image\n",
        "\n",
        "    # Iterate through image\n",
        "    for y in range(image.shape[1]):\n",
        "        # Exit Convolution\n",
        "        if y > image.shape[1] - yKernShape:\n",
        "            break\n",
        "        # Only Convolve if y has gone down by the specified Strides\n",
        "        if y % strides == 0:\n",
        "            for x in range(image.shape[0]):\n",
        "                # Go to next row once kernel is out of bounds\n",
        "                if x > image.shape[0] - xKernShape:\n",
        "                    break\n",
        "                try:\n",
        "                    # Only Convolve if x has moved by the specified Strides\n",
        "                    if x % strides == 0:\n",
        "                        output[x, y] = (kernel * imagePadded[x: x + xKernShape, y: y + yKernShape]).sum()\n",
        "                except:\n",
        "                    break\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fvmufz5YmTi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOz3uHeh_alI"
      },
      "source": [
        "##Bluring and edge detection\n",
        "\n",
        "As an alternative you can use openCV function for Blur and edge detection.\n",
        "\n",
        "Apply cv2.GaussianBlur() and cv2.Canny()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft0BaqPBYl5k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONd-z74jPoDJ"
      },
      "source": [
        "##Contours detection\n",
        "Detect contours with plt.contour and cv2.findContours()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idFalF_kYwVU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfzdIPpXNKAl"
      },
      "source": [
        "##Optional:\n",
        "Play around with resizing and cropping: cv2.resize()\n",
        "\n",
        "Cropping can be achieved by using the image data as a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5r5S4i0ZQc1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bWKtBG-Ni-X"
      },
      "source": [
        "##Optional\n",
        "Play around with a blank image\n",
        "\n",
        "Then draw a line, a rectangle, circle, and text. line(), rectangle, circle, putText function can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WePav5uAZUk-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vbpojXZTr4V"
      },
      "source": [
        "##Optional:\n",
        "Play around: normalize the pixel values to [0,1] square or squareroot the values and transform them back to [0,255], then display the resulting images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaGp9Tbcdq1j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lEZXK9yUKYN"
      },
      "source": [
        "##Feature point detection.\n",
        "\n",
        "Use cv2.cornerHarris(), cv2.goodFeaturesToTrack(), cv2.FastFeatureDetector, and cv2.ORB to detect and display feature points on an image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aN3h-rdzdzik"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cv2_imshow' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m image1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg1.ppm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m image2 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg2.ppm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcv2_imshow\u001b[49m(image2)\n\u001b[0;32m      5\u001b[0m image3 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image2,cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m      6\u001b[0m cv2_imshow(image3)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'cv2_imshow' is not defined"
          ]
        }
      ],
      "source": [
        "image1 = cv2.imread('img1.ppm')\n",
        "image2 = cv2.imread('img2.ppm')\n",
        "\n",
        "cv2_imshow(image2)\n",
        "image3 = cv2.cvtColor(image2,cv2.COLOR_BGR2GRAY)\n",
        "cv2_imshow(image3)\n",
        "dst = cv2.cornerHarris(image3,2,3,0.04)\n",
        "\n",
        "corners = cv2.goodFeaturesToTrack(image3,20,0.01,10)\n",
        "corners = np.int0(corners)\n",
        " \n",
        "# Iterate over the corners and draw a circle at that location\n",
        "for i in corners:\n",
        "    x,y = i.ravel()\n",
        "    cv2.circle(image2,(x,y),5,(0,0,255),-1)\n",
        "    \n",
        "# Display the image\n",
        "cv2.imshow('a', image2)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "\n",
        " \n",
        "# Threshold for an optimal value, it may vary depending on the image.\n",
        "image2[dst>0.01*dst.max()]=[0,0,255]\n",
        " \n",
        "cv2_imshow(dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WktRSZctU5JO"
      },
      "source": [
        "##Matching\n",
        "\n",
        "read img1.ppm and img2.ppm.\n",
        "\n",
        "After computing feature points, perform matching using cv2.BFMatcher or cv2.FlannBasedMatcher\n",
        "\n",
        "Display the two images side by side and draw a line between matched points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRnzc5SXd1ng"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7qB3bA3V8g2"
      },
      "source": [
        "##Optional\n",
        "\n",
        "compute the homography.\n",
        "\n",
        "Use ransac algorithm.\n",
        "\n",
        "apply transformation to the second image to match the first image viewpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq8_PoQPztcD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2dP5zAkoJQW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOao2sjAoJUq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTkGE4acoZ97"
      },
      "source": [
        "# PART 2: Image embeddings for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJC6whsCo7CT"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyj-AcgBoJaC"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHgSwQ0DpCyT"
      },
      "source": [
        "###Build Your dataset\n",
        "\n",
        "\n",
        "Take the first 300 digits as your dataset.\n",
        "\n",
        "Reshape the image data into 28*28 images and display one of them using cv2_imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjG5qEpEpDBi"
      },
      "outputs": [],
      "source": [
        "#Load MNIST Dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "#from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mnist = fetch_openml('mnist_784', cache=False)\n",
        "\n",
        "print(mnist.data.shape)\n",
        "print(mnist.target.shape)\n",
        "print(np.unique(mnist.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKOJy1fypSA6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')\n",
        "print(X)\n",
        "print(y)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iHOf_rmpSFe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_b8x-cUpYZj"
      },
      "source": [
        "## Extract dense features\n",
        "\n",
        "For each image extract dense daisy features using the following code.\n",
        "\n",
        "More information on daisy can be found here: https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_daisy.html\n",
        "\n",
        "Make sure you reshape the output to obtain a 2d array per image (nb features * feature dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQH_Id0npSIU"
      },
      "outputs": [],
      "source": [
        "from skimage.feature import daisy\n",
        "from skimage import data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for t in range(...):\n",
        "  print(t)\n",
        "  ...\n",
        "  descs, descs_img = daisy(img, step=2, radius=8, rings=2, histograms=6, orientations=8, visualize=True)\n",
        "  ...\n",
        "\n",
        "\n",
        "#fig, ax = plt.subplots()\n",
        "#ax.axis('off')\n",
        "#ax.imshow(descs_img)\n",
        "#descs_num = descs.shape[0] * descs.shape[1]\n",
        "#ax.set_title('%i DAISY descriptors extracted:' % descs_num)\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILBZ4eNepjTy"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "Perform l2 normalization on each feature descriptor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3XANUcgpSKf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjO9aJBkpSNC"
      },
      "outputs": [],
      "source": [
        "#You can check that the normalization is properly done.\n",
        "print(np.sum(np.power(full_desc2[0,0,:],2.0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdPjo09cppAO"
      },
      "source": [
        "##Clustering and embeddings\n",
        "\n",
        "Look and try to understand the following codes.\n",
        "These aggregators include the clustering.\n",
        "\n",
        "After dividing your data into train and test set, perform the clustering with k=100 clusters on all the features of the train set.\n",
        "\n",
        "Then, compute the bow and vlad descriptors for each image of the train and test sets.\n",
        "\n",
        "Pay attention to the normalization apllied on the computed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZe-YvfTpSPc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class BaseAggregator(BaseEstimator):\n",
        "    \"\"\"Implement any functions that can be shared among all feature\n",
        "    aggregation methods.\"\"\"\n",
        "\n",
        "    def __init__(self, dimension_ordering=\"tf\"):\n",
        "        self.dimension_ordering = dimension_ordering\n",
        "\n",
        "    def _reshape_local_features(self, X):\n",
        "        \"\"\"Reshape a n-dimensional array into a 2d array of local features.\n",
        "        Account for the case that X is a list because not all samples have\n",
        "        the same number of local features.\n",
        "        \"\"\"\n",
        "        if len(X) == 0:\n",
        "            raise ValueError(\"X cannot be empty\")\n",
        "\n",
        "        dims = len(X[0]) if self.dimension_ordering == \"th\" else X[0].shape[-1]\n",
        "        if isinstance(X, list):\n",
        "            arrays = [\n",
        "                x.T.reshape(-1, dims)\n",
        "                if self.dimension_ordering == \"th\"\n",
        "                else x.reshape(-1, dims)\n",
        "                for x in X\n",
        "            ]\n",
        "            lengths = [len(x) for x in arrays]\n",
        "            X = np.vstack(arrays)\n",
        "        else:\n",
        "            if self.dimension_ordering == \"th\":\n",
        "                X = X.transpose(*([0] + range(2, len(X.shape)) + [1]))\n",
        "            lengths = [int(np.prod(X.shape[1:-1]))]*X.shape[0]\n",
        "            X = X.reshape(-1, dims)\n",
        "\n",
        "        return X, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uTmbdwnpSR9"
      },
      "outputs": [],
      "source": [
        "\"\"\"Quantize local features and aggregate them in a Bag Of Words manner\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import cluster\n",
        "\n",
        "#from .base import BaseAggregator\n",
        "\n",
        "\n",
        "class BagOfWords(BaseAggregator):\n",
        "    \"\"\"Compute a Bag of Words model and aggregate local features with it.\n",
        "\n",
        "    Train a MiniBatchKMeans on the data and then use the centroids as a\n",
        "    codebook to encode any set of local features.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_codewords : int\n",
        "                  The codebook size aka the number of clusters\n",
        "    l1_norm : boolean\n",
        "              Whether to normalize the transformed data or not\n",
        "    dimension_ordering : {'th', 'tf'}\n",
        "                         Changes how n-dimensional arrays are reshaped to form\n",
        "                         simple local feature matrices. 'th' ordering means the\n",
        "                         local feature dimension is the second dimension and\n",
        "                         'tf' means it is the last dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_codewords, l1_norm=True, dimension_ordering=\"tf\"):\n",
        "        self.n_codewords = n_codewords\n",
        "        self.l1_norm = l1_norm\n",
        "        self._clusterer = cluster.MiniBatchKMeans(\n",
        "            n_clusters=self.n_codewords,\n",
        "            n_init=1,\n",
        "            compute_labels=False\n",
        "        )\n",
        "\n",
        "        super(self.__class__, self).__init__(dimension_ordering)\n",
        "\n",
        "    @property\n",
        "    def centroids(self):\n",
        "        \"\"\"The centroids of the encoding\"\"\"\n",
        "        return self._clusterer.cluster_centers_.copy()\n",
        "\n",
        "    @centroids.setter\n",
        "    def centroids(self, _centroids):\n",
        "        self._clusterer.cluster_centers_ = _centroids.copy()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Build the codebook for the Bag of Words model.\n",
        "        Apply the clustering algorithm to the data and use the cluster centers\n",
        "        as codewords for the codebook.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array_like or list\n",
        "            The local features to train on. They must be either nd arrays or\n",
        "            a list of nd arrays.\n",
        "        \"\"\"\n",
        "        X, _ = self._reshape_local_features(X)\n",
        "\n",
        "        self._clusterer.fit(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X, y=None):\n",
        "        \"\"\"Partially learn the codebook from the provided data.\n",
        "        Run a single iteration of the minibatch KMeans on the provided data.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array_like or list\n",
        "            The local features to train on. They must be either nd arrays or\n",
        "            a list of nd arrays.\n",
        "        \"\"\"\n",
        "        X, _ = self._reshape_local_features(X)\n",
        "        self._clusterer.partial_fit(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Compute the Bag of Words representation of the provided data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array_like or list\n",
        "            The local features to aggregate. They must be either nd arrays or\n",
        "            a list of nd arrays. In case of a list each item is aggregated\n",
        "            separately.\n",
        "        \"\"\"\n",
        "        # Get the local features and the number of local features per document\n",
        "        X, lengths = self._reshape_local_features(X)\n",
        "\n",
        "        # Preprocess the lengths list into indexes in the local feature array\n",
        "        starts = np.cumsum([0] + lengths).astype(int)\n",
        "        ends = np.cumsum(lengths).astype(int)\n",
        "\n",
        "        # Transform and aggregate the local features\n",
        "        words = self._clusterer.predict(X)\n",
        "        bow = np.vstack([\n",
        "            np.histogram(\n",
        "                words[s:e],\n",
        "                bins=np.arange(self.n_codewords + 1) - 0.5,\n",
        "                density=False\n",
        "            )[0]\n",
        "            for s, e in zip(starts, ends)\n",
        "        ])\n",
        "\n",
        "        if self.l1_norm:\n",
        "            bow = bow.astype(float) / bow.sum(axis=1).reshape(-1, 1)\n",
        "\n",
        "        return bow\n",
        "\n",
        "    def inertia(self, X):\n",
        "        \"\"\"Return the value of the KMeans objective function on the provided\n",
        "        data.\n",
        "        X : array_like or list\n",
        "            The local features to train on. They must be either nd arrays or\n",
        "            a list of nd arrays.\n",
        "        \"\"\"\n",
        "        X, _ = self._reshape_local_features(X)\n",
        "\n",
        "        return -self._clusterer.score(X)\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        \"\"\"Return the negative inertia so that the best score is the max\n",
        "        score.\n",
        "        X : array_like or list\n",
        "            The local features to train on. They must be either nd arrays or\n",
        "            a list of nd arrays.\n",
        "        \"\"\"\n",
        "        return -self.inertia(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFKhPQmNqDOh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTRqtW4YpSUH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyWt4-O9qKfu"
      },
      "source": [
        "##Optional:\n",
        "\n",
        "Similarly compute VLAD embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OC-acDYqKsP"
      },
      "outputs": [],
      "source": [
        "\"\"\"Quantize local features and aggregate them using the Vector of Locally\n",
        "Aggregated Descriptors (VLAD) encoding\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import cluster\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "\n",
        "class Vlad(BaseAggregator):\n",
        "    \"\"\"Compute a VLAD model and aggregate local features with it.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_codewords: int\n",
        "                 The codebook size aka the number of clusters.\n",
        "    inner_batch: int\n",
        "                 The batch size used to compute the differences between\n",
        "                 the feature descriptors and the centroids.\n",
        "    normalization: int\n",
        "                   A bitmask of possible normalizations\n",
        "    dimension_ordering : {'th', 'tf'}\n",
        "                         Changes how n-dimensional arrays are reshaped to form\n",
        "                         simple local feature matrices. 'th' ordering means the\n",
        "                         local feature dimension is the second dimension and\n",
        "                         'tf' means it is the last dimension.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    POWER_NORMALIZATION = 1\n",
        "    L2_NORMALIZATION = 2\n",
        "\n",
        "    def __init__(self, n_codewords, normalization=3, inner_batch=128,\n",
        "                 dimension_ordering=\"tf\"):\n",
        "        self.n_codewords = n_codewords\n",
        "        self.inner_batch = inner_batch\n",
        "        self.normalization = normalization\n",
        "\n",
        "        self._clusterer = cluster.MiniBatchKMeans(\n",
        "            n_clusters=self.n_codewords,\n",
        "            n_init=1,\n",
        "            compute_labels=False\n",
        "        )\n",
        "\n",
        "        super(self.__class__, self).__init__(dimension_ordering)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Build the codebook for the VLAD model using KMeans.\n",
        "        Apply the clustering algorithm to the data and use the cluster centers\n",
        "        as codewords for the codebook.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array_like or list\n",
        "            The local features to train on. They must be either nd arrays or\n",
        "            a list of nd arrays.\n",
        "        \"\"\"\n",
        "        X, _ = self._reshape_local_features(X)\n",
        "\n",
        "        self._clusterer.fit(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X, y=None):\n",
        "        \"\"\"Partially learn the codebook from the provided data.\n",
        "        Run a single iteration of the minibatch KMeans on the provided data.\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array_like or list\n",
        "            The local features to train on. They must be either nd arrays or\n",
        "            a list of nd arrays.\n",
        "        \"\"\"\n",
        "        X, _ = self._reshape_local_features(X)\n",
        "        self._clusterer.partial_fit(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Compute the Bag of Words representation of the provided data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array_like or list\n",
        "            The local features to aggregate. They must be either nd arrays or\n",
        "            a list of nd arrays. In case of a list each item is aggregated\n",
        "            separately.\n",
        "        \"\"\"\n",
        "        # Get the local features and the number of local features per document\n",
        "        X, lengths = self._reshape_local_features(X)\n",
        "\n",
        "        # Preprocess the lengths list into indexes in the local feature array\n",
        "        starts = np.cumsum([0] + lengths).astype(int)\n",
        "        ends = np.cumsum(lengths).astype(int)\n",
        "\n",
        "        words = self._clusterer.predict(X)\n",
        "        dims = len(X[0])\n",
        "\n",
        "        vlad = np.zeros((len(lengths), dims*self.n_codewords))\n",
        "        v = np.zeros((self.inner_batch, self.n_codewords, dims))\n",
        "        for i, (s, e) in enumerate(zip(starts, ends)):\n",
        "            for j in range(s, e, self.inner_batch):\n",
        "                ee = min(j+self.inner_batch, e)\n",
        "\n",
        "                v.fill(0)\n",
        "                v[range(ee-j), words[j:ee]] = \\\n",
        "                    X[j:ee] - self._clusterer.cluster_centers_[words[j:ee]]\n",
        "                vlad[i] += v[:ee-j].sum(axis=0).ravel()\n",
        "            vlad[i] /= lengths[i]\n",
        "\n",
        "        # Check if we should be normalizing the power\n",
        "        if self.normalization & self.POWER_NORMALIZATION:\n",
        "            vlad = np.sqrt(np.abs(vlad))*np.sign(vlad)\n",
        "\n",
        "        # Check if we should be performing L2 normalization\n",
        "        if self.normalization & self.L2_NORMALIZATION:\n",
        "            vlad /= np.sqrt(np.sum(vlad**2, axis=1)).reshape(-1, 1)\n",
        "\n",
        "        return vlad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W4T-tS_qy59"
      },
      "source": [
        "## Classification with SVM\n",
        "\n",
        "For BOW and VLAD perform classification of the small MNIST dataset with linear SVMs.\n",
        "\n",
        "Make sure your descriptors are L2-normalized and use sklearn:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "- give results for all encoding and various C values.\n",
        "- compare BOW and VLAD to using the full image (flatten) as input.\n",
        "- compute the kernels on train and test sets and compute the SVM again\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0oYncRhqzNQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvOk0bgJqzP5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUiD8Q2fq7-l"
      },
      "source": [
        "##Optional\n",
        "\n",
        "Using feature-aggregation code: https://github.com/paschalidoud/feature-aggregation/tree/master/feature_aggregation\n",
        "\n",
        "Compute fisher vectors for the dataset and evaluate its classifcation performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNPaB8pNq8hX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5AnNwforCwx"
      },
      "source": [
        "##Optional\n",
        "\n",
        "Evaluate different number of clusters and normalization strategies for the studied embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4eTp4ODrDOW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
