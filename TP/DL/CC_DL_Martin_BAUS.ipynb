{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlfwAnxjLxYz"
      },
      "source": [
        "# Code fourni : GAN Fully Connected sur Mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CC - Martin BAUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yoU32HB2yyAF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Seed:  4311\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x288d3b7d910>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#cudnn.benchmark = True\n",
        "\n",
        "#set manual seed to a constant get a consistent output\n",
        "manualSeed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2LTwJ49SW8s"
      },
      "source": [
        "## Modification du data loader\n",
        "\n",
        "* Le dataloader ci-dessous a été modifié pour retourner des batchs d'exemples avec leurs labels sous forme de vecteurs on-hot-encodings\n",
        "\n",
        "* Pour le vérifier vous pouvez taper la commande suivante et regarder les tailles de inputs, labels et le contenu de labels :\n",
        "\n",
        "  `inputs, labels = next(iter(dataloader))`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y6GPUxdoOpRw"
      },
      "outputs": [],
      "source": [
        "#loading the dataset\n",
        "from torchvision.transforms import Lambda\n",
        "\n",
        "\n",
        "dataset = dset.MNIST(root='./data', download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.Resize(28),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.5,), (0.5,)),\n",
        "                       ]), target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) )\n",
        "#number of channels in image(since the image is grayscale the number of channels are 1)\n",
        "nc=1\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5aCGuhWbS8xw"
      },
      "outputs": [
        {
          "ename": "PicklingError",
          "evalue": "Can't pickle <function <lambda> at 0x00000288DB665E40>: attribute lookup <lambda> on __main__ failed",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x00000288DB665E40>: attribute lookup <lambda> on __main__ failed"
          ]
        }
      ],
      "source": [
        "inputs, labels = next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JTHpet-pzJD1"
      },
      "outputs": [],
      "source": [
        "nz = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J4FsZpzfrKw"
      },
      "outputs": [],
      "source": [
        "class GeneratorLinear(nn.Module):\n",
        "    def __init__(self, nz=50, nh1=128, nh2=256, noutput=784):\n",
        "        super(GeneratorLinear, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.Linear(nz,nh1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh1,nh2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh2,noutput),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYbM03sefrKx"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorLinear(nn.Module):\n",
        "  def __init__(self, ninputs=784, nh1=128, nh2= 64 ):\n",
        "        super(DiscriminatorLinear, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(ninputs,nh1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh1, nh2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh2,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.main(input)\n",
        "    return output.view(-1, 1).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uDfQiKSlfrKx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GeneratorLinear(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=60, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=784, bias=True)\n",
            "    (5): Tanh()\n",
            "  )\n",
            ")\n",
            "DiscriminatorLinear(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (5): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "netG = GeneratorLinear().to(device)\n",
        "print(netG)\n",
        "netD = DiscriminatorLinear().to(device)\n",
        "print(netD)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.002, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "uCh2DxvJzFn3"
      },
      "outputs": [
        {
          "ename": "PicklingError",
          "evalue": "Can't pickle <function <lambda> at 0x00000288DB665E40>: attribute lookup <lambda> on __main__ failed",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Commented out IPython magic to ensure Python compatibility.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(niter):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m###########################\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# train with real\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         netD\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m         real_cpu \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m784\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x00000288DB665E40>: attribute lookup <lambda> on __main__ failed"
          ]
        }
      ],
      "source": [
        "fixed_noise = torch.randn(64, nz, device=device)\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "niter = 25\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "for epoch in range(niter):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].view([-1,784]).to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,), real_label, device=device, dtype=torch.float)\n",
        "\n",
        "        output = netD(real_cpu)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, device=device)\n",
        "        fake = netG(noise)\n",
        "        #label.fill_(fake_label)\n",
        "        label = torch.full((batch_size,), fake_label, device=device, dtype=torch.float)\n",
        "\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                   % (epoch, niter, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        if i % 100 == 0:\n",
        "            fake = netG(fixed_noise)\n",
        "            fake_images_np = fake.cpu().detach().numpy()\n",
        "            fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
        "            R, C = 5, 5\n",
        "            num_images_to_plot = min(batch_size, R * C)  # Limit images to plot to grid size\n",
        "            for i in range(num_images_to_plot):\n",
        "                plt.subplot(R, C, i + 1)\n",
        "                plt.imshow(fake_images_np[i], cmap='gray')\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQNzi9BJxhlm"
      },
      "source": [
        "# 1. Implémentation d'un CGAN\n",
        "\n",
        "On vous demande d'implémenter un CGAN en partant du code ci dessus (pas du votre ni d'un code récupéré sur internet). **Le code demandé doit être très proche du code fourni dans ce notebook pour le GAN.** Cela siognifie que vous ne devez modifier que ce qui doit être modifié dans le code du GAN.\n",
        "\n",
        "On vous rappelle le critère d'optimisation d'un CGAN :\n",
        "\n",
        "$$\\min_{\\theta_g} \\max_{\\theta_d} v(\\theta_g, \\theta_d) = \\mathbb{E}_{x,y \\sim p_{data}} [\\log d(x,y)] +\\mathbb{E}_{z\\sim p_z, y' \\sim p_y} [log(1-d(g(z,y'), y')]$$\n",
        "\n",
        "où le générateur comme le discriminateur prenne en entrée le y, en plus du z (pour le générateur), en plus du x pour le discriminateur. Dans un CGAN le générateur comme le classifieur doivent prendre en entrée la condition de la génération. Ici vous choisirez d'utiliser la classe de l'exemple.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Questions\n",
        "\n",
        "Vous devez dans la suite écrire la classe du générateur, du discriminateur et la fonction de train pour optimiser le critère ci-dessus du CGAN.\n",
        "\n",
        "Vous utiliserez un on-hot-encoding pour représenter les y (en dimension 10 donc) en entrée du classifieur et du discriminateur.   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLOCr8abRKSv"
      },
      "source": [
        "## Aides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wfYqyLYRL5y"
      },
      "source": [
        "1. Voici ci-dessous un exemple de comment modifier le générateur pour prendre en entrée des couples (z,y) et produire une sortie qui dépend de ces deux entrées.\n",
        "\n",
        "```\n",
        "class GeneratorLinear(nn.Module):\n",
        "    def __init__(self, nz=50, nh1=128, nh2=256, noutput=784):\n",
        "        super(GeneratorLinear, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.Linear(nz+10,nh1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh1,nh2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh2,noutput),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, input1, input2):\n",
        "        input = torch.cat((input1,input2),1)\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#On modifie le génerateur et Discriminateur pour qu'ils prennent en compte les 2 entrées\n",
        "\n",
        "class GeneratorLinear(nn.Module):\n",
        "    def __init__(self, nz=50, nh1=128, nh2=256, noutput=784):\n",
        "        super(GeneratorLinear, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.Linear(nz+10,nh1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh1,nh2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh2,noutput),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, input1, input2):\n",
        "        input = torch.cat((input1,input2),1)\n",
        "        output = self.main(input)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DiscriminatorLinear(nn.Module):\n",
        "  def __init__(self, ninputs=784, nh1=128, nh2= 64 ):\n",
        "        super(DiscriminatorLinear, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(ninputs,nh1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh1, nh2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nh2,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "    input = torch.cat((input1,input2),1)\n",
        "    output = self.main(input)\n",
        "    return output.view(-1, 1).squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s73QaiNbzSTR"
      },
      "source": [
        "2. Vous pouvez à chaque itération générer des exemples avec le CVGAN en utilisant des fixed_noise comme pour le GAN et des fixed_inputs pour les y. Cela vous aidera à voir si le modèle apprend à générer les bons chiffres pendant l'apprentissage. Vous pouvez utiliser par exemple :\n",
        "\n",
        "  `fixed_input = next(iter(dataloader))[1][:64,:] # où 64 est le batch_size`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VejS9mgMRyg7"
      },
      "source": [
        "3. Vous pouvez générer des vecteurs aléatoires de one-hot-encoding à 10 classes avec la ligne suivante:\n",
        "  \n",
        "  `input2 = one_hot(torch.randint(0, 9, (batch_size,)),num_classes=10)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#1: Implémentation d'un CGAN\n",
        "#On reprend le code d'un GAN \n",
        "\n",
        "fixed_noise = torch.randn(64, nz, device=device)\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "niter = 25\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "for epoch in range(niter):\n",
        "    fixed_input = next(iter(dataloader))[1][:64,:] #test pour fixer les yrand pour chaque itération\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x,y)) + log(1 - D(G(z,y),y))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].view([-1,784]).to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,fixed_input), real_label, device=device, dtype=torch.float)\n",
        "\n",
        "        output = netD(real_cpu)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, device=device)\n",
        "        fake = netG(noise)\n",
        "        #label.fill_(fake_label)\n",
        "        label = torch.full((batch_size,), fake_label, device=device, dtype=torch.float)\n",
        "\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                   % (epoch, niter, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        if i % 100 == 0:\n",
        "            fake = netG(fixed_noise)\n",
        "            fake_images_np = fake.cpu().detach().numpy()\n",
        "            fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
        "            R, C = 5, 5\n",
        "            num_images_to_plot = min(batch_size, R * C)  # Limit images to plot to grid size\n",
        "            for i in range(num_images_to_plot):\n",
        "                plt.subplot(R, C, i + 1)\n",
        "                plt.imshow(fake_images_np[i], cmap='gray')\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVMF3pv_Tv_m"
      },
      "source": [
        "# 2. Génération d'images \"dans le style\" d'une image donnée MNIST\n",
        "\n",
        "Une fois le CGAN  appris, utilisez le pour générer des images $i$ avec des $z$ et des $y$ variés. Le $y$ conditionne la classe de l'image générée et $z$ représente le reste de l'information du tracé (image penchée, trait épais etc). Collectez les triplets $(z,y,i)$.\n",
        "\n",
        "Cela vous permet de constituer une base d'apprentissage pour apprendre un modèle $F$ à retrouver le $z$ à partir d'une image (d'une certaine façon vous apprenez a ionverser le Générateur): Ce modèle prend en entrée une image $i$ et prédit en sortie le $z$ correspondant. L'apprentissage peut être réalisé par minimisation du critère MSE (c'est une tâche de regression).\n",
        "\n",
        "Une fois l'apprentissage du modèle $F$ réalisé vous pouvez l'utiliser pour générer des images de différents chiffre \"dans le style\" d'une image MNIST donnée, disons $i$ de la façon suivante. CHoisissez une image MNIST au hasard. Utilisez $F(i)$ pour prédire un vecteur $z$ corrsondant à l'image. Puis utilisez $F(i)$ en entrée du générateur conditionnel avec différentes vecteurs $y$ pour générer des chiffres \"dans le style\" de l'image $i$.\n",
        "\n",
        "Réalisez les implémentations demandées"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
