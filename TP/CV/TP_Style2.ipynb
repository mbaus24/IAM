{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkkAFuMGsfou"
      },
      "source": [
        "<h1>TP8 Deep Learning</h1>\n",
        "\n",
        "Cette huitième séance est composée de deux parties :\n",
        "* Transfert de style, basé sur l'article \"Image Style Transfer Using Convolutional Neural Networks\" [Gatys, 2016]\n",
        "* Transférabilité de features, basé sur l'article \"How transferable are features in deep neural networks?\" [Yosinsky, 2014]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ua8jKmisfov"
      },
      "outputs": [],
      "source": [
        "venv_root = '/amuhome/ayache/deep'    # A modifier !!\n",
        "\n",
        "import sys\n",
        "sys.path.append(venv_root+'/lib/python3.5/site-packages')\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiFN_oCCrk5H"
      },
      "source": [
        "<h2> Transfert de style</h2>\n",
        "\n",
        "Cette partie vise à l'implémentation de la méthode présentée dans l'article \"Image Style Transfer Using Convolutional Neural Networks\", permettant de transférer le style d'une image <i>S</i> sur une image de contenu <i>C</i>. L'approche consiste à trouver l'image qui minimise une fonction <i>loss</i> constituée de deux termes : l'un visant à aligner les features d'une couche élevée entre l'image <i>contenue</i> et l'image générée <i>x</i> ; l'autre aligne les <i>matrices de gram</i> (covariances) obtenues sur des couches inférieures entre l'image <i>style</i> et l'image générée <i>x</i>.\n",
        "\n",
        "Commençons par télécharger quelques images :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7UTsvIdmmnX"
      },
      "outputs": [],
      "source": [
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style/content_cat.png\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style/content_bridge.png\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style/content_joconde.png\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style/style_hokusai.png\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style/style_hopper.png\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style/stef.jpg\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style/miro.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJTxlXyQqlQX"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "#display(Image.open(\"content_cat.png\"))\n",
        "\n",
        "!pip3 install scipy==1.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OkR18rKoXg1"
      },
      "source": [
        "Pour information, voici ce que vous devriez obtenir après application du style Van Gogh et HokusaÏ. Les résultats peuvent changer selon les constantes définies en début du notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y5fMpTOoZKi"
      },
      "outputs": [],
      "source": [
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style//cat_hokusai.png\n",
        "display(Image.open(\"cat_hokusai.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FWeBohfpG9s"
      },
      "outputs": [],
      "source": [
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_style//cat_vangogh.png\n",
        "display(Image.open(\"cat_vangogh.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr-Tbx7VdGVY"
      },
      "source": [
        "<h4>Import des packages et définition de quelques constantes</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FmGUcAgdGwC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow.keras.backend as K\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from scipy.misc import imresize\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define constants\n",
        "CONTENT_IMG_PATH = 'content_joconde.png'\n",
        "STYLE_IMG_PATH = 'style_hopper.png' #'style_hokusai.png' #'style_vangogh.png' #'style_mead.png' #'style_kandinsky.png' #'style.jpg'\n",
        "\n",
        "# Number of iterations to run\n",
        "ITER = 10\n",
        "\n",
        "# Weights of losses\n",
        "CONTENT_WEIGHT = 0.01\n",
        "STYLE_WEIGHT = 1000.0\n",
        "\n",
        "# Define the shape of the output image\n",
        "h, w = load_img(CONTENT_IMG_PATH).size\n",
        "img_h = 400\n",
        "img_w = int(h * img_h / w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VksTujjadQSd"
      },
      "source": [
        "Les fonctions <i>preprocess</i> et <i>postprocess</i> sont utilisées pour formater et normaliser les images qui passeront dans le réseau VGG19 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrsXUCQPdmxs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor*255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor)>3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return Image.fromarray(tensor)\n",
        "\n",
        "def preprocess(img_path):\n",
        "    # Preprocess an image to feed in VGG19\n",
        "    img = load_img(img_path)\n",
        "    img = imresize(img, (img_h, img_w, 3)) # for tensorflow\n",
        "    img = img_to_array(img)\n",
        "    img = img.astype('float32')\n",
        "    # Add the batch dimension\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)#, data_format=\"channels_last\")\n",
        "    return img\n",
        "\n",
        "\n",
        "def postprocess(img):\n",
        "    # Deprocess array from VGG19 to display image\n",
        "    mean = [103.939, 116.779, 123.68]\n",
        "    img2 = img[..., ::-1]\n",
        "    img2 = img2[..., ::-1]\n",
        "    #img2[..., 0] += mean[0]\n",
        "    #img2[..., 1] += mean[1]\n",
        "    #img2[..., 2] += mean[2]\n",
        "    img2 = img2 + mean\n",
        "    img2 = img2[..., ::-1]\n",
        "    img2 = np.array(img2)\n",
        "    #xx = xx.reshape((img_h, img_w, 3))\n",
        "    #xx = np.clip(xx, 0, 255)\n",
        "    return img2.astype('uint8')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = img_to_array(load_img(CONTENT_IMG_PATH))\n",
        "print(img)\n",
        "imgp = vgg19.preprocess_input(img)\n",
        "\n",
        "mean = [103.939, 116.779, 123.68]\n",
        "img2 = imgp[..., ::-1]\n",
        "img2 = img2[..., ::-1]\n",
        "img2[..., 0] += mean[0]\n",
        "img2[..., 1] += mean[1]\n",
        "img2[..., 2] += mean[2]\n",
        "img2 = img2[..., ::-1]\n",
        "print(\"img2\")\n",
        "print(img2)\n",
        "Image.fromarray(img2.astype('uint8'))"
      ],
      "metadata": {
        "id": "i6LlqxDHdQY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jVo2OXZdoxK"
      },
      "source": [
        "<h4>Chargement d'un modèle préentrainé de l'architecture VGG19</h4>\n",
        "On construit ici un tenseur <i>input_tensor</i> qui contient les images <i>contenu</i>, <i>style</i>, et celle <i>générée</i>. Ceci permet d'obtenir en une seule passe les features vgg19 des trois images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5nyK0LJgN1j"
      },
      "outputs": [],
      "source": [
        "# Create Keras variables of input images\n",
        "#content_img = K.variable(preprocess(CONTENT_IMG_PATH))\n",
        "#style_img = K.variable(preprocess(STYLE_IMG_PATH))\n",
        "#gen_img = K.placeholder(shape=(1, img_h, img_w, 3))\n",
        "\n",
        "# Create a single tensor containing all three images\n",
        "#input_tensor = K.concatenate([content_img, style_img, gen_img], axis=0)\n",
        "\n",
        "# Create a vgg19 model by running the input tensor though the vgg19 convolutional\n",
        "# neural network, excluding the fully connected layers\n",
        "#model = vgg19.VGG19(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
        "\n",
        "# Create an output dictionary\n",
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False\n",
        "print('VGG model loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affiche la liste des couches du modèle et définissions des représentations de contenus et de style"
      ],
      "metadata": {
        "id": "RcHh138jGHo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "for layer in vgg.layers:\n",
        "  print(layer.name)\n",
        "\n",
        "content_layers = ['block5_conv2']\n",
        "\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)\n"
      ],
      "metadata": {
        "id": "oPeJcIMRGISm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La \"functional API\" de Keras permet de créer des modèles selon leur entrrées et sorties. Ici on créé un modèle qui retourne des couches internes de l'architecture VGG19.\n"
      ],
      "metadata": {
        "id": "v-0hDcqSI6ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create keras model that outputs internal style layers\n",
        "outputs = [vgg.get_layer(name).output for name in style_layers]\n",
        "style_model = tf.keras.Model([vgg.input], outputs)\n",
        "\n",
        "# create keras model that outputs internal content layers\n",
        "outputs = [vgg.get_layer(name).output for name in content_layers]\n",
        "content_model = tf.keras.Model([vgg.input], outputs)"
      ],
      "metadata": {
        "id": "dr0ugBU8I57f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPEYNO5ogPTm"
      },
      "source": [
        "<h4>Fonctions loss</h4>\n",
        "Les fonctions suivantes permettent de définir la fonction objective pour le transfert de style. Cette loss est composée d'un terme <i>content</i> et d'un terme <i>style</i>. La notion de style est définie par la covariance entre les filtres de convolutions obtenus sur les couches basses du réseau VGG19.\n",
        "\n",
        "<b>A Faire : </b> Compléter les quelques lignes de codes commentées par A COMPLETER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lngUDlZJracZ"
      },
      "outputs": [],
      "source": [
        "# Dot product of the flattened feature map and the transpose of the\n",
        "# flattened feature map. Return a matrix of size (Depth x Depth)\n",
        "# where Depth is number of channels of feature map (ie: #filters)\n",
        "def gram_matrix(x):\n",
        "    #assert K.ndim(x) == 3\n",
        "    #x = K.permute_dimensions(x, (0, 3, 1, 2))\n",
        "    #features = K.reshape(x, (x.shape[0], x.shape[1]*x.shape[2], x.shape[3]))#K.batch_flatten(x)\n",
        "    #gram = K.dot(features, K.transpose(features))\n",
        "    #return gram\n",
        "\n",
        "    gram = tf.linalg.einsum('bijc,bijd->bcd', x, x)\n",
        "    input_shape = tf.shape(x)\n",
        "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "    return gram/(num_locations)\n",
        "\n",
        "\n",
        "    # A COMPLETER\n",
        "    # A COMPLETER\n",
        "    # A COMPLETER\n",
        "    #return gram\n",
        "\n",
        "# MSE of the gram matrices multiplied by the constant for normalization\n",
        "def style_loss(style, gen):\n",
        "    S = gram_matrix(style)\n",
        "    G = gram_matrix(gen)\n",
        "    return K.mean(K.square(S - G)) #/ (4. * (channels ** 2) * (size ** 2))\n",
        "\n",
        "# MSE between features from generated image and input content image\n",
        "def content_loss(content, gen):\n",
        "    #return # A COMPLETER\n",
        "    return K.mean(K.square(gen - content))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNIAXOr7h4Om"
      },
      "source": [
        "<h4>Définition d'une fonction qui renvoie le gradient de la loss par rapport à l'entrée du réseau</h4>\n",
        "\n",
        "La partie ci-dessous est similaire à ce qu'on a vu pour la visualisation de filtre par maximisation d'activations. A la différence que plutôt que de maximiser des activations, on minimise la loss. On définie une fonction (Keras) qui renvoie les gradients, cette fonction nous permettra de mettre à jour l'image générée."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "content_target = content_model(preprocess(CONTENT_IMG_PATH))\n",
        "style_target = style_model(preprocess(STYLE_IMG_PATH))\n",
        "#gen_img = K.placeholder(shape=(1, img_h, img_w, 3))\n",
        "\n",
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #outputs = extractor(image)\n",
        "    #loss = style_content_loss(outputs)\n",
        "    #inputs = inputs*255.0\n",
        "    #preprocessed_input = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    content = content_model(image)\n",
        "    style = style_model(image)\n",
        "\n",
        "    #loss = CONTENT_WEIGHT * content_loss(content_target, content)\n",
        "    #for i in range(num_style_layers):\n",
        "    #    s1 = style_loss(style_target[i], style[i])\n",
        "    #    loss += (STYLE_WEIGHT / num_style_layers) * s1\n",
        "    style_loss = tf.add_n([tf.reduce_mean((gram_matrix(style[i])-gram_matrix(style_target[i]))**2) for i in range(num_style_layers)])\n",
        "    style_loss *= STYLE_WEIGHT / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content[i]-content_target[i])**2) for i in range(num_content_layers)])\n",
        "    content_loss *= CONTENT_WEIGHT / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    tf.print(\"loss\",loss)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "metadata": {
        "id": "T4NPcm7VotOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Styles:')\n",
        "for i in range(num_style_layers):\n",
        "  print(\"  \", vgg.layers[i].name)\n",
        "  print(\"    shape: \", gram_matrix(style_target[i]).numpy().shape)\n",
        "  print(\"    min: \", gram_matrix(style_target[i]).numpy().min())\n",
        "  print(\"    max: \", gram_matrix(style_target[i]).numpy().max())\n",
        "  print(\"    mean: \", gram_matrix(style_target[i]).numpy().mean())\n",
        "  print()\n",
        "\n",
        "print(\"Contents:\")\n",
        "for i in range(num_content_layers):\n",
        "  print(\"  \", vgg.layers[i].name)\n",
        "  print(\"    shape: \", content_target[i].numpy().shape)\n",
        "  print(\"    min: \", content_target[i].numpy().min())\n",
        "  print(\"    max: \", content_target[i].numpy().max())\n",
        "  print(\"    mean: \", content_target[i].numpy().mean())\n",
        "\n",
        "content = content_model(image)\n",
        "style = style_model(image)\n",
        "\n",
        "print('im Styles:')\n",
        "for i in range(num_style_layers):\n",
        "  print(\"  \", vgg.layers[i].name)\n",
        "  print(\"    shape: \", gram_matrix(style[i]).numpy().shape)\n",
        "  print(\"    min: \", gram_matrix(style[i]).numpy().min())\n",
        "  print(\"    max: \", gram_matrix(style[i]).numpy().max())\n",
        "  print(\"    mean: \", gram_matrix(style[i]).numpy().mean())\n",
        "  print()\n",
        "\n",
        "print(\"im Contents:\")\n",
        "for i in range(num_content_layers):\n",
        "  print(\"  \", vgg.layers[i].name)\n",
        "  print(\"    shape: \", content[i].numpy().shape)\n",
        "  print(\"    min: \", content[i].numpy().min())\n",
        "  print(\"    max: \", content[i].numpy().max())\n",
        "  print(\"    mean: \", content[i].numpy().mean())"
      ],
      "metadata": {
        "id": "Esw-cgUOuWZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 0\n",
        "steps_per_epoch = 10\n",
        "image = tf.Variable(preprocess(CONTENT_IMG_PATH), dtype=tf.float32)\n",
        "print(image)\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='', flush=True)\n",
        "  #display.clear_output(wait=True)\n",
        "  #display.display(tensor_to_image(image))\n",
        "  print(\"Train step: {}\".format(step))\n",
        "\n",
        "train_step(image)\n",
        "print(image)\n",
        "img = postprocess(image[0])\n",
        "display(Image.fromarray(img))"
      ],
      "metadata": {
        "id": "Dw3H2IRaqDgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = image[0]*255.\n",
        "print(img)\n",
        "mean = [103.939, 116.779, 123.68]\n",
        "img2 = img[..., ::-1]\n",
        "img2 = img2[..., ::-1]\n",
        "img2 = np.array(img2)\n",
        "img2[..., 0] += mean[0]\n",
        "img2[..., 1] += mean[1]\n",
        "img2[..., 2] += mean[2]\n",
        "img2 = img2[..., ::-1]\n",
        "img2 = img2.astype('uint8')\n",
        "#img = postprocess(image[0])\n",
        "display(Image.fromarray(img2))\n",
        "\n",
        "tensor_to_image(image*255)"
      ],
      "metadata": {
        "id": "VeoTehLoRRPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WS2FpBjjCJd"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "import sys\n",
        "sys.setrecursionlimit(15000)\n",
        "loss = compute_loss()\n",
        "# Calculate gradients of the loss wrt inputs\n",
        "grads = K.gradients(loss, gen_img)\n",
        "\n",
        "# Define a Keras function that return both loss and gradients\n",
        "f_output = K.function([gen_img], [loss] + grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtciSsLQjDF-"
      },
      "source": [
        "<h4>Minimisation par l'algorithme d'optimisation pseudo-newtonien L-BFGS</h4>\n",
        "\n",
        "On utilise la fonction <i>fmin_l_bfgs_b</i> du package <i>scikit</i>, pour cela on a besoin des deux fonctions suivantes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AUo62IQh3cT"
      },
      "outputs": [],
      "source": [
        "grad_values = None\n",
        "def get_grad_values(x):\n",
        "    return grad_values\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "    global grad_values\n",
        "    x = x.reshape((1, img_h, img_w, 3))\n",
        "    # Update the loss and the gradients\n",
        "    outs = f_output([x])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1].flatten().astype('float64')\n",
        "    return loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiVyew5xjmb-"
      },
      "outputs": [],
      "source": [
        "# Run L-BFGS optimizer, start from content image\n",
        "x = preprocess(CONTENT_IMG_PATH)\n",
        "\n",
        "for i in range(ITER):\n",
        "    print('Step', i)\n",
        "    x, min_val, info = fmin_l_bfgs_b(eval_loss_and_grads, x.flatten(), fprime=get_grad_values, maxiter=20)\n",
        "    print('loss:', min_val)\n",
        "\n",
        "    # display image\n",
        "    img = postprocess(x)\n",
        "    display(Image.fromarray(img.astype('uint8')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MatFILMO6K9z"
      },
      "source": [
        "<h2>Finetuning et Transfert de modèles</h2>\n",
        "\n",
        "Cette partie reprend certaines expériences décrites dans l'article \"How transferable are features in deep neural networks?\" [Yosinski, 2014]\"\n",
        "\n",
        "Features that are extracted by NN in their hidden layers are known to be generic. To study this property we will consider two 5-classes classification tasks based on the MNIST dataset: Classifying digits 0 to 4 (task A, and Dataset A) and classifying digits 5 to 9 (task B and Dataset B).\n",
        "\n",
        "- Learn a NN on MNIST data using data from classes 0 à 4, or data from classes 5 to 9. Learn a model for each task using the whole dataset, NNA and NNB. All models have the same default architecture as defined in the function create_modelCNN_MNIST defined below.\n",
        "- Use NNA to initialize models that will be learned on DatasetB. The models have the same architecture but their weights are initialized from those of NNA up to layer number 1, 2 or 3 (other weights are initailized randomly). The models are retrained using data from taskB with datasets of various sizes. Weights initialized from NNA may be frozen or trainable.\n",
        "The same procedure should be applied to use NNB to initialize models that will be retrained on TaskA and datasets A.\n",
        "- Analyze your results and comment on these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTBV4IiScmjr"
      },
      "source": [
        "<h4>Chargement des données</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lMiPqTT8-oj"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "img_rows = img_cols = 28\n",
        "nb_classes = 10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = keras.utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nTZzuf_csF9"
      },
      "source": [
        "<h4>Définition de l'architecture considérée pour l'expérience</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7-Qj_5H7HA1"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "def create_modelCNN_MNIST(nb_classes = 10, nb_filters = 32, pool_size=(2,2), kernel_size = (3,3)):\n",
        "    model1 = Sequential()\n",
        "    model1.add(Conv2D(nb_filters, kernel_size, padding='same', input_shape=input_shape))\n",
        "    model1.add(Activation('relu'))\n",
        "    model1.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model1.add(Conv2D(nb_filters, kernel_size, padding='same'))\n",
        "    model1.add(Activation('relu'))\n",
        "    model1.add(MaxPooling2D(pool_size=pool_size))\n",
        "    model1.add(Dropout(0.5))\n",
        "\n",
        "    model1.add(Flatten())\n",
        "    model1.add(Dense(100))\n",
        "    model1.add(Activation('relu'))\n",
        "    model1.add(Dropout(0.5))\n",
        "    model1.add(Dense(nb_classes))\n",
        "    model1.add(Activation('softmax'))\n",
        "\n",
        "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return(model1)\n",
        "\n",
        "cnn1 = create_modelCNN_MNIST()\n",
        "print (cnn1.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq3rq7Mw_GBJ"
      },
      "source": [
        "<h4> Construction des datasets pour les taches A et B</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvemA-Mw--1r"
      },
      "outputs": [],
      "source": [
        "indices_0to4 = np.where(y_train <5)\n",
        "Y_train_04 = np.squeeze(Y_train[indices_0to4,0:5])\n",
        "X_train_04 = X_train[indices_0to4]\n",
        "y_train_04 = np.argmax(Y_train_04, axis=1)\n",
        "\n",
        "indices_0to4 = np.where(y_test <5)\n",
        "Y_test_04 = np.squeeze(Y_test[indices_0to4,0:5])\n",
        "X_test_04 = X_test[indices_0to4]\n",
        "y_test_04 = np.argmax(Y_test_04, axis=1)\n",
        "\n",
        "print (Y_train_04.shape, X_train_04.shape, Y_test_04.shape, X_test_04.shape)\n",
        "\n",
        "indices_5to9 = np.where(y_train >4)\n",
        "Y_train_59 = np.squeeze(Y_train[indices_5to9, 5:])\n",
        "X_train_59 = X_train[indices_5to9]\n",
        "y_train_59 = np.argmax(Y_train_59, axis=1)\n",
        "\n",
        "indices_5to9 = np.where(y_test >4)\n",
        "Y_test_59 = np.squeeze(Y_test[indices_5to9,5:])\n",
        "X_test_59 = X_test[indices_5to9]\n",
        "y_test_59 = np.argmax(Y_test_59, axis=1)\n",
        "\n",
        "print (Y_train_59.shape, X_train_59.shape, Y_test_59.shape, X_test_59.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSSmPbZz_ZwM"
      },
      "source": [
        "<h4>Apprentissage du modèle sur le dataset A sur les digits 0 .. 4 </h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNktkJla_fHM"
      },
      "outputs": [],
      "source": [
        "cnn_04 = create_modelCNN_MNIST(nb_classes=5)\n",
        "print (cnn_04.summary())\n",
        "\n",
        "batch_size = 128\n",
        "nb_epoch = 20\n",
        "\n",
        "WEIGHTS_FNAME = 'Mnist_Cnn04_My_weights.hdf'\n",
        "\n",
        "cnn_04.fit(X_train_04, Y_train_04, batch_size=batch_size, nb_epoch=nb_epoch, validation_data=(X_test_04, Y_test_04))\n",
        "cnn_04.save_weights(WEIGHTS_FNAME)\n",
        "\n",
        "score = cnn_04.evaluate(X_test_04, Y_test_04, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ZRZ7FUACtO"
      },
      "source": [
        "<h4>Apprentissage du modèle sur le dataset B sur les digits 5 .. 9 </h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG_0zE8cANmZ"
      },
      "outputs": [],
      "source": [
        "cnn_59 = create_modelCNN_MNIST(nb_classes=5)\n",
        "print (cnn_59.summary())\n",
        "\n",
        "WEIGHTS_FNAME = 'Mnist_Cnn59_My_weights.hdf'\n",
        "\n",
        "cnn_59.fit(X_train_59, Y_train_59, batch_size=batch_size, nb_epoch=nb_epoch, validation_data=(X_test_59, Y_test_59))\n",
        "cnn_59.save_weights(WEIGHTS_FNAME)\n",
        "\n",
        "score = cnn_59.evaluate(X_test_59, Y_test_59, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNJcrgBEappx"
      },
      "source": [
        "<b>A faire :</b> On s'intéresse à reproduire les expériences décrites dans l'article. Constuire un modèle où les premières couches 1, 2 ou 3 sont initialisées à partir du modèle NNA, les autres couches sont initialisées aléatoirement. Le modèle est réentrainé avec les données de taskB. Les poids provenant de NNA peuvent être réentrainés ou figés (utilisés l'attribut trainable = False pour figer une couche). Faire la même expérience en inversant les modèles/taches (NNB, entrainé sur TaskA)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}