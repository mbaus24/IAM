{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqoZ7gWZQbtP"
      },
      "source": [
        "# Building a regression model in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w2qrXRCrH19"
      },
      "source": [
        "## Imports and data download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q87z9dR3rTUy"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UNfG68LyQutQ"
      },
      "outputs": [],
      "source": [
        "# Read data\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# train-test split for model evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "# Convert to 2D PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B7ywcRaUwmju"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.return_types.min(\n",
            "values=tensor([   0.4999,    1.0000,    0.8462,    0.3333,    3.0000,    0.6923,\n",
            "          32.5400, -124.3500]),\n",
            "indices=tensor([ 2185,  3569,  4502,    35,  5305,  9929, 11203, 14238])) torch.return_types.max(\n",
            "values=tensor([ 1.5000e+01,  5.2000e+01,  1.4191e+02,  3.4067e+01,  3.5682e+04,\n",
            "         5.9971e+02,  4.1950e+01, -1.1431e+02]),\n",
            "indices=tensor([  863,     6, 10274,   586, 11250,  7755,  7578,  9503])) tensor([ 3.8593e+00,  2.8601e+01,  5.4272e+00,  1.0984e+00,  1.4298e+03,\n",
            "         2.9846e+00,  3.5620e+01, -1.1956e+02]) tensor([1.8682e+00, 1.2559e+01, 2.6983e+00, 5.2668e-01, 1.1472e+03, 5.1250e+00,\n",
            "        2.1339e+00, 2.0008e+00])\n"
          ]
        }
      ],
      "source": [
        "print(X_train.min(axis=0), X_train.max(axis=0) , X_train.mean(axis=0), X_train.std(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UsDcbVKvwmjv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.1500) tensor(5.0000) tensor(2.0649)\n"
          ]
        }
      ],
      "source": [
        "print(y_train.min(), y_train.max() , y_train.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_6gavtKJvCFd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scalerX = StandardScaler()\n",
        "scalerY = StandardScaler()\n",
        "\n",
        "X_train = scalerX.fit_transform(X_train)\n",
        "X_test = scalerX.transform(X_test)\n",
        "\n",
        "y_train = scalerY.fit_transform(y_train)\n",
        "y_test = scalerY.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kcq49nB2vo9V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.79827833 -2.19781881 -1.69781556 -1.45262355 -1.24377606 -0.4472967\n",
            " -1.44362408 -2.39448381] [  5.96354843   1.86324794  50.58246156  62.59906806  29.85891845\n",
            " 116.43843568   2.96630161   2.62361252] [ 1.05128061e-16 -6.63813005e-17 -1.13266264e-16 -7.51362087e-17\n",
            "  5.80202384e-18 -2.31341291e-17 -1.11424986e-15  2.17392227e-15] [1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(X_train.min(axis=0), X_train.max(axis=0) , X_train.mean(axis=0), X_train.std(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z8DqooPxvo9W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.673581553951286 2.565210915492403 -9.639812847579404e-17\n"
          ]
        }
      ],
      "source": [
        "print(y_train.min(), y_train.max() , y_train.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3e_n6dZGwUZt"
      },
      "outputs": [],
      "source": [
        "# Convert to 2D PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intcTXdaQ-Nk"
      },
      "source": [
        "## Define a model and learn it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x1fv1KxNRFLP"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "      nn.Linear(8,8),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(8, 8),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(8, 1)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jQMZ4TSG-M3h"
      },
      "outputs": [],
      "source": [
        "def reinit_model(model):\n",
        "  for layer in model.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "       layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CSJV-YarY0JO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 452/452 [00:00<00:00, 1175.29batch/s]\n"
          ]
        }
      ],
      "source": [
        "#Useful\n",
        "epoch =1\n",
        "batch_size = 32\n",
        "batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0) as bar:\n",
        "  bar.set_description(f\"Epoch {epoch}\")\n",
        "  for start in bar:\n",
        "    i=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8cLtE7zZRFLQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, n_epochs, batch_size, X_train, y_train, X_test, y_test, loss_fn_train, loss_fn_test, optimizer):\n",
        "\n",
        "  history = []\n",
        "  batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      model.train()\n",
        "      with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0) as bar:\n",
        "          bar.set_description(f\"Epoch {epoch}\")\n",
        "          running_loss=0.0\n",
        "          N=1\n",
        "          for start in bar:\n",
        "              # take a batch\n",
        "              model.train()\n",
        "              X_batch = X_train[start:start+batch_size]\n",
        "              y_batch = y_train[start:start+batch_size]\n",
        "              # forward pass\n",
        "              y_pred = model(X_batch)\n",
        "              loss = loss_fn_train(y_pred, y_batch)\n",
        "\n",
        "              # backward pass\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "\n",
        "              # update weights\n",
        "              optimizer.step()\n",
        "\n",
        "              # print progress\n",
        "              running_loss+=loss\n",
        "              bar.set_postfix(mse=float(running_loss/(N+0.0)))\n",
        "              N = N+1\n",
        "          # evaluate accuracy on the test set at end of each epoch and append it to hiostory\n",
        "              model.eval()\n",
        "              y_pred = model(X_test)\n",
        "              loss = loss_fn_test(y_pred, y_test)\n",
        "              history.append(loss.item())\n",
        "          \n",
        "\n",
        "  return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmG6fSPyhSs9"
      },
      "source": [
        "# To do\n",
        "\n",
        "1.   Learn few model with MSE Loss in order to achieve reasonable loss (to be defined). This will allow you to select an architecture with an optimizer, a bacth size etc to perform next experiments\n",
        "2.   Add a small noise (e.g. gaussian noise with null mean and variance 0.1 or 0.2) to the target to predict, and measure how the performance of above selected predictor degrades when trainign with such data.\n",
        "3. Define an $\\epsilon$-insensitive loss function for the MSE loss. The $\\epsilon$-insensitive loss function is defined as $min(mse- \\epsilon,0)$. What is the effect if such a loss ?\n",
        "4. Use the above loss function (with different values of $\\epsilon$) for learning while testing is still evaluated with MSE loss. Comment the results obtained.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'reinit_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mreinit_model\u001b[49m(model)\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      3\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'reinit_model' is not defined"
          ]
        }
      ],
      "source": [
        "reinit_model(model)\n",
        "batch_size = 32\n",
        "n_epochs = 10\n",
        "loss_fn_train = nn.MSELoss()\n",
        "loss_fn_test = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 452/452 [00:02<00:00, 156.48batch/s, mse=0.392]\n",
            "Epoch 1: 100%|██████████| 452/452 [00:04<00:00, 106.56batch/s, mse=0.296]\n",
            "Epoch 2: 100%|██████████| 452/452 [00:03<00:00, 116.18batch/s, mse=0.283]\n",
            "Epoch 3: 100%|██████████| 452/452 [00:03<00:00, 126.35batch/s, mse=0.278]\n",
            "Epoch 4: 100%|██████████| 452/452 [00:03<00:00, 116.49batch/s, mse=0.269]\n",
            "Epoch 5: 100%|██████████| 452/452 [00:04<00:00, 96.41batch/s, mse=0.263] \n",
            "Epoch 6: 100%|██████████| 452/452 [00:04<00:00, 112.13batch/s, mse=0.259]\n",
            "Epoch 7: 100%|██████████| 452/452 [00:03<00:00, 129.28batch/s, mse=0.258]\n",
            "Epoch 8: 100%|██████████| 452/452 [00:03<00:00, 130.55batch/s, mse=0.254]\n",
            "Epoch 9: 100%|██████████| 452/452 [00:03<00:00, 135.04batch/s, mse=0.253]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.1092199087142944,\n",
              " 1.1050695180892944,\n",
              " 1.10002601146698,\n",
              " 1.0947558879852295,\n",
              " 1.0895349979400635,\n",
              " 1.0855776071548462,\n",
              " 1.0809153318405151,\n",
              " 1.0760327577590942,\n",
              " 1.0715341567993164,\n",
              " 1.0670299530029297,\n",
              " 1.0627214908599854,\n",
              " 1.0580672025680542,\n",
              " 1.0532639026641846,\n",
              " 1.0480509996414185,\n",
              " 1.0426621437072754,\n",
              " 1.0373111963272095,\n",
              " 1.0321440696716309,\n",
              " 1.0270891189575195,\n",
              " 1.0223286151885986,\n",
              " 1.0176749229431152,\n",
              " 1.012696623802185,\n",
              " 1.0077403783798218,\n",
              " 1.0022815465927124,\n",
              " 0.9960994124412537,\n",
              " 0.9892641305923462,\n",
              " 0.9824613928794861,\n",
              " 0.9758366346359253,\n",
              " 0.9673351645469666,\n",
              " 0.9580157995223999,\n",
              " 0.9476733207702637,\n",
              " 0.9356501698493958,\n",
              " 0.9226036667823792,\n",
              " 0.9082967042922974,\n",
              " 0.8928813338279724,\n",
              " 0.8757085800170898,\n",
              " 0.8568451404571533,\n",
              " 0.8361746668815613,\n",
              " 0.8151617050170898,\n",
              " 0.7917960286140442,\n",
              " 0.7683489322662354,\n",
              " 0.7429015040397644,\n",
              " 0.7178897857666016,\n",
              " 0.6956524848937988,\n",
              " 0.6771388053894043,\n",
              " 0.6619371771812439,\n",
              " 0.6535409092903137,\n",
              " 0.6546143293380737,\n",
              " 0.6685996055603027,\n",
              " 0.6942729949951172,\n",
              " 0.7329741716384888,\n",
              " 0.7840264439582825,\n",
              " 0.8480612635612488,\n",
              " 0.9268636703491211,\n",
              " 1.0145878791809082,\n",
              " 1.1157634258270264,\n",
              " 1.1910853385925293,\n",
              " 1.2411937713623047,\n",
              " 1.2670096158981323,\n",
              " 1.2693885564804077,\n",
              " 1.277777910232544,\n",
              " 1.2880589962005615,\n",
              " 1.307167887687683,\n",
              " 1.3140417337417603,\n",
              " 1.3061331510543823,\n",
              " 1.2913328409194946,\n",
              " 1.266692042350769,\n",
              " 1.2426549196243286,\n",
              " 1.2102935314178467,\n",
              " 1.106682300567627,\n",
              " 1.0481241941452026,\n",
              " 1.0138561725616455,\n",
              " 0.9950191974639893,\n",
              " 0.9914329648017883,\n",
              " 0.9953277111053467,\n",
              " 1.0132824182510376,\n",
              " 1.0469212532043457,\n",
              " 1.090907096862793,\n",
              " 1.1488620042800903,\n",
              " 1.2189372777938843,\n",
              " 1.3032821416854858,\n",
              " 1.3991680145263672,\n",
              " 1.485364317893982,\n",
              " 1.5515758991241455,\n",
              " 1.6145955324172974,\n",
              " 1.666223168373108,\n",
              " 1.6960443258285522,\n",
              " 1.730730652809143,\n",
              " 1.7766937017440796,\n",
              " 1.8328936100006104,\n",
              " 1.9049711227416992,\n",
              " 1.970749855041504,\n",
              " 1.9988069534301758,\n",
              " 2.010732650756836,\n",
              " 1.9753602743148804,\n",
              " 1.936816692352295,\n",
              " 1.929720163345337,\n",
              " 1.9271790981292725,\n",
              " 1.9376572370529175,\n",
              " 1.9611318111419678,\n",
              " 1.9861844778060913,\n",
              " 2.0373644828796387,\n",
              " 2.0986101627349854,\n",
              " 2.1761927604675293,\n",
              " 2.1980154514312744,\n",
              " 2.191279888153076,\n",
              " 2.16648006439209,\n",
              " 2.136384963989258,\n",
              " 2.0892293453216553,\n",
              " 2.0430002212524414,\n",
              " 1.997633695602417,\n",
              " 1.9390320777893066,\n",
              " 1.8709378242492676,\n",
              " 1.827451229095459,\n",
              " 1.8131178617477417,\n",
              " 1.8160892724990845,\n",
              " 1.8351892232894897,\n",
              " 1.8481358289718628,\n",
              " 1.8774465322494507,\n",
              " 1.9146943092346191,\n",
              " 1.9645870923995972,\n",
              " 1.993295431137085,\n",
              " 2.0172226428985596,\n",
              " 2.0417144298553467,\n",
              " 1.9639378786087036,\n",
              " 1.8911242485046387,\n",
              " 1.8101718425750732,\n",
              " 1.7377159595489502,\n",
              " 1.4689712524414062,\n",
              " 1.2719165086746216,\n",
              " 1.1299681663513184,\n",
              " 1.0257591009140015,\n",
              " 0.944676399230957,\n",
              " 0.8851523399353027,\n",
              " 0.8369781374931335,\n",
              " 0.8037644028663635,\n",
              " 0.7760924696922302,\n",
              " 0.763495922088623,\n",
              " 0.7557873129844666,\n",
              " 0.7476924657821655,\n",
              " 0.7302731871604919,\n",
              " 0.7122591733932495,\n",
              " 0.6887111067771912,\n",
              " 0.6669579744338989,\n",
              " 0.6379449367523193,\n",
              " 0.6125003099441528,\n",
              " 0.5886400938034058,\n",
              " 0.573322057723999,\n",
              " 0.5593777894973755,\n",
              " 0.553281843662262,\n",
              " 0.5466198325157166,\n",
              " 0.5364677309989929,\n",
              " 0.5323282480239868,\n",
              " 0.5298101305961609,\n",
              " 0.5291231274604797,\n",
              " 0.5329977869987488,\n",
              " 0.5384070873260498,\n",
              " 0.5473399758338928,\n",
              " 0.5561855435371399,\n",
              " 0.5663000345230103,\n",
              " 0.5769816637039185,\n",
              " 0.5937357544898987,\n",
              " 0.6048928499221802,\n",
              " 0.6162917613983154,\n",
              " 0.6217961311340332,\n",
              " 0.6265995502471924,\n",
              " 0.6350338459014893,\n",
              " 0.6439458131790161,\n",
              " 0.6520520448684692,\n",
              " 0.6570611596107483,\n",
              " 0.6664869785308838,\n",
              " 0.6680496335029602,\n",
              " 0.6698837280273438,\n",
              " 0.6771733164787292,\n",
              " 0.6776422262191772,\n",
              " 0.6810958385467529,\n",
              " 0.6817695498466492,\n",
              " 0.6793943643569946,\n",
              " 0.6854614019393921,\n",
              " 0.6979721188545227,\n",
              " 0.7126429080963135,\n",
              " 0.7321223616600037,\n",
              " 0.7466446161270142,\n",
              " 0.7599298357963562,\n",
              " 0.7716453075408936,\n",
              " 0.7547751069068909,\n",
              " 0.7425334453582764,\n",
              " 0.7360643148422241,\n",
              " 0.7066379189491272,\n",
              " 0.6910310983657837,\n",
              " 0.6782146096229553,\n",
              " 0.6731700897216797,\n",
              " 0.6754609942436218,\n",
              " 0.6792433857917786,\n",
              " 0.6818771362304688,\n",
              " 0.6906718015670776,\n",
              " 0.7004095315933228,\n",
              " 0.7165448665618896,\n",
              " 0.6491662263870239,\n",
              " 0.5861176252365112,\n",
              " 0.530561089515686,\n",
              " 0.4870818853378296,\n",
              " 0.45100104808807373,\n",
              " 0.42457249760627747,\n",
              " 0.40784958004951477,\n",
              " 0.3965630531311035,\n",
              " 0.39049026370048523,\n",
              " 0.3844093978404999,\n",
              " 0.3802644610404968,\n",
              " 0.3763701021671295,\n",
              " 0.36732596158981323,\n",
              " 0.355874240398407,\n",
              " 0.3457847237586975,\n",
              " 0.33813509345054626,\n",
              " 0.33350449800491333,\n",
              " 0.3308756649494171,\n",
              " 0.32921651005744934,\n",
              " 0.32922375202178955,\n",
              " 0.33031144738197327,\n",
              " 0.3321574926376343,\n",
              " 0.3319949805736542,\n",
              " 0.33098357915878296,\n",
              " 0.3305704593658447,\n",
              " 0.3308829367160797,\n",
              " 0.33199188113212585,\n",
              " 0.3335922956466675,\n",
              " 0.3352603018283844,\n",
              " 0.3366314172744751,\n",
              " 0.34026631712913513,\n",
              " 0.3407875597476959,\n",
              " 0.3379323482513428,\n",
              " 0.3370901942253113,\n",
              " 0.33447813987731934,\n",
              " 0.33370456099510193,\n",
              " 0.33394908905029297,\n",
              " 0.3350127637386322,\n",
              " 0.3360117971897125,\n",
              " 0.3367617428302765,\n",
              " 0.33793243765830994,\n",
              " 0.33921974897384644,\n",
              " 0.341013640165329,\n",
              " 0.3412299156188965,\n",
              " 0.3436596989631653,\n",
              " 0.3181266784667969,\n",
              " 0.3155418634414673,\n",
              " 0.3244233727455139,\n",
              " 0.3368373513221741,\n",
              " 0.3592832386493683,\n",
              " 0.37389692664146423,\n",
              " 0.38527601957321167,\n",
              " 0.38973942399024963,\n",
              " 0.38736218214035034,\n",
              " 0.3856777846813202,\n",
              " 0.3851059079170227,\n",
              " 0.3838482201099396,\n",
              " 0.3838879466056824,\n",
              " 0.384954035282135,\n",
              " 0.38684266805648804,\n",
              " 0.3903033137321472,\n",
              " 0.395734578371048,\n",
              " 0.40101170539855957,\n",
              " 0.40439191460609436,\n",
              " 0.4042325019836426,\n",
              " 0.40370675921440125,\n",
              " 0.4040437936782837,\n",
              " 0.40325286984443665,\n",
              " 0.39917948842048645,\n",
              " 0.3965313136577606,\n",
              " 0.39479368925094604,\n",
              " 0.3936600089073181,\n",
              " 0.39490923285484314,\n",
              " 0.39759382605552673,\n",
              " 0.4001770317554474,\n",
              " 0.40055400133132935,\n",
              " 0.39733466506004333,\n",
              " 0.3919041156768799,\n",
              " 0.3901876211166382,\n",
              " 0.3917841911315918,\n",
              " 0.39370104670524597,\n",
              " 0.3955097794532776,\n",
              " 0.3961998224258423,\n",
              " 0.39617782831192017,\n",
              " 0.3950824439525604,\n",
              " 0.39495453238487244,\n",
              " 0.3946666717529297,\n",
              " 0.3947088420391083,\n",
              " 0.3946903645992279,\n",
              " 0.3937494456768036,\n",
              " 0.3934304416179657,\n",
              " 0.3931458294391632,\n",
              " 0.3921823799610138,\n",
              " 0.39133694767951965,\n",
              " 0.39024817943573,\n",
              " 0.39032506942749023,\n",
              " 0.391370952129364,\n",
              " 0.39311835169792175,\n",
              " 0.3926261365413666,\n",
              " 0.3898254930973053,\n",
              " 0.387079656124115,\n",
              " 0.38497522473335266,\n",
              " 0.382508248090744,\n",
              " 0.3799667954444885,\n",
              " 0.37920647859573364,\n",
              " 0.38050320744514465,\n",
              " 0.3840165138244629,\n",
              " 0.38631388545036316,\n",
              " 0.38612985610961914,\n",
              " 0.3868260979652405,\n",
              " 0.38546624779701233,\n",
              " 0.38391339778900146,\n",
              " 0.3825608789920807,\n",
              " 0.38173434138298035,\n",
              " 0.3832232356071472,\n",
              " 0.38692986965179443,\n",
              " 0.39286476373672485,\n",
              " 0.39738932251930237,\n",
              " 0.40022096037864685,\n",
              " 0.40214085578918457,\n",
              " 0.4040869176387787,\n",
              " 0.3998638689517975,\n",
              " 0.39018675684928894,\n",
              " 0.3808870017528534,\n",
              " 0.376274973154068,\n",
              " 0.37652716040611267,\n",
              " 0.3775378465652466,\n",
              " 0.380610853433609,\n",
              " 0.3832848072052002,\n",
              " 0.379830926656723,\n",
              " 0.3780500292778015,\n",
              " 0.37639331817626953,\n",
              " 0.37611714005470276,\n",
              " 0.37533143162727356,\n",
              " 0.37393173575401306,\n",
              " 0.37195393443107605,\n",
              " 0.3708173334598541,\n",
              " 0.3699207603931427,\n",
              " 0.36906951665878296,\n",
              " 0.36803632974624634,\n",
              " 0.367730975151062,\n",
              " 0.36807867884635925,\n",
              " 0.36874446272850037,\n",
              " 0.36944448947906494,\n",
              " 0.36979925632476807,\n",
              " 0.3701888918876648,\n",
              " 0.37046149373054504,\n",
              " 0.37019863724708557,\n",
              " 0.36947453022003174,\n",
              " 0.3692478537559509,\n",
              " 0.3687741458415985,\n",
              " 0.3682674765586853,\n",
              " 0.3676545023918152,\n",
              " 0.3673141598701477,\n",
              " 0.36664366722106934,\n",
              " 0.3668932020664215,\n",
              " 0.3669297695159912,\n",
              " 0.36646804213523865,\n",
              " 0.3670579195022583,\n",
              " 0.36946234107017517,\n",
              " 0.37157008051872253,\n",
              " 0.3734859526157379,\n",
              " 0.3775503933429718,\n",
              " 0.38147807121276855,\n",
              " 0.3858107328414917,\n",
              " 0.3851047456264496,\n",
              " 0.38021785020828247,\n",
              " 0.3724207878112793,\n",
              " 0.37134766578674316,\n",
              " 0.3734114468097687,\n",
              " 0.3752783536911011,\n",
              " 0.37566453218460083,\n",
              " 0.3766469657421112,\n",
              " 0.3749617338180542,\n",
              " 0.3735024631023407,\n",
              " 0.3703919053077698,\n",
              " 0.3670872151851654,\n",
              " 0.36479055881500244,\n",
              " 0.3647948205471039,\n",
              " 0.36634165048599243,\n",
              " 0.3691014349460602,\n",
              " 0.37119874358177185,\n",
              " 0.37181422114372253,\n",
              " 0.37085217237472534,\n",
              " 0.3691681921482086,\n",
              " 0.36756566166877747,\n",
              " 0.36718153953552246,\n",
              " 0.36734601855278015,\n",
              " 0.36815932393074036,\n",
              " 0.3680809736251831,\n",
              " 0.36712005734443665,\n",
              " 0.3667362332344055,\n",
              " 0.3658098876476288,\n",
              " 0.3681154251098633,\n",
              " 0.37239211797714233,\n",
              " 0.3760931193828583,\n",
              " 0.3780286908149719,\n",
              " 0.3781532645225525,\n",
              " 0.37567615509033203,\n",
              " 0.37190666794776917,\n",
              " 0.36908161640167236,\n",
              " 0.3662182688713074,\n",
              " 0.3640722930431366,\n",
              " 0.3627175986766815,\n",
              " 0.36111292243003845,\n",
              " 0.3595230281352997,\n",
              " 0.3581552505493164,\n",
              " 0.3577481806278229,\n",
              " 0.3580300807952881,\n",
              " 0.3587375581264496,\n",
              " 0.3605458736419678,\n",
              " 0.36208266019821167,\n",
              " 0.36408746242523193,\n",
              " 0.36504796147346497,\n",
              " 0.36500978469848633,\n",
              " 0.3641204535961151,\n",
              " 0.3624143600463867,\n",
              " 0.36084938049316406,\n",
              " 0.3600146174430847,\n",
              " 0.35973942279815674,\n",
              " 0.36019274592399597,\n",
              " 0.357120543718338,\n",
              " 0.3557673692703247,\n",
              " 0.3555760085582733,\n",
              " 0.3552713990211487,\n",
              " 0.3560621440410614,\n",
              " 0.35673490166664124,\n",
              " 0.3580210208892822,\n",
              " 0.360161691904068,\n",
              " 0.361762672662735,\n",
              " 0.36315080523490906,\n",
              " 0.3636845052242279,\n",
              " 0.3633808195590973,\n",
              " 0.36175692081451416,\n",
              " 0.3595726191997528,\n",
              " 0.35702967643737793,\n",
              " 0.3551153838634491,\n",
              " 0.3539336919784546,\n",
              " 0.35337215662002563,\n",
              " 0.3549979031085968,\n",
              " 0.35660895705223083,\n",
              " 0.35841885209083557,\n",
              " 0.3592982590198517,\n",
              " 0.35974836349487305,\n",
              " 0.35607442259788513,\n",
              " 0.3520354926586151,\n",
              " 0.3497065603733063,\n",
              " 0.3479551374912262,\n",
              " 0.34719961881637573,\n",
              " 0.34685853123664856,\n",
              " 0.3476335406303406,\n",
              " 0.3468533754348755,\n",
              " 0.3472138047218323,\n",
              " 0.34872323274612427,\n",
              " 0.35093313455581665,\n",
              " 0.3530682623386383,\n",
              " 0.35476332902908325,\n",
              " 0.3563656806945801,\n",
              " 0.3580370247364044,\n",
              " 0.35975363850593567,\n",
              " 0.35902154445648193,\n",
              " 0.3575473427772522,\n",
              " 0.3553362488746643,\n",
              " 0.3543263375759125,\n",
              " 0.3536176383495331,\n",
              " 0.35349494218826294,\n",
              " 0.3539404273033142,\n",
              " 0.35523107647895813,\n",
              " 0.3561984598636627,\n",
              " 0.35710564255714417,\n",
              " 0.35802701115608215,\n",
              " 0.359428346157074,\n",
              " 0.35977110266685486,\n",
              " 0.36112913489341736,\n",
              " 0.3623557388782501,\n",
              " 0.36348024010658264,\n",
              " 0.36560288071632385,\n",
              " 0.36990201473236084,\n",
              " 0.3763425648212433,\n",
              " 0.38149455189704895,\n",
              " 0.38170796632766724,\n",
              " 0.38797610998153687,\n",
              " 0.3861549496650696,\n",
              " 0.3792523741722107,\n",
              " 0.3707737326622009,\n",
              " 0.36913448572158813,\n",
              " 0.3693917691707611,\n",
              " 0.37296929955482483,\n",
              " 0.37550270557403564,\n",
              " 0.3768174648284912,\n",
              " 0.3789379894733429,\n",
              " 0.3794628977775574,\n",
              " 0.3795103132724762,\n",
              " 0.37420564889907837,\n",
              " 0.3679933249950409,\n",
              " 0.36027371883392334,\n",
              " 0.3538644015789032,\n",
              " 0.34826409816741943,\n",
              " 0.34515491127967834,\n",
              " 0.3433232307434082,\n",
              " 0.34221723675727844,\n",
              " 0.34033486247062683,\n",
              " 0.33900338411331177,\n",
              " 0.338977187871933,\n",
              " 0.33964061737060547,\n",
              " 0.3410728871822357,\n",
              " 0.3421146273612976,\n",
              " 0.34378886222839355,\n",
              " 0.3458038866519928,\n",
              " 0.34816932678222656,\n",
              " 0.35161587595939636,\n",
              " 0.35438773036003113,\n",
              " 0.35605841875076294,\n",
              " 0.35528191924095154,\n",
              " 0.3503967523574829,\n",
              " 0.3465881943702698,\n",
              " 0.3433184325695038,\n",
              " 0.3401651084423065,\n",
              " 0.3381368815898895,\n",
              " 0.3352661728858948,\n",
              " 0.33221691846847534,\n",
              " 0.32985639572143555,\n",
              " 0.3290238678455353,\n",
              " 0.33676350116729736,\n",
              " 0.3467039465904236,\n",
              " 0.357314795255661,\n",
              " 0.365547239780426,\n",
              " 0.3716547191143036,\n",
              " 0.3744896948337555,\n",
              " 0.3720713257789612,\n",
              " 0.366878867149353,\n",
              " 0.36189737915992737,\n",
              " 0.3577096164226532,\n",
              " 0.35354945063591003,\n",
              " 0.3493267297744751,\n",
              " 0.34899014234542847,\n",
              " 0.35021665692329407,\n",
              " 0.3511042892932892,\n",
              " 0.3524371087551117,\n",
              " 0.35330334305763245,\n",
              " 0.35235804319381714,\n",
              " 0.3509509563446045,\n",
              " 0.35017862915992737,\n",
              " 0.3490425944328308,\n",
              " 0.3491601347923279,\n",
              " 0.3501901924610138,\n",
              " 0.34968286752700806,\n",
              " 0.34959715604782104,\n",
              " 0.3492274582386017,\n",
              " 0.35076281428337097,\n",
              " 0.35352128744125366,\n",
              " 0.3561515808105469,\n",
              " 0.3582599461078644,\n",
              " 0.3594714403152466,\n",
              " 0.3575674295425415,\n",
              " 0.3533753752708435,\n",
              " 0.34960299730300903,\n",
              " 0.3457074761390686,\n",
              " 0.34378257393836975,\n",
              " 0.34215474128723145,\n",
              " 0.3410435616970062,\n",
              " 0.3402748703956604,\n",
              " 0.34064581990242004,\n",
              " 0.342292845249176,\n",
              " 0.3456200361251831,\n",
              " 0.34954795241355896,\n",
              " 0.35519957542419434,\n",
              " 0.35856950283050537,\n",
              " 0.35950833559036255,\n",
              " 0.35881662368774414,\n",
              " 0.3568311035633087,\n",
              " 0.3547433316707611,\n",
              " 0.35233935713768005,\n",
              " 0.34925225377082825,\n",
              " 0.3458973169326782,\n",
              " 0.3430899977684021,\n",
              " 0.3408212661743164,\n",
              " 0.34100914001464844,\n",
              " 0.3400685787200928,\n",
              " 0.3388338088989258,\n",
              " 0.3379473090171814,\n",
              " 0.33806493878364563,\n",
              " 0.33761245012283325,\n",
              " 0.3373166024684906,\n",
              " 0.3380446135997772,\n",
              " 0.3407351076602936,\n",
              " 0.34131619334220886,\n",
              " 0.3428899943828583,\n",
              " 0.33888500928878784,\n",
              " 0.3336220383644104,\n",
              " 0.3304678499698639,\n",
              " 0.3261719048023224,\n",
              " 0.32275667786598206,\n",
              " 0.32026922702789307,\n",
              " 0.31649789214134216,\n",
              " 0.3121366500854492,\n",
              " 0.30704382061958313,\n",
              " 0.3043401837348938,\n",
              " 0.3033943176269531,\n",
              " 0.30276939272880554,\n",
              " 0.30321773886680603,\n",
              " 0.303169846534729,\n",
              " 0.30418428778648376,\n",
              " 0.3055253028869629,\n",
              " 0.3084876537322998,\n",
              " 0.31042352318763733,\n",
              " 0.3113243579864502,\n",
              " 0.3112505376338959,\n",
              " 0.31063514947891235,\n",
              " 0.30828362703323364,\n",
              " 0.30552393198013306,\n",
              " 0.3021482229232788,\n",
              " 0.2994224429130554,\n",
              " 0.29722610116004944,\n",
              " 0.2953973710536957,\n",
              " 0.294040322303772,\n",
              " 0.2927008271217346,\n",
              " 0.29171222448349,\n",
              " 0.2906213402748108,\n",
              " 0.2899591326713562,\n",
              " 0.28956139087677,\n",
              " 0.28917789459228516,\n",
              " 0.28870123624801636,\n",
              " 0.2884526550769806,\n",
              " 0.2880662679672241,\n",
              " 0.28742414712905884,\n",
              " 0.2874278128147125,\n",
              " 0.2867979109287262,\n",
              " 0.28697633743286133,\n",
              " 0.2850947082042694,\n",
              " 0.2839014232158661,\n",
              " 0.2832573652267456,\n",
              " 0.2841866910457611,\n",
              " 0.28659412264823914,\n",
              " 0.2900594174861908,\n",
              " 0.293439120054245,\n",
              " 0.2950311005115509,\n",
              " 0.29584404826164246,\n",
              " 0.29463839530944824,\n",
              " 0.28784361481666565,\n",
              " 0.2833364009857178,\n",
              " 0.2815982401371002,\n",
              " 0.28334012627601624,\n",
              " 0.2882830500602722,\n",
              " 0.29424819350242615,\n",
              " 0.30138078331947327,\n",
              " 0.30804625153541565,\n",
              " 0.31286856532096863,\n",
              " 0.30975788831710815,\n",
              " 0.30358847975730896,\n",
              " 0.2975716292858124,\n",
              " 0.2938995659351349,\n",
              " 0.2913760840892792,\n",
              " 0.29057061672210693,\n",
              " 0.29005131125450134,\n",
              " 0.28958365321159363,\n",
              " 0.28890112042427063,\n",
              " 0.2887497544288635,\n",
              " 0.2906961143016815,\n",
              " 0.2921847701072693,\n",
              " 0.2941375970840454,\n",
              " 0.294819712638855,\n",
              " 0.2943298816680908,\n",
              " 0.2929145395755768,\n",
              " 0.28899291157722473,\n",
              " 0.28585779666900635,\n",
              " 0.28487661480903625,\n",
              " 0.2854826748371124,\n",
              " 0.2867230474948883,\n",
              " 0.2873612940311432,\n",
              " 0.2880118787288666,\n",
              " 0.288316011428833,\n",
              " 0.28888577222824097,\n",
              " 0.28904929757118225,\n",
              " 0.2864980697631836,\n",
              " 0.28435710072517395,\n",
              " 0.28336575627326965,\n",
              " 0.2832459807395935,\n",
              " 0.2839849591255188,\n",
              " 0.28511759638786316,\n",
              " 0.28620222210884094,\n",
              " 0.28588545322418213,\n",
              " 0.28638264536857605,\n",
              " 0.28532710671424866,\n",
              " 0.2835971415042877,\n",
              " 0.28296777606010437,\n",
              " 0.283062219619751,\n",
              " 0.2847914695739746,\n",
              " 0.2869672477245331,\n",
              " 0.2876938581466675,\n",
              " 0.28601303696632385,\n",
              " 0.28461402654647827,\n",
              " 0.28299811482429504,\n",
              " 0.2820127308368683,\n",
              " 0.2817760705947876,\n",
              " 0.2797108590602875,\n",
              " 0.278856098651886,\n",
              " 0.27906227111816406,\n",
              " 0.28005242347717285,\n",
              " 0.2808665931224823,\n",
              " 0.28106990456581116,\n",
              " 0.28232863545417786,\n",
              " 0.28364333510398865,\n",
              " 0.286070317029953,\n",
              " 0.2875010371208191,\n",
              " 0.28816255927085876,\n",
              " 0.2892611026763916,\n",
              " 0.29016488790512085,\n",
              " 0.29117831587791443,\n",
              " 0.2912808358669281,\n",
              " 0.2911429703235626,\n",
              " 0.28985145688056946,\n",
              " 0.2876944839954376,\n",
              " 0.2866838872432709,\n",
              " 0.28653275966644287,\n",
              " 0.2869012653827667,\n",
              " 0.2867432236671448,\n",
              " 0.2864241898059845,\n",
              " 0.2864631414413452,\n",
              " 0.28572747111320496,\n",
              " 0.2837677001953125,\n",
              " 0.2820664644241333,\n",
              " 0.28049880266189575,\n",
              " 0.2797093987464905,\n",
              " 0.28077149391174316,\n",
              " 0.28210756182670593,\n",
              " 0.2822364866733551,\n",
              " 0.28139209747314453,\n",
              " 0.2800846993923187,\n",
              " 0.27880653738975525,\n",
              " 0.28116080164909363,\n",
              " 0.284656286239624,\n",
              " 0.2870575785636902,\n",
              " 0.2882428467273712,\n",
              " 0.2881355583667755,\n",
              " 0.2872655987739563,\n",
              " 0.2854292690753937,\n",
              " 0.28455251455307007,\n",
              " 0.2839496433734894,\n",
              " 0.28343960642814636,\n",
              " 0.28358688950538635,\n",
              " 0.2842108905315399,\n",
              " 0.28534501791000366,\n",
              " 0.2858690619468689,\n",
              " 0.28580906987190247,\n",
              " 0.28568267822265625,\n",
              " 0.2852292060852051,\n",
              " 0.28570517897605896,\n",
              " 0.2881369888782501,\n",
              " 0.29162895679473877,\n",
              " 0.29288020730018616,\n",
              " 0.29114875197410583,\n",
              " 0.28831833600997925,\n",
              " 0.28622761368751526,\n",
              " 0.2841833233833313,\n",
              " 0.2811868488788605,\n",
              " 0.2794314920902252,\n",
              " 0.2790926992893219,\n",
              " 0.28077906370162964,\n",
              " 0.2822255790233612,\n",
              " 0.2823650538921356,\n",
              " 0.2830694019794464,\n",
              " 0.2828831076622009,\n",
              " 0.28265777230262756,\n",
              " 0.28181788325309753,\n",
              " 0.2818976044654846,\n",
              " 0.2846424877643585,\n",
              " 0.28998255729675293,\n",
              " 0.2973833978176117,\n",
              " 0.3021463453769684,\n",
              " 0.3059617280960083,\n",
              " 0.3096069395542145,\n",
              " 0.3131769299507141,\n",
              " 0.31089192628860474,\n",
              " 0.30251169204711914,\n",
              " 0.2921493351459503,\n",
              " 0.2856075167655945,\n",
              " 0.28498128056526184,\n",
              " 0.2864755690097809,\n",
              " 0.2905183434486389,\n",
              " 0.2941123843193054,\n",
              " 0.2927141487598419,\n",
              " 0.2921392023563385,\n",
              " 0.2913058400154114,\n",
              " 0.29138657450675964,\n",
              " 0.29087191820144653,\n",
              " 0.28984153270721436,\n",
              " 0.2885025441646576,\n",
              " 0.2878025770187378,\n",
              " 0.28751426935195923,\n",
              " 0.287463903427124,\n",
              " 0.2863718271255493,\n",
              " 0.28564131259918213,\n",
              " 0.28563007712364197,\n",
              " 0.28630882501602173,\n",
              " 0.28658825159072876,\n",
              " 0.28696149587631226,\n",
              " 0.28748518228530884,\n",
              " 0.2881162464618683,\n",
              " 0.2880931496620178,\n",
              " 0.28779780864715576,\n",
              " 0.287673681974411,\n",
              " 0.28713512420654297,\n",
              " 0.2868233323097229,\n",
              " 0.2862144112586975,\n",
              " 0.285360187292099,\n",
              " 0.2839137017726898,\n",
              " 0.2832297682762146,\n",
              " 0.2822396755218506,\n",
              " 0.2810174524784088,\n",
              " 0.2806110084056854,\n",
              " 0.281965047121048,\n",
              " 0.2833251357078552,\n",
              " 0.28375381231307983,\n",
              " 0.2854054868221283,\n",
              " 0.2862737774848938,\n",
              " 0.28770872950553894,\n",
              " 0.28587356209754944,\n",
              " 0.28270408511161804,\n",
              " 0.2793661653995514,\n",
              " 0.2801700234413147,\n",
              " 0.2823616564273834,\n",
              " 0.28477743268013,\n",
              " 0.2862835228443146,\n",
              " 0.28820908069610596,\n",
              " 0.2883901298046112,\n",
              " 0.2891625165939331,\n",
              " 0.2880980372428894,\n",
              " 0.28570714592933655,\n",
              " 0.2835676074028015,\n",
              " 0.2829324007034302,\n",
              " 0.2830110490322113,\n",
              " 0.2835186719894409,\n",
              " 0.28392350673675537,\n",
              " 0.2840374708175659,\n",
              " 0.283804714679718,\n",
              " 0.2831658124923706,\n",
              " 0.2827361226081848,\n",
              " 0.2829311192035675,\n",
              " 0.2828822433948517,\n",
              " 0.2826356887817383,\n",
              " 0.28171899914741516,\n",
              " 0.2808142602443695,\n",
              " 0.2807197868824005,\n",
              " 0.28105390071868896,\n",
              " 0.2852144241333008,\n",
              " 0.29031938314437866,\n",
              " 0.29329919815063477,\n",
              " 0.2939675450325012,\n",
              " 0.291931688785553,\n",
              " 0.2883390486240387,\n",
              " 0.2842705249786377,\n",
              " 0.282188355922699,\n",
              " 0.2818997800350189,\n",
              " 0.2819788455963135,\n",
              " 0.28250885009765625,\n",
              " 0.28331440687179565,\n",
              " 0.2824530601501465,\n",
              " 0.279914915561676,\n",
              " 0.2794531583786011,\n",
              " 0.28030842542648315,\n",
              " 0.2811069190502167,\n",
              " 0.2835536599159241,\n",
              " 0.2857164442539215,\n",
              " 0.2885516583919525,\n",
              " 0.2900983393192291,\n",
              " 0.29024046659469604,\n",
              " 0.28866255283355713,\n",
              " 0.28632667660713196,\n",
              " 0.2838955819606781,\n",
              " 0.2818663418292999,\n",
              " 0.2806240916252136,\n",
              " 0.28052929043769836,\n",
              " 0.27904629707336426,\n",
              " 0.27908194065093994,\n",
              " 0.2800406813621521,\n",
              " 0.28057634830474854,\n",
              " 0.28198930621147156,\n",
              " 0.2827453315258026,\n",
              " 0.2834227383136749,\n",
              " 0.284041166305542,\n",
              " 0.28473958373069763,\n",
              " 0.28535839915275574,\n",
              " 0.2849450409412384,\n",
              " 0.28423815965652466,\n",
              " 0.2829993963241577,\n",
              " 0.2814023792743683,\n",
              " 0.2797205150127411,\n",
              " 0.2786330282688141,\n",
              " 0.27783864736557007,\n",
              " 0.27767106890678406,\n",
              " 0.27901607751846313,\n",
              " 0.2804226875305176,\n",
              " 0.2824133038520813,\n",
              " 0.28351905941963196,\n",
              " 0.2848477363586426,\n",
              " 0.28312844038009644,\n",
              " 0.28000789880752563,\n",
              " 0.2778325080871582,\n",
              " 0.2764160633087158,\n",
              " 0.27520886063575745,\n",
              " 0.2747148871421814,\n",
              " 0.27486732602119446,\n",
              " 0.27472734451293945,\n",
              " 0.2764997184276581,\n",
              " 0.28012776374816895,\n",
              " 0.2840791344642639,\n",
              " 0.28885698318481445,\n",
              " 0.29174262285232544,\n",
              " 0.29366615414619446,\n",
              " 0.2948260009288788,\n",
              " 0.29510968923568726,\n",
              " 0.29333990812301636,\n",
              " 0.2907421588897705,\n",
              " 0.28719979524612427,\n",
              " 0.28492215275764465,\n",
              " 0.28324341773986816,\n",
              " 0.2824006974697113,\n",
              " 0.2817882001399994,\n",
              " 0.28145739436149597,\n",
              " 0.2810538709163666,\n",
              " 0.28090712428092957,\n",
              " 0.28128671646118164,\n",
              " 0.28210124373435974,\n",
              " 0.28195884823799133,\n",
              " 0.2828403115272522,\n",
              " 0.2828832268714905,\n",
              " 0.2826792001724243,\n",
              " 0.28303778171539307,\n",
              " 0.2860848307609558,\n",
              " 0.29143616557121277,\n",
              " 0.2967210114002228,\n",
              " 0.2991005778312683,\n",
              " 0.30728691816329956,\n",
              " 0.3078291118144989,\n",
              " 0.3020622432231903,\n",
              " 0.29416242241859436,\n",
              " 0.29188451170921326,\n",
              " 0.2904335558414459,\n",
              " 0.2919940650463104,\n",
              " 0.29412657022476196,\n",
              " 0.2960039973258972,\n",
              " 0.2997601330280304,\n",
              " 0.30236732959747314,\n",
              " 0.3046674430370331,\n",
              " 0.3014533817768097,\n",
              " 0.29705101251602173,\n",
              " 0.2906825840473175,\n",
              " 0.2849752604961395,\n",
              " 0.2801319360733032,\n",
              " 0.27783408761024475,\n",
              " 0.2762015461921692,\n",
              " 0.27591055631637573,\n",
              " 0.2749687433242798,\n",
              " 0.27474290132522583,\n",
              " 0.2754201591014862,\n",
              " 0.27676284313201904,\n",
              " 0.27902451157569885,\n",
              " 0.2818446755409241,\n",
              " 0.2855782210826874,\n",
              " 0.2894637882709503,\n",
              " 0.2929377555847168,\n",
              " 0.29748281836509705,\n",
              " 0.3003818392753601,\n",
              " 0.30149146914482117,\n",
              " 0.299852579832077,\n",
              " 0.294471800327301,\n",
              " 0.29114335775375366,\n",
              " 0.289975106716156,\n",
              " 0.2900432050228119,\n",
              " 0.29007282853126526,\n",
              " 0.28960782289505005,\n",
              " 0.28514760732650757,\n",
              " 0.28046590089797974,\n",
              " 0.2776508331298828,\n",
              " 0.28122127056121826,\n",
              " 0.28890302777290344,\n",
              " 0.29922646284103394,\n",
              " 0.3073137700557709,\n",
              " 0.3135298490524292,\n",
              " 0.31374698877334595,\n",
              " 0.30907997488975525,\n",
              " 0.30255886912345886,\n",
              " 0.2972327470779419,\n",
              " 0.29237356781959534,\n",
              " 0.2869225740432739,\n",
              " 0.28136345744132996,\n",
              " 0.28032487630844116,\n",
              " 0.2810426950454712,\n",
              " 0.2815403938293457,\n",
              " 0.28271928429603577,\n",
              " 0.28333336114883423,\n",
              " 0.2818275988101959,\n",
              " 0.2802398204803467,\n",
              " 0.27968746423721313,\n",
              " 0.27928757667541504,\n",
              " 0.2799944579601288,\n",
              " 0.2815607488155365,\n",
              " 0.28015848994255066,\n",
              " 0.2793462574481964,\n",
              " 0.27708297967910767,\n",
              " 0.2762417793273926,\n",
              " 0.2772098481655121,\n",
              " ...]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_model(model, n_epochs, batch_size, X_train, y_train, X_test, y_test, loss_fn_train, loss_fn_test, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bausm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ],
      "source": [
        "reinit_model(model)\n",
        "batch_size = 32\n",
        "n_epochs = 10\n",
        "epsilon = 1e-6\n",
        "loss_fn_test = nn.MSELoss()\n",
        "loss_fn_train = epsilonMSELoss(epsilon)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/452 [00:00<?, ?batch/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'backward'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[15], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, n_epochs, batch_size, X_train, y_train, X_test, y_test, loss_fn_train, loss_fn_test, optimizer)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'backward'"
          ]
        }
      ],
      "source": [
        "train_model(model, n_epochs, batch_size, X_train, y_train, X_test, y_test, loss_fn_train, loss_fn_test, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mypy: allow-untyped-defs\n",
        "\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torch import Tensor\n",
        "from typing import Callable, Optional\n",
        "from typing_extensions import deprecated\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class epsilonMSELoss(_Loss):\n",
        "    r\"\"\"Creates a criterion that measures the mean squared error (squared L2 norm) between\n",
        "    each element in the input :math:`x` and target :math:`y`.\n",
        "\n",
        "    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n",
        "\n",
        "    .. math::\n",
        "        \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
        "        l_n = \\left( x_n - y_n \\right)^2,\n",
        "\n",
        "    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n",
        "    (default ``'mean'``), then:\n",
        "\n",
        "    .. math::\n",
        "        \\ell(x, y) =\n",
        "        \\begin{cases}\n",
        "            \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n",
        "            \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n",
        "        \\end{cases}\n",
        "\n",
        "    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total\n",
        "    of :math:`n` elements each.\n",
        "\n",
        "    The mean operation still operates over all the elements, and divides by :math:`n`.\n",
        "\n",
        "    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.\n",
        "\n",
        "    Args:\n",
        "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
        "            the losses are averaged over each loss element in the batch. Note that for\n",
        "            some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
        "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
        "            when :attr:`reduce` is ``False``. Default: ``True``\n",
        "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
        "            losses are averaged or summed over observations for each minibatch depending\n",
        "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
        "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
        "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
        "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
        "            ``'mean'``: the sum of the output will be divided by the number of\n",
        "            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
        "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
        "            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
        "        - Target: :math:`(*)`, same shape as the input.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> loss = nn.MSELoss()\n",
        "        >>> input = torch.randn(3, 5, requires_grad=True)\n",
        "        >>> target = torch.randn(3, 5)\n",
        "        >>> output = loss(input, target)\n",
        "        >>> output.backward()\n",
        "    \"\"\"\n",
        "    __constants__ = ['reduction']\n",
        "\n",
        "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
        "        super().__init__(size_average, reduce, reduction)\n",
        "\n",
        "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
        "        self.epsilon = 1e-6  \n",
        "        return min(F.mse_loss(input, target, reduction=self.reduction)-self.epsilon, 0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
