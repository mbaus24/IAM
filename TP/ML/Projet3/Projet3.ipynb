{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Objectif \n",
    "\n",
    "L’objectif de ce projet est de programmer un modèle de langage neuronal à l’aide d’un perceptron multi-couches. Ce modèle de langage prend en entrée les plongements de k mots consécutifs et produit en sortie une distribution de probabilité sur l’ensemble du vocabulaire. Contrairement au projet précédent, nous ne ferons pas le calcul du gradient à la main, mais utiliserons la librairie pytorch et les outils de dérivation automatique qu’elle propose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Leclassifieur \n",
    "Le cœur du modèle est constitué d’un perceptron multicouche \\(C\\) composé d’une couche d’entrée \\(x\\), d’une couche cachée \\(h\\) et d’une couche de sortie \\(y\\). Les paramètres du modèle sont regroupés dans deux matrices : \\(W\\) et \\(U\\) et deux vecteurs : \\(b_1\\) et \\(b_2\\). \\(W\\) et \\(b_1\\) permettent de calculer les valeurs de la couche cachée à partir de la couche d’entrée tandis que \\(U\\) et \\(b_2\\) permettent de calculer les valeurs de la couche de sortie à partir de la couche cachée. Étant donné une séquence de mots \\(m_1, \\ldots, m_n\\), le classifieur \\(C\\) prend en entrée les plongements des \\(k\\) mots \\(m_{i-k}, \\ldots, m_{i-1}\\) et produit en sortie une distribution de probabilité sur l’ensemble des mots du vocabulaire \\(V\\). On considère que les plongements de chaque mot sont de dimension \\(d\\). La taille de la couche d’entrée, notée \\(x\\), vaut par conséquent \\(d_x = k \\times d\\) et la couche de sortie a pour dimension \\(|V|\\), la taille du vocabulaire. La dimension de la couche cachée est arbitraire, on la notera \\(d_h\\). Les dimensions des deux matrices \\(W\\) et \\(U\\) sont donc \\(d_x \\times d_h\\) pour \\(W\\) et \\(d_h \\times |V|\\) pour \\(U\\).\n",
    "\n",
    "Le calcul de la couche cachée peut être décomposé en deux étapes :\n",
    "- une étape linéaire : \\(h' = xW + b_1\\)\n",
    "- suivie d’une étape non linéaire : \\(h = \\text{ReLU}(h')\\)\n",
    "\n",
    "De même, le calcul de la couche de sortie peut aussi être décomposé en deux étapes :\n",
    "- une étape linéaire : \\(y' = hU + b_2\\)\n",
    "- suivie d’une étape non linéaire : \\(y = \\text{Softmax}(y')\\)\n",
    "\n",
    "La fonction \\(y = \\text{softmax}(y')\\) permet de transformer un vecteur de valeurs réelles \\(y'\\) en un vecteur de probabilités \\(y\\). Chaque composante \\(y_i\\) de \\(y\\) est calculée de la manière suivante :\n",
    "\n",
    "\\[ y_i = \\frac{\\exp(y'_i)}{\\sum_{j=1}^{|V|} \\exp(y'_j)} \\]\n",
    "\n",
    "Autrement dit, la fonction \\(\\exp(\\cdot)\\) rend la valeur de \\(y'_i\\) positive, et ensuite chaque valeur est normalisée par la somme de toutes les valeurs du vecteur \\(y'_j\\). Cela garantit que \\(y_i\\) est bien une probabilité car \\(\\sum_{i=1}^{|V|} y_i = 1\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.dico_voca = {}\n",
    "        self.word_array = []\n",
    "        if \"emb_filename\" in kwargs :\n",
    "            with open(kwargs[\"emb_filename\"],'r') as fi:\n",
    "                ligne = fi.readline()\n",
    "                ligne = ligne.strip()\n",
    "                (self.vocab_size, self.emb_dim) = map(int,ligne.split(\" \"))\n",
    "                self.matrice = torch.zeros((self.vocab_size, self.emb_dim))\n",
    "                indice_mot = 0\n",
    "        \n",
    "                ligne = fi.readline()\n",
    "                ligne = ligne.strip()\n",
    "                while ligne != '': \n",
    "                    splitted_ligne = ligne.split()\n",
    "                    self.dico_voca[splitted_ligne[0]] = indice_mot\n",
    "                    self.word_array.append(splitted_ligne[0])\n",
    "                    for i in range(1,len(splitted_ligne)):\n",
    "                        self.matrice[indice_mot, i-1] = float(splitted_ligne[i])\n",
    "                    indice_mot += 1\n",
    "                    ligne = fi.readline()\n",
    "                    ligne = ligne.strip()\n",
    "        else:\n",
    "            fichier_corpus = kwargs[\"corpus_filename\"]\n",
    "            self.emb_dim = kwargs[\"emb_dim\"]\n",
    "            nb_tokens = 0\n",
    "            with open(fichier_corpus,'r') as fi:\n",
    "                for line in fi:\n",
    "                    line = line.rstrip()\n",
    "                    tokens = line.split(\" \")\n",
    "                    for token in tokens:\n",
    "                        if token not in self.dico_voca :\n",
    "                            self.word_array.append(token)\n",
    "                            self.dico_voca[token] = nb_tokens\n",
    "                            nb_tokens += 1\n",
    "            self.vocab_size = nb_tokens\n",
    "            print(\"vocab size =\", self.vocab_size, \"emb_dim =\", self.emb_dim)\n",
    "            self.matrice = torch.zeros((self.vocab_size, self.emb_dim))\n",
    "\n",
    "    def get_word_index(self, mot):\n",
    "        if not mot in self.dico_voca:\n",
    "            return None\n",
    "        return self.dico_voca[mot]\n",
    "                \n",
    "    def get_word_index2(self, mot):\n",
    "        if not mot in self.dico_voca:\n",
    "            return self.dico_voca['<unk>']\n",
    "        return self.dico_voca[mot]\n",
    "                \n",
    "    def get_emb(self, mot):\n",
    "        if not mot in self.dico_voca:\n",
    "            return None\n",
    "        return  self.matrice[self.dico_voca[mot]]\n",
    "    \n",
    "    def get_emb_torch(self, indice_mot):\n",
    "        # OPTIMISATION: no verificaiton allows to get embeddings a bit faster\n",
    "        #if indice_mot < 0 or indice_mot >= self.matrice.shape()[0]: # not valid index\n",
    "        #    return None\n",
    "        #return self.matrice[indice_mot]\n",
    "        return self.matrice[indice_mot]\n",
    "        \n",
    "    def get_one_hot(self, mot):\n",
    "        vect = torch.zeros(len(self.dico_voca))\n",
    "        vect[self.dico_voca[mot]] = 1\n",
    "        return vect\n",
    "\n",
    "    def get_word(self, index):\n",
    "        if index < len(self.word_array):\n",
    "            return self.word_array[index]\n",
    "        else:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Vocab object at 0x000002502089F2F0>\n"
     ]
    }
   ],
   "source": [
    "Vocab(emb_filename=\"../data/embeddings-word2vecofficial.train.unk5.txt\")\n",
    "W, U = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ecrire la fonction de pr´ eparation des donn´ ees qui prend en entr´ ee un f ichier de texte T ainsi qu’un dictionnaire associant `a tout mot du vo3cabulaire, un indice1. Cette fonction produit en sortie une liste dont les ´el´ ements sont des listes de k + 1 mots cons´ ecutifs de T repr´ esent´ es par leur indices. Par exemple, si le corpus se pr´ esente de la fa¸con suivante a b c d e f et que le mot a correspond `a l’indice 1, le mot b correspond `a l’indice 2 . . .et que k vaut 3, les listes X et Y se pr´ esenteront sous la forme suivante : X = [[1,2,3,4],[2,3,4,5],[3,4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path, vocab, k=3):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().split()\n",
    "    \n",
    "    data = []\n",
    "    for i in range(len(text) - k):\n",
    "        sequence = [Vocab.get_word_index(word) for word in text[i:i+k+1]]\n",
    "        data.append(sequence)\n",
    "    return data\n",
    "\n",
    "prepare_data(\"Le_co\", Vo, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = torch.tensor([[1,0,3], [0,1,3]])\n",
    "var.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modele(k, d, h, V):\n",
    "    \"\"\"modélisation du perceptron possédant une couche d'entrée de taille k*d, une couche cachée de taille h et une couche de sortie de taille la dimension de V, les matrices de poids sont  W et U\"\"\"\n",
    "    \n",
    "    W, U = Vocab(emb_filename=\"../embeddings.txt\")\n",
    "    \n",
    "\n",
    "    return W, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(plongements):\n",
    "    \"\"\"extraction des plongements de taille d à partir de la matrice de plongements de taille k*d\"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m---> 11\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[43mk\u001b[49m \u001b[38;5;241m*\u001b[39m d, h),\n\u001b[0;32m     12\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m     13\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(h, \u001b[38;5;28mlen\u001b[39m(V))\n\u001b[0;32m     14\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "def forward(W, U, x):\n",
    "    \"\"\"propagation avant du perceptron\"\"\"\n",
    "    h = torch.mm(W, x)\n",
    "    h = torch.relu(h)\n",
    "    y = torch.mm(U, h)\n",
    "    y = torch.softmax(y, 0)\n",
    "    return y\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    \"\"\"calcul de la fonction de perte\"\"\"\n",
    "    return -torch.sum(y*torch.log(y_pred))\n",
    "\n",
    "def backward(W, U, x, y, y_pred):\n",
    "    \"\"\"rétropropagation du gradient\"\"\"\n",
    "    dL_dy = y_pred - y\n",
    "    dL_dU = torch.mm(dL_dy, h.t())\n",
    "    dL_dh = torch.mm(U.t(), dL_dy)\n",
    "    dL_dh[h <= 0] = 0\n",
    "    dL_dW = torch.mm(dL_dh, x.t())\n",
    "    return dL_dW, dL_dU\n",
    "\n",
    "def update(W, U, dL_dW, dL_dU, lr):\n",
    "    \"\"\"mise à jour des poids\"\"\"\n",
    "    W -= lr*dL_dW\n",
    "    U -= lr*dL_dU\n",
    "    return W, U\n",
    "\n",
    "def train(W, U, x, y, lr):\n",
    "    \"\"\"entraînement du perceptron\"\"\"\n",
    "    y_pred = forward(W, U, x)\n",
    "    l = loss(y, y_pred)\n",
    "    dL_dW, dL_dU = backward(W, U, x, y, y_pred)\n",
    "    W, U = update(W, U, dL_dW, dL_dU, lr)\n",
    "    return l\n",
    "\n",
    "def test(W, U, x, y):\n",
    "    \"\"\"test du perceptron\"\"\"\n",
    "    y_pred = forward(W, U, x)\n",
    "    l = loss(y, y_pred)\n",
    "    return l\n",
    "\n",
    "def main():\n",
    "    W, U = Vocab(emb_filename=\"../embeddings.txt\")\n",
    "    W = W.to(device)\n",
    "    U = U.to(device)\n",
    "    W.requires_grad = True\n",
    "    U.requires_grad = True\n",
    "    lr = 0.01\n",
    "    for i in range(100):\n",
    "        l = train(W, U, x, y, lr)\n",
    "        print(\"Train Epoch: \"+i+\" Loss: \"+l)\n",
    "        \n",
    "    l = test(W, U, x, y)\n",
    "    print(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = (np.exp(y))/np.sum(np.exp(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def modèle(plongements=\"plongements\"):\n",
    "\n",
    "    \"\"\"Fonction qui prend en entrée le fichier de plongements (k mots de d plongements) et qui produit la distribution de probabilité sur l'ensemble des mots du vocabulaire\"\"\"\n",
    "    W, U = modele(k, d, h, V)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
