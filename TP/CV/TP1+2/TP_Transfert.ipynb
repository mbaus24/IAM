{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOMP0WoFBA5f"
      },
      "source": [
        "<h1> TP transfert - Deep Learning </h1>\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<b>N'envoyez pas votre travail par mail :</b>\n",
        "- Zippez votre fichier notebook, et nommez l'archive avec votre nom\n",
        "- Envoyez l'archive via la page :  https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/upload/upload.php\n",
        "\n",
        "<hr/>\n",
        "\n",
        "Un musée spécialisé en peintures représentant des animaux de la savane a malheureusement perdu son fichier d'inventaire qui regroupait des informations précieuses sur les 2000 oeuvres du musée ! Fort heureusement, l'informaticien toujours prévoyant avait conservé une copie des 2000 photos des oeuvres, et il vient juste de suivre une formation de Deep Learning ...\n",
        "\n",
        "<center>\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/0.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/1.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/2.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/3.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/4.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/5.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/6.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/7.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/8.jpg\" width=\"90px\" />\n",
        "<img src=\"https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/9.jpg\" width=\"90px\" />\n",
        "</center>\n",
        "\n",
        "Cet examen porte sur la résolution d'une tâche de classification d'images sur un jeu de données faiblement annoté. Plusieurs solutions sont envisagées  consistant à tirer parti d'annotations de classes équivalentes sur un autre dataset entièrement étiquetté (imagenet). Les deux jeux de données contiennent 2000 images réparties (également) en quatre classes (d'animaux : zèbres, gorilles, léopards, tigres). 1500 images sont utilisées pour l'entrainement, 500 pour l'évaluation. Seul le jeu de données A (imagenet) est complètement annoté, le dataset B (les photos des oeuvres du musée) ne contient que très peu d'annotations.\n",
        "\n",
        "La partie 1 consiste à entrainer et évaluer les performances d'un modèle CNN fourni sur les deux datasets d'images. Les parties 2 et 3 consistent à mettre en oeuvre deux solutions par <i>adaptation de domaines</i> pour tenter d'améliorer les performances de classification sur le dataset faiblement annoté. Vous pouvez consacrer environ une heure à chaque partie, notées également.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhEFJi2Jm-AU"
      },
      "source": [
        "<h3> Téléchargement des données </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOoNnEmRXA0O"
      },
      "source": [
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/imagenetXtrain.npy\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/imagenetYtrain.npy\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/imagenetXtest.npy\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/imagenetYtest.npy\n",
        "\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/awaXtrain.npy\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/awaYtrain.npy\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/awaXtest.npy\n",
        "!wget https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/awaYtest.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JELpwwuImeSc"
      },
      "source": [
        "<h3>Chargement des données</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbvBt_nnAkmy"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Dense, Flatten, LeakyReLU\n",
        "from keras.layers import AveragePooling2D, Conv2D, Activation, BatchNormalization, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHDrGe_IYIUm"
      },
      "source": [
        "path = './'\n",
        "num_classes = 4\n",
        "\n",
        "x_trainA = np.load(path+'imagenetXtrain.npy')\n",
        "x_trainA = x_trainA / 255.\n",
        "y_trainA = np.load(path+'imagenetYtrain.npy')\n",
        "y_trainA = keras.utils.to_categorical(y_trainA, num_classes)\n",
        "\n",
        "x_testA = np.load(path+'imagenetXtest.npy')\n",
        "x_testA = x_testA / 255.\n",
        "y_testA = np.load(path+'imagenetYtest.npy')\n",
        "y_testA = keras.utils.to_categorical(y_testA, num_classes)\n",
        "\n",
        "x_trainB = np.load(path+'awaXtrain.npy')\n",
        "x_trainB = x_trainB / 255.\n",
        "y_trainB = np.load(path+'awaYtrain.npy')\n",
        "y_trainB = keras.utils.to_categorical(y_trainB, num_classes)\n",
        "\n",
        "x_testB = np.load(path+'awaXtest.npy')\n",
        "x_testB = x_testB / 255.\n",
        "y_testB = np.load(path+'awaYtest.npy')\n",
        "y_testB = keras.utils.to_categorical(y_testB, num_classes)\n",
        "\n",
        "print('x_trainA.shape:', x_trainA.shape, 'y_trainA.shape:', y_trainA.shape)\n",
        "print('x_trainB.shape:', x_trainB.shape, 'y_trainB.shape:', y_trainB.shape)\n",
        "\n",
        "print('x_testA.shape:', x_testA.shape, 'y_testA.shape:', y_testA.shape)\n",
        "print('x_testB.shape:', x_testB.shape, 'y_testB.shape:', y_testB.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwLsluNtYHkG"
      },
      "source": [
        "Les deux jeux de données contiennent 2000 images, de 64x64x3 pixels, réparties en quatre classes. 1500 images sont utilisées pour l'entrainement, 500 pour l'évaluation. Seul le jeu de données A est complètement annoté, le jeu de donnée B ne contient que très peu d'annotations, correspondantent aux 50 premiers exemples de x_trainB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NRp44rQagA-"
      },
      "source": [
        "<h2> Partie 1 : Modèle convolutionnel pour la classification d'images </h2>\n",
        "\n",
        "On définie ci-dessous une architecture convolutionnelle simple, à ne pas modifier. Cette partie vise à évaluer la performance d'un modèle CNN sur les deux dataset A et B. Entrainez ce modèle des différentes manières possibles et reportez les performances obtenues dans le tableau situé en fin de cette partie. A chaque nouvel apprentissage, utilisez les configurations suivantes :\n",
        "- 30 epochs\n",
        "- Taille des minibatchs à 64\n",
        "- 10% des données pour validation\n",
        "\n",
        "Tracez les courbes d'apprentissage, et commentez vos résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVzDIs5-a_jI"
      },
      "source": [
        "# Couches d'apprentissage de représentations par convolutions\n",
        "ximg = Input(shape=(x_trainA.shape[1], x_trainA.shape[2], x_trainA.shape[3]))\n",
        "\n",
        "l = Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same')(ximg)\n",
        "l = BatchNormalization(axis=-1)(l)\n",
        "l = Activation('relu')(l)\n",
        "l = Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same')(l)\n",
        "l = BatchNormalization(axis=-1)(l)\n",
        "l = Activation('relu')(l)\n",
        "l = Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same')(l)\n",
        "l = BatchNormalization(axis=-1)(l)\n",
        "l = Activation('relu')(l)\n",
        "l = AveragePooling2D((4,4))(l)\n",
        "l = Conv2D(256, kernel_size=(3,3), strides=(2,2), padding='same')(l)\n",
        "l = BatchNormalization(axis=-1)(l)\n",
        "l = Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same')(l)\n",
        "l = AveragePooling2D((2,2))(l)\n",
        "l = Activation('relu')(l)\n",
        "l = Dropout(0.2)(l)\n",
        "feat_layer = Flatten()(l)\n",
        "\n",
        "# conv_model(ximg) retourne la couche 'feat_layer' : 4096 dimensions pour une image 64x64x3\n",
        "conv_model = Model(ximg, feat_layer)\n",
        "\n",
        "# Couches de classification\n",
        "xfeat = Input(shape=(4096,))\n",
        "y = Dense(1024, activation='relu')(xfeat)\n",
        "y = Dropout(0.5)(y)\n",
        "y = Dense(512, activation='relu')(y)\n",
        "out_layer = Dense(num_classes, activation='softmax')(y)\n",
        "\n",
        "# classif_model(xfeat) renvoie la couche sortie du classifieur : 4 dimensions\n",
        "classif_model = Model(xfeat, out_layer)\n",
        "\n",
        "# Modèle CNN : représentation + classifier\n",
        "feat = conv_model(ximg)\n",
        "clf = classif_model(feat)\n",
        "model = Model(ximg, clf)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-0G5SHsaX06"
      },
      "source": [
        "<h4> Entrainement du modèle sur les données A, puis évaluation sur A et B</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GvEjDRqbNXm"
      },
      "source": [
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZgXh4ZmOqkG"
      },
      "source": [
        "Commentaires :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDIVGF9dbnSO"
      },
      "source": [
        "<h4> Entrainement et évaluation du modèle sur les données B</h4>\n",
        "\n",
        "<i>Attention à ne pas réutiliser la variable <b>model</b>, déjà entrainée sur A, il faut créer une nouvelle instance <b>model2</b> à partir des fonctions <b>conv_model()</b> et <b>classif_model()</b> définient précédemment.</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVnJo3ZUbtRD"
      },
      "source": [
        "# A compléter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ82M78uxDXD"
      },
      "source": [
        "Commentaires :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083Zv7bAb6eR"
      },
      "source": [
        "<h4>Finetuning sur B du modèle entrainé sur A</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQU24qAcBcy"
      },
      "source": [
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-3j2kTBxEat"
      },
      "source": [
        "Commentaires :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGTo8Gnyx-8s"
      },
      "source": [
        "<table>\n",
        "  <thead>\n",
        "    <th></th><th>Accuracy train A</th><th>Accuracy dev A</th><th>Accuracy test A</th><th>Accuracy train B</th><th>Accuracy dev B</th><th>Accuracy test B</th>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr><th>Entrainement sur A</th><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td></tr>\n",
        "    <tr><th>Entrainement sur B</th><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td></tr>\n",
        "    <tr><th>Entrainement sur A <br/>+ Finetuning sur B</th><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td><td>###</td></tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "Commentaires :\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vv5WxG2cRgu"
      },
      "source": [
        "\n",
        "<h2>Partie 2 : Adaptation de domaine par alignement des activations</h2>\n",
        "\n",
        "Cette partie consiste à implémenter et évaluer un modèle inspiré de l'article <a href=\"https://arxiv.org/pdf/1607.01719.pdf\">Deep CORAL: Correlation Alignment for Deep Domain Adaptation [Sun & Saenko, 2016] </a>, dont la figure suivante illustre le fonctionnement.\n",
        "\n",
        "<center><img src=\"http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/coral16.png\" width=\"50%\" /></center>\n",
        "\n",
        "Considérez les instructions suivantes :\n",
        "- Utilisez les mêmes modules de convolutions que dans la Partie 1 afin de rester comparable. Pour cela, considérez une nouvelle instance <b>conv_model3</b> comme effectué précédemment.\n",
        "- Seules les couches de convolutions sont partagées. cf https://keras.io/getting-started/functional-api-guide/#shared-layers\n",
        "- Seulement deux couches denses après les convolutions (ie: fc6 et fc7), dropout entre les deux.\n",
        "- Version simplifiée de la <i>CORAL loss</i> : minimisation de la distance entre les dernières couches denses (et/ou maximisation de la corrélation). Utilisez l'une des couches documentées sur la page suivante : https://keras.io/layers/merge/\n",
        "- Entrainez pendant 50 epochs, avec minibatchs = 128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhO-2mGcErz7"
      },
      "source": [
        "<h4> Définition du modèle </h4>\n",
        "\n",
        "xA et xB sont les données des deux domaines, yA est la sortie du classifieur pour les données de xA dans le domaine source, et <i>coral</i> correspond à la métrique (distance ou corrélation) entre un minibatch d'exemples xA et xB, dans les deux domaines. Le modèle optimise les deux loss : <i>categorical_crossentropy</i> pour classer les images du domaine source ; et <i>mse</i> qui minimise (ou maximise) la métrique considérée.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCf5TACncWUd"
      },
      "source": [
        "xA = Input(shape=(x_trainA.shape[1], x_trainA.shape[2], x_trainA.shape[3]))\n",
        "xB = Input(shape=(x_trainA.shape[1], x_trainA.shape[2], x_trainA.shape[3]))\n",
        "\n",
        "# A compléter\n",
        "\n",
        "model4 = Model([xA,xB],[yA, coral])\n",
        "model4.compile(loss=['categorical_crossentropy','mse'], optimizer=Adam(0.0001), metrics=['accuracy'])\n",
        "print(model4.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKBerCO6mKyQ"
      },
      "source": [
        "<h4> Entrainement du modèle d'adaptation par alignement </h4>\n",
        "\n",
        "Selon le critère <i>coral</i> que vous optimisez, considérez le vecteur objectif ne contenant que des 1 ou que des 0.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R1UYCtvmRK7"
      },
      "source": [
        "ones = np.ones((len(x_trainA),1)) # ou zeros = np.zeros((len(x_trainA),1))\n",
        "\n",
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUk9ZasIICOJ"
      },
      "source": [
        "Commentaires :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP_v8XaKcZJd"
      },
      "source": [
        "<h4>Evaluation sur les datasets A et B</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTo7eJ6icecX"
      },
      "source": [
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOdK5HRQcf-0"
      },
      "source": [
        "Commentaires :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP6p06aYm6LB"
      },
      "source": [
        "<h2>Partie 3 : Adaptation de domaine par contrainte adversarial</h2>\n",
        "\n",
        "Cette partie consiste à implémenter un modèle d'adaptation de modèle par contrainte adversarial, inspiré de l'article <a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf\">Adversarial Discriminative Domain Adaptation [Tzeng, 2017]</a>.\n",
        "\n",
        "<center><img src=\"http://stephane.ayache.perso.luminy.univ-amu.fr/examdeep/adversadapt17.png\" width=\"80%\" /></center>\n",
        "\n",
        "Dans la figure ci-dessus, les modules en pointillés indiquent une réutilisation de modèles déjà entrainés, tandis que les modules en lignes pleines désignent des modèles nouvellement entrainés. Pour la phase de </i>Pre-training</i>, considérez le modèle entrainé sur le dataset A dans la partie 1.\n",
        "\n",
        "La stratégie consiste donc à :\n",
        "- Faire le prétraining du modèle source sur les données source avec un critère de classification. Cette étape a été faite en partie 1 : <b>conv_model</b> et <b>classif_model</b> sont déjà entrainés.\n",
        "- Ne pas réentrainer ces deux modules.\n",
        "- Apprendre from scratch le modèle extracteur de caractéristiques pour les données cible à l'aide d'un discriminateur adversarial appris à discriminer entre les données source transformées par le modèle préappris et les données du domaine cible transformées par le modèle cible.  \n",
        "- Construire un classifieur en empilant la couche de classification du modèle source sur l'extracteur de caractéristiques sur les données cible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGHcH8dz_RxC"
      },
      "source": [
        "<h4> Définition du modèle discriminateur </h4>\n",
        "Le discriminateur à pour objectif de distinguer les domaines de deux sources de données (A et B) à parir de représentations extraites de couches de convolutions. Ecrivez un modèle simple à deux couches cachées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miTfCEHBJLSh"
      },
      "source": [
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWrauoL8ZJ1R"
      },
      "source": [
        "<h4> Modèle adversarial </h4>\n",
        "\n",
        "Ecrivez ci-dessous le modèle adversarial qui combine le module <i>Target CNN</i> et le discriminateur. <i>Target CNN</i> consistera en une nouvelle instance <b>conv_model4</b> pour extraire des représentations similaires au modèle entrainé en Partie 1 sur les images sources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INLzkXpmbEnf"
      },
      "source": [
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhKa_hKDcp9m"
      },
      "source": [
        "<h4> Modèle utilisé en phase de test </h4>\n",
        "\n",
        "Construisez ci-dessous le modèle final qui combine les modules <i>Target CNN</i>  (<b>conv_model4</b>) entrainés précédemment et <b>classif_model</b> entrainé en Partie 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po8eUTuZc_JT"
      },
      "source": [
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZtiQNMiduFd"
      },
      "source": [
        "<h4> Entrainement du modèle adversarial</h4>\n",
        "\n",
        "Utilisez les fonctions de création de minibatchs suivantes qui randomisent les ensembles d'apprentissage à chaque nouvel epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiWTQLVpeVxc"
      },
      "source": [
        "batch_size = 64\n",
        "nbdata = len(x_trainA)\n",
        "\n",
        "def get_batchA():\n",
        "    global x_trainA, y_trainA\n",
        "    i = 0\n",
        "    while True:\n",
        "        i = i + batch_size\n",
        "        if i+batch_size > nbdata:\n",
        "            i = 0\n",
        "            lidx = list(range(nbdata))\n",
        "            shuffle(lidx)\n",
        "            x_trainA = x_trainA[lidx]\n",
        "            y_trainA = y_trainA[lidx]\n",
        "        yield x_trainA[i:i+batch_size], y_trainA[i:i+batch_size]\n",
        "\n",
        "def get_batchB():\n",
        "    global x_trainB, y_trainB\n",
        "    i = 0\n",
        "    while True:\n",
        "        i = i + batch_size\n",
        "        if i+batch_size > nbdata:\n",
        "            i = 0\n",
        "            lidx = list(range(nbdata))\n",
        "            shuffle(lidx)\n",
        "            x_trainB = x_trainB[lidx]\n",
        "        yield x_trainB[i:i+batch_size]\n",
        "\n",
        "data_genA = get_batchA()\n",
        "data_genB = get_batchB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKV6F6K9e1QD"
      },
      "source": [
        "<h4> Boucle d'apprentissage </h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Nw9iZce6Ly"
      },
      "source": [
        "ones = np.ones((batch_size,1))\n",
        "zeros = np.zeros((batch_size,1))\n",
        "nb_batchs = int(len(x_trainA)/batch_size)\n",
        "\n",
        "for epoch in range(30):\n",
        "    loss = 0.0\n",
        "    acc = 0.0\n",
        "    for batch in range(nb_batchs):\n",
        "        # get minibatchs\n",
        "        xA, yA = next(data_genA)\n",
        "        xB = next(data_genB)\n",
        "\n",
        "        # A compléter\n",
        "\n",
        "        # train discriminator\n",
        "\n",
        "        # train adversarial model on new minibatch\n",
        "\n",
        "        # monitoring\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruaBJXzFD608"
      },
      "source": [
        "Commentaires :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8gWmvhPD_p8"
      },
      "source": [
        "<h4> Evaluation sur les dataset A et B </h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNSAXf6cD1FU"
      },
      "source": [
        "# A compléter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NT_OtnfEWlm"
      },
      "source": [
        "Commentaires :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSvJq4lFRG0b"
      },
      "source": [
        "<hr/>\n",
        "\n",
        "<b>N'envoyez pas votre travail par mail :</b>\n",
        "- Zippez votre fichier notebook, et nommez l'archive avec votre nom\n",
        "- Envoyez l'archive via la page :  https://pageperso.lis-lab.fr/stephane.ayache/TP_transfer/upload/upload.php\n"
      ]
    }
  ]
}